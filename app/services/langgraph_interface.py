# app/services/langgraph_interface.py

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from datetime import datetime
import os

class LangGraphService(ABC):
    """LangGraph ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤ (íŒ€ì›ì´ êµ¬í˜„í•  ë¶€ë¶„)"""
    
    @abstractmethod
    async def generate_initial_message(
        self, 
        room_id: str, 
        user_info: Dict[str, Any], 
        is_new_room: bool
    ) -> str:
        """ì´ˆê¸° ë©”ì‹œì§€ ìƒì„±"""
        pass
    
    @abstractmethod
    async def process_user_message(
        self, 
        room_id: str, 
        user_id: str, 
        message: str
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬í•˜ì—¬ AI ì‘ë‹µ ìƒì„±"""
        pass

class MockLangGraphService(LangGraphService):
    """ì„ì‹œ êµ¬í˜„ (OpenAI ì§ì ‘ í¬í•¨, ë‚˜ì¤‘ì— ì‹¤ì œ LangGraphë¡œ êµì²´)"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì´ˆê¸°í™”
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.openai_client = AsyncOpenAI(api_key=api_key)
                self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
                self.max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1000"))
                self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
                print("MockLangGraphService - OpenAI ì§ì ‘ ì—°ê²° ì™„ë£Œ")
            else:
                print("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
                self.openai_client = None
        except Exception as e:
            print(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.openai_client = None
    
    async def generate_initial_message(
        self, 
        room_id: str, 
        user_info: Dict[str, Any], 
        is_new_room: bool
    ) -> str:
        """OpenAIë¥¼ í™œìš©í•œ ì´ˆê¸° ë©”ì‹œì§€ ìƒì„±"""
        print(f"[Mock] LangGraph ì´ˆê¸° ë©”ì‹œì§€ ìƒì„± ì¤‘... room_id={room_id}")
        
        if is_new_room:
            return await self._generate_welcome_message_with_openai(user_info)
        else:
            return await self._generate_reconnect_message(user_info)
    
    async def _generate_welcome_message_with_openai(self, user_info: Dict[str, Any]) -> str:
        """OpenAIë¥¼ í™œìš©í•œ í™˜ì˜ ë©”ì‹œì§€ ìƒì„±"""
        name = user_info.get('name', 'ì‚¬ìš©ì')
        projects = user_info.get('projects', [])

        # í”„ë¡œì íŠ¸ ë¶„ì„ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        all_skills = set()
        domains = []
        roles = []

        projects_text = ""
        if projects:
            formatted_projects = []
            for project in projects[:3]:
                project_name = project.get('project_name', 'í”„ë¡œì íŠ¸ëª… ë¯¸ìƒ')
                domain = project.get('domain', 'ë„ë©”ì¸ ë¯¸ìƒ')
                role = project.get('role', 'ì—­í•  ë¯¸ìƒ')
                scale = project.get('scale', 'ë¯¸ê¸°ì…')
                project_skills = project.get('skills', [])
                
                domains.append(domain)
                roles.append(role)
                all_skills.update(project_skills)
                
                project_info = f"â€¢ {project_name} ({domain} ë„ë©”ì¸)"
                project_info += f" - {role} ì—­í•  ({scale} ê·œëª¨)"
                if project_skills:
                    project_info += f" - {len(project_skills)}ê°œ ê¸°ìˆ  í™œìš©"
                formatted_projects.append(project_info)
            
            if len(projects) > 3:
                formatted_projects.append(f"... ì™¸ {len(projects) - 3}ê°œ í”„ë¡œì íŠ¸")
            
            projects_text = '\n'.join(formatted_projects)
        else:
            projects_text = "ì§„í–‰í•œ í”„ë¡œì íŠ¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        from collections import Counter
        domain_counts = Counter(domains)
        role_counts = Counter(roles)
        
        primary_domain = domain_counts.most_common(1)[0][0] if domain_counts else "ë¯¸ë¶„ë¥˜"
        primary_role = role_counts.most_common(1)[0][0] if role_counts else "ë¯¸ë¶„ë¥˜"
        
        domain_text = ', '.join([f"{domain}({count}íšŒ)" for domain, count in domain_counts.items()]) if domain_counts else "ë„ë©”ì¸ ê²½í—˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        role_text = ', '.join([f"{role}({count}íšŒ)" for role, count in role_counts.items()]) if role_counts else "ì—­í•  ê²½í—˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # OpenAI í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
        enhanced_prompt = f"""
        ë‹¤ìŒì€ ìƒˆë¡œ ë§Œë‚œ ì‚¬ìš©ìì˜ ìƒì„¸ ì •ë³´ì…ë‹ˆë‹¤:
        
        === ê¸°ë³¸ ì •ë³´ ===
        ì´ë¦„: {name}
        ì´ í”„ë¡œì íŠ¸ ê²½í—˜: {len(projects)}ê°œ
        ì£¼ìš” ë„ë©”ì¸: {primary_domain}
        ì£¼ìš” ì—­í• : {primary_role}
        
        === í”„ë¡œì íŠ¸ ê²½í—˜ ===
        {projects_text}
        
        === ë³´ìœ  ìŠ¤í‚¬ ===
        {', '.join(list(all_skills)[:10])}{'...' if len(all_skills) > 10 else ''}
        
        === ë„ë©”ì¸ë³„ ê²½í—˜ ===
        {domain_text}
        
        === ì—­í• ë³„ ê²½í—˜ ===
        {role_text}
        
        ë‹¹ì‹ ì€ SK AX ì‚¬ë‚´ ì»¤ë¦¬ì–´íŒ¨ìŠ¤ ì „ë¬¸ ìƒë‹´ì‚¬ "G.Navi"ì…ë‹ˆë‹¤. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” ê°œì¸í™”ëœ ì¸ì‚¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        1. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±
        2. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ êµ¬ì„±
        3. ì‚¬ìš©ìì˜ ì£¼ìš” ê²½í—˜ì´ë‚˜ ìŠ¤í‚¬ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰
        4. ì–´ë–¤ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
        5. í•œêµ­ì–´ë¡œ ì‘ì„±
        
        ì˜ˆì‹œ ìŠ¤íƒ€ì¼: "ì•ˆë…•í•˜ì„¸ìš” [ì´ë¦„]ë‹˜! [ì£¼ìš” ê²½í—˜/ìŠ¤í‚¬ ì–¸ê¸‰]. [ì œê³µ ê°€ëŠ¥í•œ ë„ì›€ ì œì‹œ]"
        """
        
        # OpenAI API ì§ì ‘ í˜¸ì¶œ
        if self.openai_client:
            try:
                print("OpenAI ì‘ë‹µ ìƒì„± ì¤‘...")
                response = await self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user",
                            "content": enhanced_prompt
                        }
                    ],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                
                initial_message = response.choices[0].message.content.strip()
                print(f"OpenAI ì‘ë‹µ ìƒì„± ì™„ë£Œ: {initial_message[:100]}...")
                return initial_message
                
            except Exception as e:
                print(f"OpenAI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°± ë©”ì‹œì§€
                return f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜! SK AX ì»¤ë¦¬ì–´íŒ¨ìŠ¤ ì „ë¬¸ ìƒë‹´ì‚¬ G.Naviì…ë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
        else:
            # OpenAI ì—†ì„ ë•Œ ê¸°ë³¸ ë©”ì‹œì§€
            if projects:
                return f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜! {primary_domain} ë¶„ì•¼ì—ì„œ {primary_role}ë¡œ í™œë™í•˜ê³  ê³„ì‹œëŠ”êµ°ìš”. SK AX ì»¤ë¦¬ì–´íŒ¨ìŠ¤ ì „ë¬¸ ìƒë‹´ì‚¬ G.Naviì…ë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
            else:
                return f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜! SK AX ì»¤ë¦¬ì–´íŒ¨ìŠ¤ ì „ë¬¸ ìƒë‹´ì‚¬ G.Naviì…ë‹ˆë‹¤. ì»¤ë¦¬ì–´ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
    
    async def _generate_reconnect_message(self, user_info: Dict[str, Any]) -> str:
        """ì¬ì ‘ì† ë©”ì‹œì§€ ìƒì„±"""
        name = user_info.get('name', 'ì‚¬ìš©ì')
        return f"ë‹¤ì‹œ ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤, {name}ë‹˜! ì´ì „ ëŒ€í™”ë¥¼ ì´ì–´ê°€ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
    
    async def process_user_message(
        self, 
        room_id: str, 
        user_id: str, 
        message: str
    ) -> Dict[str, Any]:
        """OpenAIë¥¼ í™œìš©í•œ ë©”ì‹œì§€ ì²˜ë¦¬"""
        print(f"ğŸ¤– [Mock] LangGraph ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘... message={message[:30]}...")
        
        if self.openai_client:
            try:
                response = await self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ SK AX ì‚¬ë‚´ ì»¤ë¦¬ì–´íŒ¨ìŠ¤ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì»¤ë¦¬ì–´ ê´€ë ¨ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user",
                            "content": message
                        }
                    ],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                
                ai_response = response.choices[0].message.content.strip()
                
                return {
                    "content": ai_response,
                    "metadata": {
                        "processing_method": "mock_langgraph_with_openai",
                        "timestamp": datetime.utcnow().isoformat(),
                        "confidence": 0.8
                    }
                }
                
            except Exception as e:
                print(f"âŒ OpenAI ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return {
                    "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "metadata": {
                        "processing_method": "fallback",
                        "error": str(e)
                    }
                }
        else:
            # OpenAI ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ
            return {
                "content": f"'{message}'ì— ëŒ€í•´ ë§ì”€í•´ì£¼ì…¨êµ°ìš”. ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ë” ë‚˜ì€ ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "metadata": {
                    "processing_method": "fallback_no_openai"
                }
            }