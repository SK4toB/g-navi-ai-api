# careerhistory_data_processor.py
# csv íŒŒì¼ì„ ì½ì–´ ê²½ë ¥ ë°ì´í„°ë¥¼ ê·¸ë£¹í•‘í•˜ê³  docsê³¼ vectordbë¥¼ í•œë²ˆì— ìƒì„±í•˜ëŠ” ë„êµ¬

"""
ê¸°ì¡´ VectorDBì˜ ê·¸ë£¹í•‘ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³  ìˆ˜ì • (í†µí•© ìƒì„±ë„êµ¬)

ì´ ë„êµ¬ëŠ” ê²½ë ¥ ë°ì´í„°ë¥¼ ê°œì¸ë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³ , 
ê° ê°œì¸ì˜ ê²½ë ¥ íƒ€ì„ë¼ì¸ì„ í†µí•©í•˜ì—¬ ë¬¸ì„œë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ì´í›„, ìƒì„±ëœ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ë° ë¶„ì„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.

ì´ ë„êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤:
1. CSV íŒŒì¼ì—ì„œ ê²½ë ¥ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê°œì¸ë³„ë¡œ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤.
2. ê° ê°œì¸ì˜ ê²½ë ¥ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì—°ì†ì ì¸ íƒ€ì„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
3. ìƒì„±ëœ íƒ€ì„ë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ê´„ì ì´ê³  ì •í™•í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
4. ìƒì„±ëœ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.

ì£¼ì˜ : readonly ì˜¤ë¥˜ ë°œìƒì‹œ, vector_stores/career_data ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
"""

import os
import shutil
import json
import pandas as pd
import logging
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datetime import datetime
from dotenv import load_dotenv
load_dotenv()


class VectorDBGroupingFixer:
    """VectorDB ê·¸ë£¹í•‘ ë¬¸ì œ ìë™ ìˆ˜ì • ë„êµ¬"""
    
    def __init__(self, 
                 csv_path: str = "app/data/csv/career_history_v2.csv",
                 skillset_csv_path: str = "app/data/csv/skill_set.csv",
                 persist_directory: str = "app/storage/vector_stores/career_data",
                 cache_directory: str = "app/storage/cache/embedding_cache",
                 docs_json_path: str = "app/storage/docs/career_history.json"):
        
        self.csv_path = os.path.abspath(csv_path)
        self.skillset_csv_path = os.path.abspath(skillset_csv_path)
        self.persist_directory = os.path.abspath(persist_directory)
        self.cache_directory = os.path.abspath(cache_directory)
        self.docs_json_path = os.path.abspath(docs_json_path)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì„ë² ë”© ì„¤ì •
        self.base_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            dimensions=1536
        )
        self.cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
            self.base_embeddings,
            LocalFileStore(cache_directory),
            namespace="career_embeddings"
        )
        
        # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=150,
            separators=["\n=== ê²½ë ¥ ", "\ní”„ë¡œì íŠ¸:", "\nì£¼ìš”ì—…ë¬´:", "\nì—­í• :", "\n\n", "\n", ": ", " "],
            length_function=len
        )
        
        # ìŠ¤í‚¬ì…‹ ë§¤í•‘
        self.skillset_mapping = {}
        self._load_skillset_mapping()
    
    def _load_skillset_mapping(self):
        """ìŠ¤í‚¬ì…‹ ë§¤í•‘ ë¡œë“œ"""
        try:  # ìŠ¤í‚¬ì…‹ ë§¤í•‘ ë¡œë“œ ì‹œì‘
            if os.path.exists(self.skillset_csv_path):  # ìŠ¤í‚¬ì…‹ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                skill_df = pd.read_csv(self.skillset_csv_path, encoding='utf-8')  # CSV íŒŒì¼ ë¡œë“œ
                mapping = {}
                for _, row in skill_df.iterrows():  # ìŠ¤í‚¬ì…‹ ë°ì´í„° ìˆœíšŒ
                    if pd.notna(row.get('ì½”ë“œ')):  # ì½”ë“œê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                        code = str(row['ì½”ë“œ']).strip()  # ì½”ë“œ ì •ë¦¬
                        mapping[code] = {
                            'skill_name': str(row.get('Skill set', '')).strip(),
                            'job_category': str(row.get('Skillset-ì§ë¬´ì—°ê³„', '')).strip(),
                            'description': str(row.get('Skill set', '')).strip()
                        }
                # end for (ìŠ¤í‚¬ì…‹ ë°ì´í„° ìˆœíšŒ)
                self.skillset_mapping = mapping
                self.logger.info(f"ìŠ¤í‚¬ì…‹ ë§¤í•‘ ì™„ë£Œ: {len(mapping)}ê°œ")
        except Exception as e:  # ì˜ˆì™¸ ì²˜ë¦¬
            self.logger.warning(f"ìŠ¤í‚¬ì…‹ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def load_and_group_career_data(self) -> Dict[str, pd.DataFrame]:
        """ê²½ë ¥ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê°œì¸ë³„ë¡œ ì˜¬ë°”ë¥´ê²Œ ê·¸ë£¹í•‘"""
        try:  # ê²½ë ¥ ë°ì´í„° ë¡œë“œ ì‹œì‘
            # CSV ë¡œë“œ
            df = pd.read_csv(self.csv_path, encoding='utf-8')  # CSV íŒŒì¼ ë¡œë“œ
            self.logger.info(f"ê²½ë ¥ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
            
            # ë°ì´í„° ì •ì œ
            df = df.dropna(subset=['ê³ ìœ ë²ˆí˜¸'])  # ê³ ìœ ë²ˆí˜¸ ëˆ„ë½ ë°ì´í„° ì œê±°
            df['ê³ ìœ ë²ˆí˜¸'] = df['ê³ ìœ ë²ˆí˜¸'].astype(str)  # ê³ ìœ ë²ˆí˜¸ ë¬¸ìì—´ ë³€í™˜
            
            # ì—°ë„ ë° ì—°ì°¨ ë°ì´í„° ì •ì œ
            if 'ì—°ë„' in df.columns:  # ì—°ë„ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                df['ì—°ë„_numeric'] = pd.to_numeric(df['ì—°ë„'], errors='coerce')  # ì—°ë„ ìˆ«ì ë³€í™˜
            if 'ì—°ì°¨' in df.columns:  # ì—°ì°¨ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                df['ì—°ì°¨_numeric'] = pd.to_numeric(df['ì—°ì°¨'], errors='coerce')  # ì—°ì°¨ ìˆ«ì ë³€í™˜
            
            # ê°œì¸ë³„ ê·¸ë£¹í•‘ ë° ì—°ë„-ì—°ì°¨ìˆœ ì •ë ¬
            grouped = df.groupby('ê³ ìœ ë²ˆí˜¸')  # ê³ ìœ ë²ˆí˜¸ë³„ ê·¸ë£¹í•‘
            employee_groups = {}
            
            for emp_id, group_df in grouped:  # ì§ì›ë³„ ê·¸ë£¹ ìˆœíšŒ
                group_df = group_df.copy()
                
                # ì—°ë„ì™€ ì—°ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì—°ë„ ìš°ì„ , ì—°ì°¨ ë³´ì¡°)
                if 'ì—°ë„_numeric' in group_df.columns and 'ì—°ì°¨_numeric' in group_df.columns:  # ì—°ë„ì™€ ì—°ì°¨ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    group_df = group_df.sort_values(['ì—°ë„_numeric', 'ì—°ì°¨_numeric'], na_position='last')  # ì—°ë„-ì—°ì°¨ ìˆœ ì •ë ¬
                elif 'ì—°ì°¨_numeric' in group_df.columns:  # ì—°ì°¨ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    group_df = group_df.sort_values('ì—°ì°¨_numeric', na_position='last')  # ì—°ì°¨ìˆœ ì •ë ¬
                
                employee_groups[emp_id] = group_df.reset_index(drop=True)
                
                # ê·¸ë£¹í•‘ ê²°ê³¼ ë¡œê¹… (ì—°ë„ ë²”ìœ„ í¬í•¨)
                if 'ì—°ë„_numeric' in group_df.columns:  # ì—°ë„ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    years = group_df['ì—°ë„_numeric'].dropna()  # ì—°ë„ ë°ì´í„° ì¶”ì¶œ
                    career_years = group_df['ì—°ì°¨_numeric'].dropna()  # ì—°ì°¨ ë°ì´í„° ì¶”ì¶œ
                    
                    year_info = ""
                    if len(years) > 0:  # ì—°ë„ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                        year_info += f"{int(years.min())}-{int(years.max())}ë…„"
                    if len(career_years) > 0:  # ì—°ì°¨ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                        year_info += f" ({int(career_years.min())}-{int(career_years.max())}ë…„ì°¨)"
                    
                    self.logger.info(f"  {emp_id}: {len(group_df)}í–‰, {year_info}")
            # end for (ì§ì›ë³„ ê·¸ë£¹ ìˆœíšŒ)
            
            self.logger.info(f"ê°œì¸ë³„ ê·¸ë£¹í•‘ ì™„ë£Œ: {len(employee_groups)}ëª…")
            return employee_groups
            
        except Exception as e:  # ì˜ˆì™¸ ì²˜ë¦¬
            self.logger.error(f"ê²½ë ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _find_column_by_keyword(self, df: pd.DataFrame, keywords: List[str]) -> Optional[str]:
        """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í—¬í¼ í•¨ìˆ˜"""
        for col in df.columns:
            col_clean = col.replace('\n', ' ').strip()
            for keyword in keywords:
                if keyword in col_clean:
                    return col
        return None
    
    def create_integrated_career_timeline(self, emp_df: pd.DataFrame, emp_id: str) -> str:
        """ê°œì¸ì˜ ëª¨ë“  ê²½ë ¥ì„ í†µí•©í•œ ì—°ì†ì ì¸ íƒ€ì„ë¼ì¸ ìƒì„± (ì—°ë„ ì •ë³´ í¬í•¨)"""
        
        # í—¤ë” ìƒì„±
        timeline_text = f"â–  ê³ ìœ ë²ˆí˜¸: {emp_id}\n"
        timeline_text += f"â–  ì´ ê²½ë ¥ ê¸°ë¡: {len(emp_df)}ê°œ\n"
        
        # ì—°ë„ ë° ê²½ë ¥ ê¸°ê°„ ì •ë³´ (ê°•í™”ëœ ë¡œì§)
        year_info = self._extract_year_career_info(emp_df)
        if year_info:
            timeline_text += f"â–  í™œë™ ì—°ë„: {year_info['year_range']}\n"
            timeline_text += f"â–  ê²½ë ¥ ê¸°ê°„: {year_info['career_range']}\n"
            timeline_text += f"â–  ì´ í™œë™ ê¸°ê°„: {year_info['total_active_years']}ë…„\n"
            timeline_text += f"â–  ê²½ë ¥ ë°œì „: {year_info['career_progression']}\n"
        
        # ì»¬ëŸ¼ëª… ë™ì  ì°¾ê¸°
        project_col = self._find_column_by_keyword(emp_df, ['ì£¼ìš” ì—…ë¬´', 'í”„ë¡œì íŠ¸'])
        role_col = self._find_column_by_keyword(emp_df, ['ìˆ˜í–‰ì—­í• ', 'ì—­í• '])
        domain_col = self._find_column_by_keyword(emp_df, ['Industry', 'Domain'])
        scale_col = self._find_column_by_keyword(emp_df, ['í”„ë¡œì íŠ¸ ê·œëª¨'])
        
        # ì£¼ìš” ë„ë©”ì¸ ìš”ì•½
        if domain_col and domain_col in emp_df.columns:
            domains = emp_df[domain_col].dropna().unique()
            if len(domains) > 0:
                timeline_text += f"â–  ì£¼ìš” ë„ë©”ì¸: {', '.join(domains)}\n"
        
        # ì£¼ìš” ì—­í•  ìš”ì•½
        if role_col and role_col in emp_df.columns:
            roles = emp_df[role_col].dropna().unique()
            if len(roles) > 0:
                timeline_text += f"â–  ì£¼ìš” ì—­í• : {', '.join(roles)}\n"
        
        timeline_text += "\n"
        
        # ì—°ë„-ì—°ì°¨ë³„ ìƒì„¸ ì •ë³´ (ì‹œê°„ìˆœ)
        timeline_text += "=== ì—°ë„ë³„ ê²½ë ¥ ìƒì„¸ ===\n\n"
        
        for idx, (_, row) in enumerate(emp_df.iterrows(), 1):
            # ì—°ë„ ë° ì—°ì°¨ ì •ë³´ í†µí•© í‘œì‹œ
            year_career_info = self._format_year_career_info(row)
            timeline_text += f"â–£ {year_career_info}\n"
            
            # í”„ë¡œì íŠ¸/ì—…ë¬´ ì •ë³´
            if project_col and pd.notna(row.get(project_col)):
                timeline_text += f"  ğŸ“‹ ì—…ë¬´: {row[project_col]}\n"
            
            # ì—­í•  ì •ë³´
            if role_col and pd.notna(row.get(role_col)):
                timeline_text += f"  ğŸ‘¤ ì—­í• : {row[role_col]}\n"
            
            # ë„ë©”ì¸ ì •ë³´
            if domain_col and pd.notna(row.get(domain_col)):
                timeline_text += f"  ğŸ¢ ë„ë©”ì¸: {row[domain_col]}\n"
            
            # í”„ë¡œì íŠ¸ ê·œëª¨
            if scale_col and pd.notna(row.get(scale_col)):
                timeline_text += f"  ğŸ“Š ê·œëª¨: {row[scale_col]}\n"
            
            # ìŠ¤í‚¬ì…‹ ì •ë³´
            skills = self._extract_skills_from_row(row)
            if skills:
                resolved_skills = self._resolve_skill_codes(skills)
                if resolved_skills['skill_names']:
                    timeline_text += f"  ğŸ”§ í™œìš© ê¸°ìˆ : {', '.join(resolved_skills['skill_names'][:5])}\n"
            
            # ì¤‘ìš” ê²½ë ¥ í¬ì¸íŠ¸
            if self._is_important_career_point(row):
                impact_desc = row.get('í° ì˜í–¥ì„ ë°›ì€ ì—…ë¬´/ì‹œê¸°ì— ëŒ€í•œ ì„¤ëª…')
                if pd.notna(impact_desc):
                    timeline_text += f"  â­ í•µì‹¬ ê²½ë ¥: {impact_desc}\n"
            
            timeline_text += "\n"
        
        return timeline_text
    
    def _extract_year_career_info(self, emp_df: pd.DataFrame) -> Dict[str, Any]:
        """ì—°ë„ ë° ê²½ë ¥ ì •ë³´ ì¶”ì¶œ ë° ë¶„ì„"""
        info = {}
        
        # ì—°ë„ ì •ë³´ ì²˜ë¦¬
        if 'ì—°ë„' in emp_df.columns:
            years = pd.to_numeric(emp_df['ì—°ë„'], errors='coerce').dropna()
            if not years.empty:
                min_year = int(years.min())
                max_year = int(years.max())
                info['year_range'] = f"{min_year}ë…„ ~ {max_year}ë…„"
                info['total_active_years'] = max_year - min_year + 1
                info['year_list'] = sorted(years.astype(int).tolist())
        
        # ê²½ë ¥ ì—°ì°¨ ì •ë³´ ì²˜ë¦¬
        if 'ì—°ì°¨' in emp_df.columns:
            career_years = pd.to_numeric(emp_df['ì—°ì°¨'], errors='coerce').dropna()
            if not career_years.empty:
                min_career = int(career_years.min())
                max_career = int(career_years.max())
                info['career_range'] = f"{min_career}ë…„ì°¨ ~ {max_career}ë…„ì°¨"
                info['career_progression'] = f"{max_career - min_career + 1}ë…„ê°„ ë°œì „"
                info['career_list'] = sorted(career_years.astype(int).tolist())
        
        # ì—°ë„ì™€ ê²½ë ¥ì˜ ì¼ê´€ì„± í™•ì¸
        if 'year_list' in info and 'career_list' in info:
            # ì—°ë„ ë²”ìœ„ì™€ ê²½ë ¥ ë°œì „ì˜ ì—°ê´€ì„± ë¶„ì„
            year_span = info['total_active_years']
            career_span = len(set(info['career_list']))
            
            if year_span > 0 and career_span > 0:
                info['consistency_ratio'] = career_span / year_span
                if info['consistency_ratio'] > 0.8:
                    info['career_consistency'] = "ì—°ì†ì  ê²½ë ¥ ë°œì „"
                elif info['consistency_ratio'] > 0.5:
                    info['career_consistency'] = "ì¤‘ê°„ ìˆ˜ì¤€ ê²½ë ¥ ë°œì „"
                else:
                    info['career_consistency'] = "ë‹¨í¸ì  ê²½ë ¥ ê¸°ë¡"
        
        return info
    
    def _format_year_career_info(self, row: pd.Series) -> str:
        """ê°œë³„ í–‰ì˜ ì—°ë„-ì—°ì°¨ ì •ë³´ë¥¼ í¬ë§·íŒ…"""
        year = row.get('ì—°ë„')
        career_year = row.get('ì—°ì°¨')
        
        parts = []
        
        if pd.notna(year):
            parts.append(f"{int(year)}ë…„")
        
        if pd.notna(career_year):
            parts.append(f"{int(career_year)}ë…„ì°¨")
        
        if not parts:
            # ì—°ë„/ì—°ì°¨ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë‹¤ë¥¸ ì‹ë³„ ì •ë³´ ì‚¬ìš©
            if pd.notna(row.get('ì‹œì‘ì—°ì°¨')) and pd.notna(row.get('ì¢…ë£Œì—°ì°¨')):
                start_year = int(row['ì‹œì‘ì—°ì°¨'])
                end_year = int(row['ì¢…ë£Œì—°ì°¨'])
                parts.append(f"{start_year}-{end_year}ë…„ì°¨ ê¸°ê°„")
            else:
                parts.append("ê¸°ë¡")
        
        return " ".join(parts)
    
    def _extract_skills_from_row(self, row: pd.Series) -> List[str]:
        """í–‰ì—ì„œ ìŠ¤í‚¬ ì½”ë“œë“¤ ì¶”ì¶œ (ìƒˆë¡œìš´ ì»¬ëŸ¼ëª… ì§€ì›)"""
        skills = []
        
        # ìƒˆë¡œìš´ v2 í˜•ì‹ì˜ ìŠ¤í‚¬ ì»¬ëŸ¼ë“¤
        skill_columns = [
            'í™œìš© Skill set 1', 'í™œìš© Skill set 2', 'í™œìš© Skill set 3', 'í™œìš© Skill set 4'
        ]
        
        for col in skill_columns:
            if col in row.index and pd.notna(row[col]):
                skill_code = str(row[col]).strip()
                if skill_code:
                    skills.append(skill_code)
        
        # ê¸°ì¡´ í˜•ì‹ë„ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
        for col in row.index:
            if 'Skill set' in col and col not in skill_columns and pd.notna(row[col]):
                skill_code = str(row[col]).strip()
                if skill_code:
                    skills.append(skill_code)
        
        return skills
    
    def _resolve_skill_codes(self, skill_codes: List[str]) -> Dict[str, Any]:
        """ìŠ¤í‚¬ ì½”ë“œë¥¼ ì‹¤ì œ ìŠ¤í‚¬ëª…ìœ¼ë¡œ ë³€í™˜"""
        resolved_skills = {
            'skill_names': [],
            'job_categories': [],
            'technical_skills': [],
            'management_skills': []
        }
        
        for code in skill_codes:
            if code and code in self.skillset_mapping:
                skill_info = self.skillset_mapping[code]
                skill_name = skill_info['skill_name']
                job_category = skill_info['job_category']
                
                resolved_skills['skill_names'].append(skill_name)
                resolved_skills['job_categories'].append(job_category)
                
                # ê¸°ìˆ /ê´€ë¦¬ ìŠ¤í‚¬ ë¶„ë¥˜
                if any(keyword in skill_name.lower() for keyword in 
                      ['dev', 'eng', 'architect', 'database', 'system', 'network']):
                    resolved_skills['technical_skills'].append(skill_name)
                elif any(keyword in skill_name.lower() for keyword in 
                        ['pm', 'management', 'ê´€ë¦¬', 'lead']):
                    resolved_skills['management_skills'].append(skill_name)
        
        # ì¤‘ë³µ ì œê±°
        for key in resolved_skills:
            resolved_skills[key] = list(set(resolved_skills[key]))
        
        return resolved_skills
    
    def _is_important_career_point(self, row: pd.Series) -> bool:
        """ì¤‘ìš”í•œ ê²½ë ¥ í¬ì¸íŠ¸ì¸ì§€ í™•ì¸ (ìƒˆë¡œìš´ ì»¬ëŸ¼ëª… ì§€ì›)"""
        # ìƒˆë¡œìš´ v2 í˜•ì‹
        impact_col_v2 = 'ì»¤ë¦¬ì–´ í˜•ì„±ì— í° ì˜í–¥ì„ ë°›ì€ ì—…ë¬´ë‚˜ ì‹œê¸°'
        if impact_col_v2 in row.index:
            return pd.notna(row.get(impact_col_v2)) and str(row.get(impact_col_v2)).upper() == 'TRUE'
        
        # ê¸°ì¡´ í˜•ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        impact_col_v1 = 'ì»¤ë¦¬ì–´ í˜•ì„±ì— í° ì˜í–¥ì„ ë°›ì€ ì—…ë¬´ë‚˜ ì‹œê¸°'
        return pd.notna(row.get(impact_col_v1)) and str(row.get(impact_col_v1)).upper() == 'TRUE'
    
    def create_comprehensive_metadata(self, emp_id: str, emp_df: pd.DataFrame) -> Dict[str, Any]:
        """í¬ê´„ì ì´ê³  ì •í™•í•œ ë©”íƒ€ë°ì´í„° ìƒì„± (ì—°ë„ ì •ë³´ í¬í•¨)"""
        metadata = {
            'employee_id': emp_id,
            'total_career_records': len(emp_df),
            'source_file': self.csv_path,
            'processing_timestamp': datetime.now().isoformat(),
            'processing_method': 'integrated_career_timeline_with_year_data'
        }
        
        # ì—°ë„ ì •ë³´ ì¶”ê°€ (ì‹ ê·œ)
        if 'ì—°ë„' in emp_df.columns:
            years = pd.to_numeric(emp_df['ì—°ë„'], errors='coerce').dropna()
            if not years.empty:
                min_year = int(years.min())
                max_year = int(years.max())
                metadata.update({
                    'activity_start_year': min_year,
                    'activity_end_year': max_year,
                    'total_activity_years': max_year - min_year + 1,
                    'activity_years_list': sorted(years.astype(int).tolist()),
                    'activity_decade': f"{min_year//10*10}s-{max_year//10*10}s"
                })
        
        # ì—°ì°¨ ì •ë³´ (ê¸°ì¡´ ë¡œì§ ê°œì„ )
        if 'ì—°ì°¨' in emp_df.columns:
            career_years = pd.to_numeric(emp_df['ì—°ì°¨'], errors='coerce').dropna()
            if not career_years.empty:
                min_career = int(career_years.min())
                max_career = int(career_years.max())
                metadata.update({
                    'career_start_year': min_career,
                    'career_end_year': max_career,
                    'total_experience_years': max_career - min_career + 1,
                    'experience_level': self._categorize_experience_level(max_career),
                    'career_years_list': sorted(career_years.astype(int).tolist()),
                    'career_progression_span': max_career - min_career + 1
                })
        
        # ì—°ë„-ì—°ì°¨ ì¼ê´€ì„± ë¶„ì„ (ì‹ ê·œ)
        if 'activity_years_list' in metadata and 'career_years_list' in metadata:
            activity_span = metadata['total_activity_years']
            career_span = metadata['career_progression_span']
            
            metadata.update({
                'year_career_consistency': career_span / activity_span if activity_span > 0 else 0,
                'career_continuity': self._analyze_career_continuity(
                    metadata['activity_years_list'], 
                    metadata['career_years_list']
                )
            })
        
        # ë„ë©”ì¸ ë¶„ì„ (ì»¬ëŸ¼ëª… ì—…ë°ì´íŠ¸)
        domain_columns = ['Industry/Domain', 'Domain ê²½í—˜']
        for col in domain_columns:
            if col in emp_df.columns:
                domains = emp_df[col].dropna().unique()
                if len(domains) > 0:
                    metadata.update({
                        'domains': list(domains),
                        'primary_domain': domains[0],
                        'domain_diversity': len(domains),
                        'is_domain_specialist': len(domains) <= 2
                    })
                break
        
        # ì—­í•  ë¶„ì„ (ì»¬ëŸ¼ëª… ì—…ë°ì´íŠ¸)
        role_columns = ['ìˆ˜í–‰ì—­í• ', 'ì—­í• ']
        for col in role_columns:
            if col in emp_df.columns:
                roles = emp_df[col].dropna().unique()
                if len(roles) > 0:
                    metadata.update({
                        'roles': list(roles),
                        'role_progression_complexity': len(roles),
                        'is_leadership_track': any('PM' in str(role) or 'ê´€ë¦¬' in str(role) or 'Lead' in str(role) 
                                                 for role in roles)
                    })
                break
        
        # ìŠ¤í‚¬ì…‹ ì¢…í•© ë¶„ì„
        all_skills = []
        for _, row in emp_df.iterrows():
            row_skills = self._extract_skills_from_row(row)
            all_skills.extend(row_skills)
        
        if all_skills:
            resolved_skills = self._resolve_skill_codes(list(set(all_skills)))
            metadata.update({
                'total_skill_codes': len(set(all_skills)),
                'skill_names': resolved_skills['skill_names'][:10],  # ìƒìœ„ 10ê°œë§Œ
                'job_categories': list(set(resolved_skills['job_categories'])),
                'technical_skills': resolved_skills['technical_skills'][:5],
                'management_skills': resolved_skills['management_skills'][:5],
                'skill_diversity_score': len(set(resolved_skills['job_categories']))
            })
        
        # í”„ë¡œì íŠ¸ ê·œëª¨ ë¶„ì„ (ì‹ ê·œ)
        scale_columns = ['í”„ë¡œì íŠ¸ ê·œëª¨ (ëŒ€ëµ)', 'í”„ë¡œì íŠ¸ ê·œëª¨(M/M)']
        for col in scale_columns:
            if col in emp_df.columns:
                scales = emp_df[col].dropna()
                if not scales.empty:
                    metadata.update({
                        'project_scales': list(scales.unique()),
                        'project_scale_diversity': len(scales.unique()),
                        'has_large_projects': any('ëŒ€í˜•' in str(scale) or '50ì–µ ì´ìƒ' in str(scale) 
                                                for scale in scales)
                    })
                break
        
        # ì¤‘ìš” ê²½ë ¥ í¬ì¸íŠ¸ ìˆ˜
        important_points = sum(1 for _, row in emp_df.iterrows() if self._is_important_career_point(row))
        metadata['critical_career_points'] = important_points
        
        # ê²½ë ¥ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì‹ ê·œ)
        metadata['career_quality_score'] = self._calculate_career_quality_score(metadata)
        
        return metadata
    
    def _analyze_career_continuity(self, activity_years: List[int], career_years: List[int]) -> str:
        """ê²½ë ¥ ì—°ì†ì„± ë¶„ì„"""
        if not activity_years or not career_years:
            return "insufficient_data"
        
        # í™œë™ ì—°ë„ì˜ ì—°ì†ì„± í™•ì¸
        activity_gaps = []
        for i in range(1, len(activity_years)):
            gap = activity_years[i] - activity_years[i-1]
            if gap > 1:
                activity_gaps.append(gap)
        
        # ê²½ë ¥ ì—°ì°¨ì˜ ì—°ì†ì„± í™•ì¸
        career_gaps = []
        for i in range(1, len(career_years)):
            gap = career_years[i] - career_years[i-1]
            if gap > 1:
                career_gaps.append(gap)
        
        if not activity_gaps and not career_gaps:
            return "continuous"
        elif len(activity_gaps) <= 1 and len(career_gaps) <= 1:
            return "mostly_continuous"
        else:
            return "fragmented"
    
    def _calculate_career_quality_score(self, metadata: Dict[str, Any]) -> float:
        """ê²½ë ¥ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100ì )"""
        score = 0.0
        
        # ê²½ë ¥ ê¸°ê°„ ì ìˆ˜ (30ì )
        total_years = metadata.get('total_experience_years', 0)
        if total_years >= 10:
            score += 30
        elif total_years >= 5:
            score += 20
        elif total_years >= 2:
            score += 10
        
        # ë„ë©”ì¸ ì „ë¬¸ì„± ì ìˆ˜ (20ì )
        domain_diversity = metadata.get('domain_diversity', 0)
        if domain_diversity >= 3:
            score += 20
        elif domain_diversity >= 2:
            score += 15
        elif domain_diversity >= 1:
            score += 10
        
        # ìŠ¤í‚¬ ë‹¤ì–‘ì„± ì ìˆ˜ (20ì )
        skill_diversity = metadata.get('skill_diversity_score', 0)
        if skill_diversity >= 5:
            score += 20
        elif skill_diversity >= 3:
            score += 15
        elif skill_diversity >= 1:
            score += 10
        
        # ë¦¬ë”ì‹­ ê²½í—˜ ì ìˆ˜ (15ì )
        if metadata.get('is_leadership_track', False):
            score += 15
        
        # ê²½ë ¥ ì—°ì†ì„± ì ìˆ˜ (15ì )
        continuity = metadata.get('career_continuity', 'insufficient_data')
        if continuity == 'continuous':
            score += 15
        elif continuity == 'mostly_continuous':
            score += 10
        elif continuity == 'fragmented':
            score += 5
        
        return min(score, 100.0)
    
    def _categorize_experience_level(self, max_year: int) -> str:
        """ê²½ë ¥ ë ˆë²¨ ë¶„ë¥˜"""
        if max_year <= 3:
            return "junior"
        elif max_year <= 7:
            return "mid-level"
        elif max_year <= 15:
            return "senior"
        else:
            return "expert"
    
    def create_fixed_documents(self, employee_groups: Dict[str, pd.DataFrame]) -> List[Document]:
        """ìˆ˜ì •ëœ ê·¸ë£¹í•‘ ë¡œì§ìœ¼ë¡œ Document ìƒì„±"""
        documents = []
        
        for emp_id, emp_df in employee_groups.items():
            try:
                # í†µí•©ëœ ê²½ë ¥ íƒ€ì„ë¼ì¸ í…ìŠ¤íŠ¸ ìƒì„±
                timeline_text = self.create_integrated_career_timeline(emp_df, emp_id)
                
                # í¬ê´„ì  ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = self.create_comprehensive_metadata(emp_id, emp_df)
                
                # ë¬¸ì„œ í¬ê¸° ì²´í¬ ë° ë¶„í• 
                if len(timeline_text) <= 1500:
                    # ë‹¨ì¼ ë¬¸ì„œ
                    doc = Document(
                        page_content=timeline_text,
                        metadata=metadata
                    )
                    documents.append(doc)
                    
                else:
                    # í° ë¬¸ì„œëŠ” ë¶„í•  (í•˜ì§€ë§Œ ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€)
                    split_texts = self.text_splitter.split_text(timeline_text)
                    self.logger.info(f"{emp_id}: {len(split_texts)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
                    
                    for i, split_text in enumerate(split_texts):
                        chunk_metadata = metadata.copy()
                        chunk_metadata.update({
                            'chunk_index': i,
                            'total_chunks': len(split_texts),
                            'chunk_size': len(split_text),
                            'original_document_size': len(timeline_text)
                        })
                        
                        chunk_doc = Document(
                            page_content=split_text,
                            metadata=chunk_metadata
                        )
                        documents.append(chunk_doc)
                
            except Exception as e:
                self.logger.error(f"{emp_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì´ {len(documents)}ê°œ Document ìƒì„± ì™„ë£Œ")
        return documents
    
    def rebuild_vectorstore_with_fixed_grouping(self) -> Dict[str, Any]:
        """ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ VectorStore ì¬êµ¬ì¶•"""
        result = {
            "steps_completed": [],
            "errors": [],
            "success": False,
            "build_summary": {}
        }
        
        try:
            # 1. ê¸°ì¡´ VectorStore ë° ìºì‹œ ì‚­ì œ
            self.logger.info("ê¸°ì¡´ VectorStore ì‚­ì œ ì¤‘...")
            if os.path.exists(self.persist_directory):
                shutil.rmtree(self.persist_directory)
            if os.path.exists(self.cache_directory):
                shutil.rmtree(self.cache_directory)
            result["steps_completed"].append("ê¸°ì¡´ ë°ì´í„° ì‚­ì œ")
            
            # 2. ë””ë ‰í† ë¦¬ ì¬ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            os.makedirs(self.cache_directory, exist_ok=True)
            os.makedirs(os.path.dirname(self.docs_json_path), exist_ok=True)
            
            # 3. ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
            self.logger.info("ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            employee_groups = self.load_and_group_career_data()
            result["steps_completed"].append("ê°œì¸ë³„ ë°ì´í„° ê·¸ë£¹í•‘")
            
            # 4. ìˆ˜ì •ëœ Document ìƒì„±
            self.logger.info("í†µí•© Document ìƒì„± ì¤‘...")
            documents = self.create_fixed_documents(employee_groups)
            result["steps_completed"].append("í†µí•© Document ìƒì„±")
            
            # 5. JSON íŒŒì¼ ì €ì¥
            self.logger.info("docs JSON íŒŒì¼ ì €ì¥ ì¤‘...")
            with open(self.docs_json_path, 'w', encoding='utf-8') as f:
                json_docs = [
                    {"page_content": doc.page_content, "metadata": doc.metadata}
                    for doc in documents
                ]
                json.dump(json_docs, f, ensure_ascii=False, indent=2)
            result["steps_completed"].append("docs JSON ì €ì¥")
            
            # 6. ChromaDB êµ¬ì¶•
            self.logger.info("ChromaDB êµ¬ì¶• ì¤‘...")
            
            # ë©”íƒ€ë°ì´í„° ì •ì œ (ChromaDB í˜¸í™˜ì„±)
            filtered_documents = []
            for doc in documents:
                clean_metadata = {}
                for key, value in doc.metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        clean_metadata[key] = value
                    elif isinstance(value, list) and all(isinstance(item, str) for item in value):
                        clean_metadata[key] = ", ".join(value[:5])
                    elif value is not None:
                        clean_metadata[key] = str(value)
                
                filtered_doc = Document(
                    page_content=doc.page_content,
                    metadata=clean_metadata
                )
                filtered_documents.append(filtered_doc)
            
            # VectorStore ìƒì„±
            vector_store = Chroma.from_documents(
                documents=filtered_documents,
                embedding=self.cached_embeddings,
                persist_directory=self.persist_directory,
                collection_name="career_history"
            )
            
            vector_store.persist()
            result["steps_completed"].append("ChromaDB êµ¬ì¶•")
            
            # 7. êµ¬ì¶• ì •ë³´ ìš”ì•½
            result["build_summary"] = {
                'total_employees': len(employee_groups),
                'total_documents': len(filtered_documents),
                'persist_directory': self.persist_directory,
                'build_timestamp': datetime.now().isoformat(),
                'grouping_fixed': True,
                'employee_sample': {
                    emp_id: {
                        'rows': len(group),
                        'years': f"{int(group['ì—°ì°¨'].min())}-{int(group['ì—°ì°¨'].max())}" if 'ì—°ì°¨' in group.columns and not group['ì—°ì°¨'].isna().all() else "N/A"
                    } for emp_id, group in list(employee_groups.items())[:3]
                }
            }
            
            result["success"] = True
            self.logger.info("VectorStore ì¬êµ¬ì¶• ì™„ë£Œ!")
            
        except Exception as e:
            error_msg = f"VectorStore ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}"
            result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        return result
    
    def verify_fix(self, target_user: str = "EMP-525170") -> Dict[str, Any]:
        """ìˆ˜ì • ê²°ê³¼ ê²€ì¦ (ì—°ë„ ì •ë³´ í¬í•¨)"""
        verification = {
            "target_user": target_user,
            "vectorstore_test": {},
            "grouping_verification": {},
            "year_analysis_verification": {},
            "fix_success": False
        }
        
        try:
            # VectorStore ë¡œë“œ í…ŒìŠ¤íŠ¸
            if os.path.exists(self.persist_directory):
                vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.cached_embeddings,
                    collection_name="career_history"
                )
                
                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = vectorstore.similarity_search(target_user, k=2)
                verification["vectorstore_test"] = {
                    "search_successful": True,
                    "results_count": len(results),
                    "results_analysis": []
                }
                
                for i, doc in enumerate(results):
                    # ê¸°ë³¸ ë¬¸ì„œ ë¶„ì„
                    content = doc.page_content
                    
                    analysis = {
                        "document_index": i,
                        "content_length": len(content),
                        "appears_integrated": "ì´ ê²½ë ¥ ê¸°ë¡:" in content and content.count("â–£") > 1,
                        "has_year_info": "í™œë™ ì—°ë„:" in content,
                        "has_career_progression": "ê²½ë ¥ ë°œì „:" in content,
                        "metadata_quality": {
                            "employee_id": doc.metadata.get("employee_id", "ì—†ìŒ"),
                            "activity_start_year": doc.metadata.get("activity_start_year", "ì—†ìŒ"),
                            "activity_end_year": doc.metadata.get("activity_end_year", "ì—†ìŒ"),
                            "total_experience_years": doc.metadata.get("total_experience_years", "ì—†ìŒ"),
                            "career_quality_score": doc.metadata.get("career_quality_score", "ì—†ìŒ"),
                            "domains": doc.metadata.get("domains", "ì—†ìŒ"),
                            "roles": doc.metadata.get("roles", "ì—†ìŒ")
                        }
                    }
                    verification["vectorstore_test"]["results_analysis"].append(analysis)
                
                # ê·¸ë£¹í•‘ ê²€ì¦
                if results:
                    first_result = verification["vectorstore_test"]["results_analysis"][0]
                    verification["grouping_verification"] = {
                        "metadata_properly_extracted": first_result["metadata_quality"]["employee_id"] != "ì—†ìŒ",
                        "content_appears_integrated": first_result["appears_integrated"],
                        "has_reasonable_content": first_result["content_length"] > 100
                    }
                    
                    # ì—°ë„ ë¶„ì„ ê²€ì¦ (ì‹ ê·œ)
                    verification["year_analysis_verification"] = {
                        "year_info_present": first_result["has_year_info"],
                        "career_progression_analyzed": first_result["has_career_progression"],
                        "activity_year_metadata": first_result["metadata_quality"]["activity_start_year"] != "ì—†ìŒ",
                        "career_quality_calculated": first_result["metadata_quality"]["career_quality_score"] != "ì—†ìŒ"
                    }
                    
                    # ì „ì²´ì  ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                    basic_success = all([
                        verification["grouping_verification"]["metadata_properly_extracted"],
                        verification["grouping_verification"]["content_appears_integrated"],
                        verification["grouping_verification"]["has_reasonable_content"]
                    ])
                    
                    year_analysis_success = any([
                        verification["year_analysis_verification"]["year_info_present"],
                        verification["year_analysis_verification"]["activity_year_metadata"],
                        verification["year_analysis_verification"]["career_quality_calculated"]
                    ])
                    
                    verification["fix_success"] = basic_success and year_analysis_success
            
        except Exception as e:
            verification["error"] = f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
        
        return verification

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ VectorDB ê·¸ë£¹í•‘ ë¬¸ì œ ìë™ ìˆ˜ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤... (v2 - ì—°ë„ ë¶„ì„ í¬í•¨)")
    
    fixer = VectorDBGroupingFixer()
    
    # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
    print("\n1ï¸âƒ£ í˜„ì¬ VectorDB ìƒíƒœ í™•ì¸...")
    current_verification = fixer.verify_fix()
    
    if current_verification.get("fix_success"):
        print("âœ… VectorDBê°€ ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        # ì—°ë„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        if "year_analysis_verification" in current_verification:
            year_analysis = current_verification["year_analysis_verification"]
            print("ğŸ“… ì—°ë„ ë¶„ì„ ìƒíƒœ:")
            for key, value in year_analysis.items():
                status = "âœ…" if value else "âŒ"
                print(f"  {status} {key}: {value}")
        return
    
    # 2. VectorStore ì¬êµ¬ì¶•
    print("\n2ï¸âƒ£ ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ VectorStore ì¬êµ¬ì¶• ì¤‘... (ì—°ë„ ì •ë³´ í¬í•¨)")
    rebuild_result = fixer.rebuild_vectorstore_with_fixed_grouping()
    
    # 3. ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì¬êµ¬ì¶• ê²°ê³¼:")
    print(f"   - ì„±ê³µ ì—¬ë¶€: {'âœ…' if rebuild_result['success'] else 'âŒ'}")
    print(f"   - ì™„ë£Œëœ ë‹¨ê³„: {len(rebuild_result['steps_completed'])}ê°œ")
    
    for step in rebuild_result['steps_completed']:
        print(f"     âœ“ {step}")
    
    if rebuild_result['errors']:
        print(f"   - ì˜¤ë¥˜: {len(rebuild_result['errors'])}ê°œ")
        for error in rebuild_result['errors']:
            print(f"     âŒ {error}")
    
    if rebuild_result['success']:
        build_summary = rebuild_result['build_summary']
        print(f"\nğŸ“ˆ êµ¬ì¶• ìš”ì•½:")
        print(f"   - ì²˜ë¦¬ëœ ì‚¬ìš©ì: {build_summary['total_employees']}ëª…")
        print(f"   - ìƒì„±ëœ ë¬¸ì„œ: {build_summary['total_documents']}ê°œ")
        print(f"   - ê·¸ë£¹í•‘ ìˆ˜ì •: {'ì˜ˆ' if build_summary['grouping_fixed'] else 'ì•„ë‹ˆì˜¤'}")
        print(f"   - ì—°ë„ ë¶„ì„ í¬í•¨: ì˜ˆ")
        
        # ìƒ˜í”Œ ì‚¬ìš©ì ì •ë³´
        if build_summary['employee_sample']:
            print(f"   - ìƒ˜í”Œ ì‚¬ìš©ì:")
            for emp_id, info in build_summary['employee_sample'].items():
                print(f"     â€¢ {emp_id}: {info['rows']}í–‰ â†’ {info['years']}")

if __name__ == "__main__":
    main()