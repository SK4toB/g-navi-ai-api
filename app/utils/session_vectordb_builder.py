# app/utils/session_vectordb_builder.py
"""
ğŸ—ƒï¸ ì‚¬ìš©ìë³„ ì±„íŒ… ì„¸ì…˜ ëŒ€í™”ë‚´ì—­ VectorDB êµ¬ì¶• ì‹œìŠ¤í…œ

ğŸ“‹ í•µì‹¬ ê¸°ëŠ¥:
1. ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì‚¬ìš©ìì˜ current_session_messagesë¥¼ VectorDBë¡œ ìë™ êµ¬ì¶•
2. ì‚¬ìš©ìë³„(member_id) ë¶„ë¦¬ëœ VectorDB ì €ì¥ â†’ ê°œì¸ì •ë³´ ë³´í˜¸ ë° ê°œì¸í™” ê²€ìƒ‰
3. OpenAI Embeddings + ChromaDBë¥¼ í™œìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
4. ê³¼ê±° ëŒ€í™” ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

ğŸ”„ ë™ì‘ í”Œë¡œìš°:
ì„¸ì…˜ ì¢…ë£Œ â†’ current_session_messages ìˆ˜ì§‘ â†’ ìŠ¤ë§ˆíŠ¸ ê·œì¹™ê¸°ë°˜ ìš”ì•½ â†’ VectorDB êµ¬ì¶• â†’ í–¥í›„ ê²€ìƒ‰ í™œìš©

ğŸ“ ì €ì¥ êµ¬ì¡°:
storage/vector_stores/user_{member_id}_sessions/
â”œâ”€â”€ chroma.sqlite3              # ChromaDB ë²¡í„° ì €ì¥ì†Œ
â”œâ”€â”€ session_index.json          # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤
â””â”€â”€ ê¸°íƒ€ ChromaDB íŒŒì¼ë“¤

âš ï¸ ì¤‘ìš” ì‚¬í•­:
- ê° ì‚¬ìš©ìë³„ë¡œ ì™„ì „íˆ ë¶„ë¦¬ëœ VectorDB ìƒì„± (íƒ€ ì‚¬ìš©ì ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€)
- ì„¸ì…˜ ì¢…ë£Œ ì‹œì—ë§Œ VectorDB êµ¬ì¶• (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì•„ë‹˜)
- ê²€ìƒ‰ ì‹œ ê´€ë ¨ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ í’ˆì§ˆ ë³´ì¥
"""

import os
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

class SessionVectorDBBuilder:
    """
    ğŸ—ƒï¸ ì‚¬ìš©ìë³„ ì±„íŒ… ì„¸ì…˜ ëŒ€í™”ë‚´ì—­ VectorDB êµ¬ì¶• ë° ê´€ë¦¬ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì±„íŒ… ì„¸ì…˜ì´ ì¢…ë£Œë  ë•Œ í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë‚´ì—­ì„
    ì‚¬ìš©ìë³„ë¡œ ë¶„ë¦¬ëœ VectorDBì— ì €ì¥í•˜ì—¬, í–¥í›„ ê°œì¸í™”ëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ì£¼ìš” ì±…ì„:
    - ì„¸ì…˜ ì¢…ë£Œ ì‹œ current_session_messages â†’ VectorDB ìë™ êµ¬ì¶•
    - ì‚¬ìš©ìë³„ VectorDB ë¶„ë¦¬ ê´€ë¦¬ (member_id ê¸°ì¤€)
    - ê³¼ê±° ëŒ€í™” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
    - ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ë° í†µê³„ ê´€ë¦¬
    """
    
    def __init__(self):
        """
        VectorDB êµ¬ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        ì„¤ì • ë‚´ìš©:
        - ì €ì¥ ê²½ë¡œ: app/storage/vector_stores/
        - ì„ë² ë”© ëª¨ë¸: OpenAI text-embedding-3-small
        - í…ìŠ¤íŠ¸ ë¶„í• : 1000ì ì²­í¬, 200ì ì˜¤ë²„ë©
        """
        # ğŸ“ VectorDB ì €ì¥ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìë³„ í´ë”ë¡œ ë¶„ë¦¬ë¨)
        self.storage_path = Path(__file__).parent.parent / "storage" / "vector_stores"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # ğŸ¤– OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"  # ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸
        )
        
        # âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì„¤ì • (ê¸´ ëŒ€í™”ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,    # ê° ì²­í¬ ìµœëŒ€ 1000ì
            chunk_overlap=200,  # ì²­í¬ ê°„ 200ì ì¤‘ë³µ (ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´)
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]  # ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í• ì 
        )
        
        print(f"SessionVectorDBBuilder ì´ˆê¸°í™” ì™„ë£Œ (ì €ì¥ê²½ë¡œ: {self.storage_path})")
    
    async def summarize_session_content(self, messages: List[Dict[str, Any]], user_name: str) -> str:
        """
        ğŸ“ ì„¸ì…˜ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
        
        Args:
            messages: ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ [{"role": "user/assistant", "content": "..."}]
            user_name: ì‚¬ìš©ì ì´ë¦„
            
        Returns:
            str: "ì‚¬ìš©ì {ì´ë¦„}ì˜ {ì„¸ì…˜ìœ í˜•} - Nê°œ ì§ˆë¬¸, Mê°œ ì‘ë‹µ | ì£¼ì œ: í‚¤ì›Œë“œë“¤"
            
        ğŸ’¡ ê¸°ëŠ¥:
            - ì„¸ì…˜ ìœ í˜• ìë™ ë¶„ë¥˜ (ì»¤ë¦¬ì–´ìƒë‹´, ê¸°ìˆ í•™ìŠµ, ì°½ì—…ìƒë‹´ ë“±)
            - ì‹¤ì œ ë©”ì‹œì§€ ê°œìˆ˜ ì •í™• ê³„ì‚°
            - ë„ë©”ì¸ë³„ ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ
            - VectorDB ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ ìš”ì•½
        """
        try:
            if not messages:
                return f"ì‚¬ìš©ì {user_name}ì˜ ë¹ˆ ëŒ€í™” ì„¸ì…˜"
            
            # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ì„ ì¤€ë¹„
            conversation_text = self._format_messages_to_text(messages)
            
            if not conversation_text.strip():
                return f"ì‚¬ìš©ì {user_name}ì˜ ë¹ˆ ëŒ€í™” ì„¸ì…˜"
            
            # ìŠ¤ë§ˆíŠ¸ ê·œì¹™ ê¸°ë°˜ ìš”ì•½ ìƒì„±
            summary = await self._generate_smart_summary(conversation_text, user_name, messages)
            
            return summary
            
        except Exception as e:
            print(f"ì„¸ì…˜ ë‚´ìš© ìš”ì•½ ì‹¤íŒ¨: {e}")
            return f"ì‚¬ìš©ì {user_name}ì˜ ëŒ€í™” ì„¸ì…˜ (ìš”ì•½ ì‹¤íŒ¨)"
    
    def _format_messages_to_text(self, messages: List[Dict[str, Any]]) -> str:
        """ë©”ì‹œì§€ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        for msg in messages:
            if isinstance(msg, dict):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                
                if role == 'user':
                    text_parts.append(f"ì‚¬ìš©ì: {content}")
                elif role == 'assistant':
                    text_parts.append(f"AI: {content}")
                elif role == 'system':
                    text_parts.append(f"ì‹œìŠ¤í…œ: {content}")
                else:
                    text_parts.append(f"{role}: {content}")
        
        return "\n".join(text_parts)
    
    async def _generate_smart_summary(self, conversation_text: str, user_name: str, messages: List[Dict[str, Any]]) -> str:
        """
        ğŸ§  ìŠ¤ë§ˆíŠ¸ ê·œì¹™ ê¸°ë°˜ ëŒ€í™” ìš”ì•½ ìƒì„±
        
        Args:
            conversation_text: ì „ì²´ ëŒ€í™” í…ìŠ¤íŠ¸
            user_name: ì‚¬ìš©ì ì´ë¦„
            messages: ì›ë³¸ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: êµ¬ì¡°í™”ëœ ì„¸ì…˜ ìš”ì•½
            
        ğŸ’¡ ì²˜ë¦¬ ê³¼ì •:
            1. ì‹¤ì œ ë©”ì‹œì§€ ê°œìˆ˜ ì •í™• ê³„ì‚° (user vs assistant)
            2. ëŒ€í™” ì£¼ì œ ë° ì„¸ì…˜ ìœ í˜• ìë™ ë¶„ì„
            3. ë„ë©”ì¸ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ
            4. ì„¸ì…˜ ê¸¸ì´ì— ë”°ë¥¸ ì ì‘í˜• ìš”ì•½ ìƒì„±
        """
        try:
            # ğŸ” ë””ë²„ê¹…: ë©”ì‹œì§€ ë¶„ì„ ìƒì„¸ ë¡œê·¸
            print(f"   ğŸ“Š ë©”ì‹œì§€ ë¶„ì„ ì‹œì‘:")
            print(f"     ì „ì²´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
            
            # ë©”ì‹œì§€ ìœ í˜•ë³„ ì¹´ìš´íŒ… ë° ìƒì„¸ ë¶„ì„
            user_messages = []
            assistant_messages = []
            system_messages = []
            other_messages = []
            
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                print(f"     #{i+1} {role}: {content[:50]}{'...' if len(content) > 50 else ''}")
                
                if role == 'user':
                    user_messages.append(msg)
                elif role == 'assistant':
                    assistant_messages.append(msg)
                elif role == 'system':
                    system_messages.append(msg)
                else:
                    other_messages.append(msg)
            
            user_count = len(user_messages)
            assistant_count = len(assistant_messages)
            
            print(f"     ğŸ“ˆ ì¹´ìš´íŒ… ê²°ê³¼:")
            print(f"       ì‚¬ìš©ì ë©”ì‹œì§€: {user_count}ê°œ")
            print(f"       AI ì‘ë‹µ: {assistant_count}ê°œ")
            print(f"       ì‹œìŠ¤í…œ ë©”ì‹œì§€: {len(system_messages)}ê°œ")
            print(f"       ê¸°íƒ€ ë©”ì‹œì§€: {len(other_messages)}ê°œ")
            
            # ëŒ€í™” ì£¼ì œ ë° ì„¸ì…˜ ìœ í˜• ë¶„ì„
            topic_analysis = self._analyze_conversation_topics(conversation_text, messages)
            
            # ì„¸ì…˜ ê¸¸ì´ì— ë”°ë¥¸ ì ì‘í˜• ìš”ì•½ ìƒì„±
            if len(messages) >= 10:
                # ê¸´ ëŒ€í™”: ìƒì„¸í•œ ìš”ì•½ (ì£¼ì œ + í‚¤ì›Œë“œ)
                summary = f"ì‚¬ìš©ì {user_name}ì˜ {topic_analysis['session_type']} - "
                summary += f"{user_count}ê°œ ì§ˆë¬¸, {assistant_count}ê°œ ì‘ë‹µ"
                
                if topic_analysis['main_topics']:
                    summary += f" | ì£¼ì œ: {', '.join(topic_analysis['main_topics'][:3])}"
                    
                if topic_analysis['keywords']:
                    summary += f" | í‚¤ì›Œë“œ: {', '.join(topic_analysis['keywords'][:3])}"
            else:
                # ì§§ì€ ëŒ€í™”: ê°„ê²°í•œ ìš”ì•½ (ì£¼ì œ ì¤‘ì‹¬)
                summary = f"ì‚¬ìš©ì {user_name}ì˜ {topic_analysis['session_type']} - "
                summary += f"{user_count}ê°œ ì§ˆë¬¸, {assistant_count}ê°œ ì‘ë‹µ"
                
                if topic_analysis['main_topics']:
                    summary += f" | ì£¼ì œ: {', '.join(topic_analysis['main_topics'][:2])}"
            
            print(f"   âœ… ìƒì„±ëœ ìš”ì•½: {summary}")
            return summary
            
        except Exception as e:
            print(f"ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ìš”ì•½ ìƒì„±
            user_count = len([msg for msg in messages if msg.get('role') == 'user'])
            assistant_count = len([msg for msg in messages if msg.get('role') == 'assistant'])
            return f"ì‚¬ìš©ì {user_name}ì˜ ëŒ€í™”ì„¸ì…˜ - {user_count}ê°œ ì§ˆë¬¸, {assistant_count}ê°œ ì‘ë‹µ"
    
    def _analyze_conversation_topics(self, conversation_text: str, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ğŸ” ê°œì„ ëœ ëŒ€í™” ë‚´ìš© ë¶„ì„ - ë§¥ë½ê³¼ ì£¼ì œ ì „í™˜ì„ ë” ì •í™•íˆ íŒŒì•…
        
        Args:
            conversation_text: ì „ì²´ ëŒ€í™” í…ìŠ¤íŠ¸
            messages: ì›ë³¸ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: {
                'session_type': 'ì„¸ì…˜ ìœ í˜•',
                'main_topics': ['í•µì‹¬ ì£¼ì œ ë¦¬ìŠ¤íŠ¸'],
                'keywords': ['í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸'],
                'message_count': ë©”ì‹œì§€ ìˆ˜,
                'complexity_indicators': ['ë³µì¡ì„± ì§€í‘œ']
            }
            
        ğŸ’¡ ê°œì„ ëœ ë¶„ì„ ê³¼ì •:
            1. ì£¼ì œ ì „í™˜ íŒ¨í„´ ê°ì§€
            2. ë¶€ì •ì  í‘œí˜„ê³¼ ê¸ì •ì  í‘œí˜„ êµ¬ë¶„
            3. ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì£¼ì œ ë³€í™” ì¶”ì 
            4. ë³µí•© ì£¼ì œ ì‹ë³„
        """
        try:
            # ğŸ’¼ ê¸°ë³¸ ì„¸ì…˜ ìœ í˜• ë¶„ì„ (ê¸°ì¡´ ë¡œì§)
            session_types = {
                'ì»¤ë¦¬ì–´ìƒë‹´': ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì·¨ì—…', 'ì´ì§', 'ì„±ì¥', 'ê°œë°œì', 'ì§ì—…', 'ë¶„ì•¼', 'í¬ì§€ì…˜'],
                'ê¸°ìˆ í•™ìŠµ': ['React', 'Python', 'í”„ë¡œê·¸ë˜ë°', 'ê°œë°œ', 'ì½”ë”©', 'ì–¸ì–´', 'í”„ë ˆì„ì›Œí¬', 'ê¸°ìˆ '],
                'ì°½ì—…ìƒë‹´': ['ì°½ì—…', 'ì‚¬ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'íšŒì‚¬'],
                'êµìœ¡ê³¼ì •': ['ê°•ì˜', 'ìˆ˜ì—…', 'í•™ìŠµ', 'êµìœ¡', 'ê³¼ì •', 'ì½”ìŠ¤'],
                'ì¼ë°˜ìƒë‹´': []  # ê¸°ë³¸ê°’
            }
            
            # ğŸ”„ ì£¼ì œ ì „í™˜ ê°ì§€ (ê°œì„ ë¨)
            topic_progression = self._track_topic_progression(messages, session_types)
            
            # ğŸ“Š ì£¼ìš” ì„¸ì…˜ ìœ í˜• ê²°ì • (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
            session_type = self._determine_primary_session_type(topic_progression, conversation_text)
            
            # ğŸ”‘ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_contextual_keywords(conversation_text, session_type)
            
            # ğŸ¯ ë„ë©”ì¸ë³„ íŠ¹í™” ì£¼ì œ ì‹ë³„ (ê¸°ì¡´ ë¡œì§ ê°œì„ )
            main_topics = self._identify_specialized_topics(session_type, keywords, conversation_text)
            
            # âš ï¸ ë³µì¡ì„± ì§€í‘œ ì‹ë³„
            complexity_indicators = self._identify_complexity_indicators(conversation_text, messages)
            
            return {
                'session_type': session_type,
                'main_topics': main_topics[:3],
                'keywords': keywords[:5],
                'message_count': len(messages),
                'topic_progression': topic_progression,
                'complexity_indicators': complexity_indicators
            }
            
        except Exception as e:
            print(f"ëŒ€í™” ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'session_type': 'ëŒ€í™”ì„¸ì…˜',
                'main_topics': [],
                'keywords': [],
                'message_count': len(messages),
                'topic_progression': [],
                'complexity_indicators': []
            }
    
    def _track_topic_progression(self, messages: List[Dict[str, Any]], session_types: Dict[str, List[str]]) -> List[str]:
        """ğŸ”„ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì£¼ì œ ë³€í™” ì¶”ì """
        progression = []
        
        for msg in messages:
            if msg.get('role') != 'user':
                continue
                
            content = msg.get('content', '')
            msg_topics = []
            
            # ê° ë©”ì‹œì§€ë³„ ì£¼ì œ ì‹ë³„
            for type_name, keywords in session_types.items():
                if any(keyword in content for keyword in keywords):
                    msg_topics.append(type_name)
            
            if msg_topics:
                progression.append(msg_topics[0])
            elif progression:  # ì´ì „ ì£¼ì œ ìœ ì§€
                progression.append(progression[-1])
            else:
                progression.append('ì¼ë°˜ìƒë‹´')
        
        return progression
    
    def _determine_primary_session_type(self, topic_progression: List[str], conversation_text: str) -> str:
        """ğŸ“Š ì£¼ì œ ì§„í–‰ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ìš” ì„¸ì…˜ ìœ í˜• ê²°ì •"""
        if not topic_progression:
            return 'ì¼ë°˜ìƒë‹´'
        
        # ì£¼ì œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        topic_weights = {}
        total_messages = len(topic_progression)
        
        for i, topic in enumerate(topic_progression):
            # ë‚˜ì¤‘ì— ë‚˜ì˜¨ ì£¼ì œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ (ìµœê·¼ ì£¼ì œê°€ ë” ì¤‘ìš”)
            weight = (i + 1) / total_messages * 1.5 + 0.5
            topic_weights[topic] = topic_weights.get(topic, 0) + weight
        
        # ë³µí•© ì£¼ì œ ê°ì§€
        if len(set(topic_progression)) > 1:
            # ì£¼ì œ ì „í™˜ì´ ìˆëŠ” ê²½ìš°
            dominant_topic = max(topic_weights, key=topic_weights.get)
            
            # ì°½ì—…+ê¸°ìˆ  ì¡°í•© ê°ì§€
            if ('ì°½ì—…ìƒë‹´' in topic_weights and 'ê¸°ìˆ í•™ìŠµ' in topic_weights):
                return 'ì°½ì—…-ê¸°ìˆ  ë³µí•©ìƒë‹´'
            elif ('ì»¤ë¦¬ì–´ìƒë‹´' in topic_weights and 'ê¸°ìˆ í•™ìŠµ' in topic_weights):
                return 'ì»¤ë¦¬ì–´-ê¸°ìˆ  ë³µí•©ìƒë‹´'
            else:
                return dominant_topic
        else:
            return topic_progression[0]
    
    def _extract_contextual_keywords(self, conversation_text: str, session_type: str) -> List[str]:
        """ğŸ”‘ ì„¸ì…˜ ìœ í˜•ì— ë§ëŠ” ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        basic_keywords = self._extract_keywords(conversation_text)
        
        # ì„¸ì…˜ ìœ í˜•ë³„ íŠ¹í™” í‚¤ì›Œë“œ ë³´ê°•
        context_keywords = []
        
        if 'ë³µí•©ìƒë‹´' in session_type:
            # ë³µí•© ìƒë‹´ì˜ ê²½ìš° ë‘ ì˜ì—­ í‚¤ì›Œë“œ ëª¨ë‘ ì¤‘ìš”
            tech_patterns = ['ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'React', 'Python', 'ê¸°ìˆ ']
            career_patterns = ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì°½ì—…', 'ì´ì§']
            
            for keyword in basic_keywords:
                if any(pattern in keyword for pattern in tech_patterns + career_patterns):
                    context_keywords.append(keyword)
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ë³¸ í‚¤ì›Œë“œ
        return context_keywords[:5] if len(context_keywords) >= 3 else basic_keywords[:5]
    
    def _identify_specialized_topics(self, session_type: str, keywords: List[str], conversation_text: str) -> List[str]:
        """ğŸ¯ ê°œì„ ëœ ë„ë©”ì¸ë³„ íŠ¹í™” ì£¼ì œ ì‹ë³„"""
        main_topics = []
        
        if session_type == 'ì»¤ë¦¬ì–´ìƒë‹´' or 'ì»¤ë¦¬ì–´' in session_type:
            career_topics = {
                'ë°±ì—”ë“œê°œë°œ': ['ë°±ì—”ë“œ', 'Django', 'FastAPI', 'API', 'ì„œë²„'],
                'í”„ë¡ íŠ¸ì—”ë“œ': ['í”„ë¡ íŠ¸ì—”ë“œ', 'React', 'Vue', 'JavaScript', 'ì›¹'],
                'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤': ['ë°ì´í„°', 'ë¶„ì„', 'ë¨¸ì‹ ëŸ¬ë‹', 'AI', 'íŒŒì´ì¬'],
                'ì»¤ë¦¬ì–´ì „í™˜': ['ì „í™˜', 'ì´ì§', 'ë³€ê²½', 'ë°”ê¾¸ê³ '],
                'ì„±ì¥ê³ ë¯¼': ['ì„±ì¥', 'ë°œì „', 'ì‹¤ë ¥', 'ì—­ëŸ‰']
            }
            main_topics = self._match_topics_by_keywords(career_topics, keywords, conversation_text)
        
        elif session_type == 'ê¸°ìˆ í•™ìŠµ' or 'ê¸°ìˆ ' in session_type:
            tech_topics = {
                'Reactí•™ìŠµ': ['React', 'ì»´í¬ë„ŒíŠ¸', 'JSX', 'Hook'],
                'Pythonê¸°ì´ˆ': ['Python', 'íŒŒì´ì¬', 'ê¸°ì´ˆ', 'ë³€ìˆ˜'],
                'ì›¹ê°œë°œ': ['ì›¹', 'HTML', 'CSS', 'ê°œë°œ'],
                'AIí•™ìŠµ': ['AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥'],
                'í”„ë¡œê·¸ë˜ë°ì…ë¬¸': ['í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'ì…ë¬¸', 'ì‹œì‘']
            }
            main_topics = self._match_topics_by_keywords(tech_topics, keywords, conversation_text)
        
        elif session_type == 'ì°½ì—…ìƒë‹´' or 'ì°½ì—…' in session_type:
            business_topics = {
                'ê¸°ìˆ ì°½ì—…': ['ê¸°ìˆ ', 'ê°œë°œ', 'IT', 'ì•±'],
                'ì•„ì´ë””ì–´ê²€ì¦': ['ì•„ì´ë””ì–´', 'ê²€ì¦', 'ì‹œì¥'],
                'íŒ€ë¹Œë”©': ['íŒ€', 'êµ¬ì„±', 'ì¸ë ¥', 'ì±„ìš©'],
                'ì‚¬ì—…ê³„íš': ['ì‚¬ì—…', 'ê³„íš', 'ì „ëµ', 'ìˆ˜ìµ']
            }
            main_topics = self._match_topics_by_keywords(business_topics, keywords, conversation_text)
        
        # ë³µí•© ìƒë‹´ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if 'ë³µí•©ìƒë‹´' in session_type:
            if 'ì°½ì—…-ê¸°ìˆ ' in session_type:
                main_topics.append('ì°½ì—…ì„ ìœ„í•œ ê¸°ìˆ ì—­ëŸ‰')
            elif 'ì»¤ë¦¬ì–´-ê¸°ìˆ ' in session_type:
                main_topics.append('ê°œë°œ ì»¤ë¦¬ì–´ ê³ ë¯¼')
        
        # ì£¼ì œê°€ ì—†ìœ¼ë©´ ìƒìœ„ í‚¤ì›Œë“œë¡œ ëŒ€ì²´
        return main_topics[:3] if main_topics else keywords[:3]
    
    def _match_topics_by_keywords(self, topic_dict: Dict[str, List[str]], keywords: List[str], conversation_text: str) -> List[str]:
        """í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ì£¼ì œ ì‹ë³„"""
        matched_topics = []
        
        for topic, topic_keywords in topic_dict.items():
            # í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­
            keyword_matches = sum(1 for kw in keywords if any(tk in kw for tk in topic_keywords))
            # ëŒ€í™” í…ìŠ¤íŠ¸ ì§ì ‘ ë§¤ì¹­
            text_matches = sum(1 for tk in topic_keywords if tk in conversation_text)
            
            # ë§¤ì¹­ ì ìˆ˜ê°€ ì¶©ë¶„í•˜ë©´ ì£¼ì œë¡œ ì„ ì •
            if keyword_matches >= 1 or text_matches >= 2:
                matched_topics.append(topic)
        
        return matched_topics
    
    def _identify_complexity_indicators(self, conversation_text: str, messages: List[Dict[str, Any]]) -> List[str]:
        """âš ï¸ ëŒ€í™” ë³µì¡ì„± ì§€í‘œ ì‹ë³„"""
        indicators = []
        
        # ë¶€ì •ì  ê°ì • í‘œí˜„
        negative_patterns = ['ì–´ë ¤ì›Œ', 'í˜ë“¤ì–´', 'í¬ê¸°', 'ëª¨ë¥´ê² ì–´', 'ê±±ì •']
        if any(pattern in conversation_text for pattern in negative_patterns):
            indicators.append('ë¶€ì •ì  ê°ì •')
        
        # ì£¼ì œ ì „í™˜
        if len(set([msg.get('content', '')[:10] for msg in messages if msg.get('role') == 'user'])) > len(messages) // 2:
            indicators.append('ì£¼ì œ ë‹¤ì–‘ì„±')
        
        # ê¸´ ëŒ€í™”
        if len(messages) >= 8:
            indicators.append('ì¥ì‹œê°„ ëŒ€í™”')
        
        # ë³µí•© ì§ˆë¬¸
        complex_question_count = sum(1 for msg in messages 
                                   if msg.get('role') == 'user' and len(msg.get('content', '')) > 50)
        if complex_question_count >= 2:
            indicators.append('ë³µì¡í•œ ì§ˆë¬¸')
        
        return indicators
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ğŸ”‘ ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ - ëŒ€í™”ì—ì„œ ì˜ë¯¸ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë§Œ ì„ ë³„
        
        Args:
            text: ì „ì²´ ëŒ€í™” í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
            
        ğŸ’¡ ì¶”ì¶œ ì „ëµ:
            1. í¬ê´„ì ì¸ ë¶ˆìš©ì–´ í•„í„°ë§ (ì¡°ì‚¬, ì¼ë°˜ì  í‘œí˜„ ë“±)
            2. ê¸°ìˆ /ì»¤ë¦¬ì–´ ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            3. ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ (2ê¸€ì ì´ìƒ)
            4. ìµœì¢… 5ê°œ í‚¤ì›Œë“œ ì„ ë³„
        """
        # ğŸš« í¬ê´„ì ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ - í•œêµ­ì–´ ì¡°ì‚¬, ì¼ë°˜ì  í‘œí˜„, ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë“±
        common_words = {
            # í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€',
            # ì¼ë°˜ì ì¸ ëŒ€í™” í‘œí˜„
            'AI', 'ì‚¬ìš©ì', 'ì‹œìŠ¤í…œ', 'ì•ˆë…•í•˜ì„¸ìš”', 'ë‹˜', 'í•©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 
            'í•´ì£¼ì„¸ìš”', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ê·¸', 'ì €', 'ì œ', 'ê±°', 'ë„¤', 'ìš”', 'ì¢€', 'ë”', 'ì •ë§',
            'ì‚¬ì‹¤', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ë§Œì•½', 'í˜¹ì‹œ', 'ì•„ë§ˆ', 'íŠ¹íˆ', 'ì˜ˆë¥¼ë“¤ì–´', 'ë•Œë¬¸ì—',
            # ì‹œìŠ¤í…œ íŠ¹í™” í‘œí˜„ (G.Navi ê´€ë ¨)
            'ì˜¤í˜„ì§„ì˜', 'ì˜¤í˜„ì§„ë‹˜!', 'Growth', 'Navigatorì—', 'G.Navi', 'ì „ë¬¸', 'ìƒë‹´ì‚¬ì¸', 'í…ŒìŠ¤íŠ¸ì‚¬ìš©ìì˜',
            'ê°œë°œìê°€', 'ì‹¶ì–´ìš”.', 'ì•ˆë…•í•˜ì„¸ìš”!'
        }
        
        # ğŸ“ ë‹¨ì–´ ì¶”ì¶œ ë° ê¸°ë³¸ í•„í„°ë§
        import re
        words = re.findall(r'\b\w+\b', text)
        
        # ê¸°ë³¸ ì¡°ê±´ ë§Œì¡±í•˜ëŠ” í‚¤ì›Œë“œë§Œ ìˆ˜ì§‘
        filtered_keywords = []
        for word in words:
            if (len(word) > 1 and                    # 2ê¸€ì ì´ìƒ
                word not in common_words and         # ë¶ˆìš©ì–´ê°€ ì•„ë‹˜
                not word.isdigit() and               # ìˆ«ìê°€ ì•„ë‹˜
                word not in filtered_keywords):      # ì¤‘ë³µì´ ì•„ë‹˜
                filtered_keywords.append(word)
                if len(filtered_keywords) >= 15:     # ì¶©ë¶„í•œ í›„ë³´ ìˆ˜ì§‘
                    break
        
        # ğŸ¯ ì¤‘ìš”ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì„ ë³„
        priority_keywords = []
        
        # ê¸°ìˆ  ê´€ë ¨ ê³ ì¤‘ìš”ë„ í‚¤ì›Œë“œ
        tech_keywords = ['AI', 'ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'Python', 'ë°ì´í„°', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 
                        'React', 'JavaScript', 'ì½”ë”©', 'ê¸°ìˆ ', 'ìŠ¤í‚¬', 'ì–¸ì–´', 'í”„ë ˆì„ì›Œí¬']
        
        # ì»¤ë¦¬ì–´ ê´€ë ¨ ê³ ì¤‘ìš”ë„ í‚¤ì›Œë“œ  
        career_keywords = ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì·¨ì—…', 'ì´ì§', 'ì„±ì¥', 'ëª©í‘œ', 'ê³„íš', 'ë°©í–¥', 
                          'ê°œë°œì', 'ì§ì—…', 'ë¶„ì•¼', 'í¬ì§€ì…˜']
        
        # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì„ ë³„
        for keyword in filtered_keywords:
            if any(tech_word in keyword for tech_word in tech_keywords):
                priority_keywords.append(keyword)
            elif any(career_word in keyword for career_word in career_keywords):
                priority_keywords.append(keyword)
        
        # ğŸ”§ ìµœì¢… í‚¤ì›Œë“œ ê²°ì •: ìš°ì„ ìˆœìœ„ â†’ ì¼ë°˜ í‚¤ì›Œë“œ ìˆœ
        final_keywords = priority_keywords[:5] if priority_keywords else filtered_keywords[:5]
        
        # ğŸ› ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"   ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ìƒì„¸:")
        print(f"     ğŸ“Š ì „ì²´ ë‹¨ì–´: {len(words)}ê°œ")
        print(f"     âœ… í•„í„°ë§ í›„: {len(filtered_keywords)}ê°œ")
        print(f"     â­ ìš°ì„ ìˆœìœ„: {priority_keywords}")
        print(f"     ğŸ¯ ìµœì¢… ì„ íƒ: {final_keywords}")
        
        return final_keywords
    
    async def build_vector_db(self, 
                            conversation_id: str, 
                            member_id: str, 
                            user_name: str, 
                            messages: List[Dict[str, Any]],
                            session_metadata: Dict[str, Any]) -> bool:
        """
        ğŸ—ƒï¸ ì„¸ì…˜ ëŒ€í™” ë‚´ì—­ì„ ì‚¬ìš©ìë³„ VectorDBì— ì €ì¥í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ
        
        Args:
            conversation_id: ëŒ€í™”ë°© ê³ ìœ  ID (ì˜ˆ: "chat_session_123")
            member_id: ì‚¬ìš©ì ê³ ìœ  ID (VectorDB ë¶„ë¦¬ ê¸°ì¤€)
            user_name: ì‚¬ìš©ì ì´ë¦„ (ë©”íƒ€ë°ì´í„°ìš©)
            messages: ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ë“¤
            session_metadata: ì„¸ì…˜ ê¸°ë³¸ ì •ë³´ (ìƒì„±ì‹œê°„, ì§€ì†ì‹œê°„ ë“±)
            
        Returns:
            bool: VectorDB êµ¬ì¶• ì„±ê³µ ì—¬ë¶€
            
        ğŸ”„ ì²˜ë¦¬ ê³¼ì •:
        1. ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        2. ì‚¬ìš©ìë³„ VectorDB í´ë” ìƒì„±/ì ‘ê·¼
        3. ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹
        4. OpenAI Embeddingsë¡œ ë²¡í„°í™”
        5. ChromaDBì— ì €ì¥ + ë©”íƒ€ë°ì´í„° ì²¨ë¶€
        6. ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸
        
        ğŸ’¾ ì €ì¥ ìœ„ì¹˜: storage/vector_stores/user_{member_id}_sessions/
        """
        try:
            print(f"ğŸ—ƒï¸ build_vector_db ì‹œì‘: {conversation_id}")
            print(f"ğŸ“Š ì „ë‹¬ë°›ì€ messages ê°œìˆ˜: {len(messages) if messages else 0}ê°œ")
            print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_name} (member_id: {member_id})")
            
            if messages:
                print(f"ğŸ“‹ ì „ë‹¬ë°›ì€ messages ìƒì„¸:")
                for i, msg in enumerate(messages):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')[:50]
                    print(f"     #{i+1} {role}: {content}{'...' if len(msg.get('content', '')) > 50 else ''}")
            
            # âœ… 1ë‹¨ê³„: ë¹ˆ ì„¸ì…˜ ê²€ì¦
            if not messages:
                print(f"ë¹ˆ ë©”ì‹œì§€ ì„¸ì…˜ - VectorDB êµ¬ì¶• ìƒëµ: {conversation_id}")
                return False
            
            # ğŸ“ 2ë‹¨ê³„: ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´)
            summary = await self.summarize_session_content(messages, user_name)
            print(f"ì„¸ì…˜ ìš”ì•½ ìƒì„± ì™„ë£Œ: {conversation_id} - {summary}")
            
            # ğŸ“ 3ë‹¨ê³„: ì‚¬ìš©ìë³„ VectorDB ì €ì¥ ê²½ë¡œ ìƒì„±
            # ì¤‘ìš”: ê° ì‚¬ìš©ìë§ˆë‹¤ ì™„ì „íˆ ë¶„ë¦¬ëœ í´ë”ë¡œ ê°œì¸ì •ë³´ ë³´í˜¸
            user_db_path = self.storage_path / f"user_{member_id}_sessions"
            user_db_path.mkdir(parents=True, exist_ok=True)
            
            # ğŸ”¤ 4ë‹¨ê³„: ëŒ€í™” ë©”ì‹œì§€ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = self._format_messages_to_text(messages)
            
            # âœ‚ï¸ 5ë‹¨ê³„: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ì— ì í•©í•œ í¬ê¸°ë¡œ ë¶„í•  (ì²­í‚¹)
            chunks = self.text_splitter.split_text(conversation_text)
            
            if not chunks:
                print(f"ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ - VectorDB êµ¬ì¶• ìƒëµ: {conversation_id}")
                return False
            
            # ğŸ·ï¸ 6ë‹¨ê³„: ê° ì²­í¬ì— ì²¨ë¶€í•  ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "conversation_id": conversation_id,        # ì„¸ì…˜ ID
                "member_id": member_id,                   # ì‚¬ìš©ì ID (ê²€ìƒ‰ í•„í„°ë§ìš©)
                "user_name": user_name,                   # ì‚¬ìš©ì ì´ë¦„
                "summary": summary,                       # AI ìƒì„± ìš”ì•½
                "created_at": session_metadata.get("created_at", datetime.utcnow()).isoformat(),
                "session_duration_minutes": session_metadata.get("session_duration_minutes", 0),
                "message_count": len(messages),           # ì´ ë©”ì‹œì§€ ìˆ˜
                "indexed_at": datetime.utcnow().isoformat()  # VectorDB êµ¬ì¶• ì‹œì 
            }
            
            # ğŸ—ƒï¸ 7ë‹¨ê³„: ChromaDB VectorStore ì´ˆê¸°í™” (ì‚¬ìš©ìë³„ ì»¬ë ‰ì…˜)
            vectorstore = Chroma(
                collection_name=f"user_{member_id}_sessions",  # ì‚¬ìš©ìë³„ ì»¬ë ‰ì…˜ëª…
                embedding_function=self.embeddings,            # OpenAI ì„ë² ë”© í•¨ìˆ˜  
                persist_directory=str(user_db_path)            # ì €ì¥ ê²½ë¡œ
            )
            
            # ğŸ“¦ 8ë‹¨ê³„: ê° ì²­í¬ì— ê³ ìœ  ë©”íƒ€ë°ì´í„° ì¶”ê°€í•˜ì—¬ VectorDBì— ì €ì¥
            metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                chunk_metadata["chunk_index"] = i                   # ì²­í¬ ìˆœë²ˆ
                chunk_metadata["chunk_content"] = chunk[:100] + "..." if len(chunk) > 100 else chunk  # ë¯¸ë¦¬ë³´ê¸°
                metadatas.append(chunk_metadata)
            
            # ğŸ’¾ VectorDBì— í…ìŠ¤íŠ¸ ì²­í¬ë“¤ ì €ì¥
            vectorstore.add_texts(
                texts=chunks,
                metadatas=metadatas,
                ids=[f"{conversation_id}_chunk_{i}" for i in range(len(chunks))]
            )
            
            # ğŸ“ ì˜ì†í™” ì²˜ë¦¬
            print(f"   ğŸ’¾ VectorDB ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            # ğŸ“‹ 9ë‹¨ê³„: ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸ (ë¹ ë¥¸ ì„¸ì…˜ íƒìƒ‰ìš©)
            await self._update_session_index(user_db_path, conversation_id, metadata)
            
            print(f"âœ… VectorDB êµ¬ì¶• ì„±ê³µ: {conversation_id}")
            print(f"   ğŸ‘¤ ì‚¬ìš©ì: {user_name} (ID: {member_id})")
            print(f"   ğŸ“ ìš”ì•½: {summary}")
            print(f"   ğŸ“Š ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
            print(f"   ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {user_db_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ VectorDB êµ¬ì¶• ì‹¤íŒ¨: {conversation_id} - {e}")
            return False
    
    async def _update_session_index(self, user_db_path: Path, conversation_id: str, metadata: Dict[str, Any]):
        """
        ğŸ“‹ ì‚¬ìš©ìë³„ ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸
        
        Args:
            user_db_path: ì‚¬ìš©ì VectorDB ì €ì¥ ê²½ë¡œ
            conversation_id: ëŒ€í™” ì„¸ì…˜ ID
            metadata: ì„¸ì…˜ ë©”íƒ€ë°ì´í„°
            
        ğŸ’¡ ê¸°ëŠ¥:
            - ë¹ ë¥¸ ì„¸ì…˜ íƒìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±/ì—…ë°ì´íŠ¸
            - ì„¸ì…˜ë³„ ìš”ì•½, ìƒì„±ì‹œê°„, ë©”ì‹œì§€ ìˆ˜ ë“± ì •ë³´ ì €ì¥
            - JSON í˜•íƒœë¡œ êµ¬ì¡°í™”ëœ ì¸ë±ìŠ¤ ìœ ì§€
        """
        try:
            index_file = user_db_path / "session_index.json"
            
            # ğŸ“– ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            else:
                index_data = {
                    "member_id": metadata["member_id"],
                    "created_at": datetime.utcnow().isoformat(),
                    "sessions": {}
                }
            
            # â• ìƒˆ ì„¸ì…˜ ì •ë³´ ì¶”ê°€
            index_data["sessions"][conversation_id] = {
                "summary": metadata["summary"],
                "created_at": metadata["created_at"],
                "indexed_at": metadata["indexed_at"],
                "message_count": metadata["message_count"],
                "session_duration_minutes": metadata["session_duration_minutes"]
            }
            
            # ğŸ“ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ë° ì €ì¥
            index_data["last_updated"] = datetime.utcnow().isoformat()
            index_data["total_sessions"] = len(index_data["sessions"])
            
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ“‹ ì„¸ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {metadata['member_id']} - ì´ {len(index_data['sessions'])}ê°œ ì„¸ì…˜")
            
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_user_vectorstore(self, member_id: str) -> Optional[Chroma]:
        """
        ğŸ—ƒï¸ ì‚¬ìš©ìë³„ VectorDB ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        
        Args:
            member_id: ì‚¬ìš©ì ê³ ìœ  ID
            
        Returns:
            Optional[Chroma]: ì‚¬ìš©ìì˜ VectorStore ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ None)
            
        ğŸ’¡ ìš©ë„:
            - ê³¼ê±° ì„¸ì…˜ ê²€ìƒ‰ ì‹œ ì‚¬ìš©
            - ê°œì¸í™”ëœ ëŒ€í™” ë‚´ì—­ ì¡°íšŒ
            - ì‚¬ìš©ìë³„ ì™„ì „ ë¶„ë¦¬ëœ VectorDB ì ‘ê·¼
        """
        try:
            user_db_path = self.storage_path / f"user_{member_id}_sessions"
            
            if not user_db_path.exists():
                print(f"âŒ ì‚¬ìš©ì VectorDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {member_id}")
                return None
            
            vectorstore = Chroma(
                collection_name=f"user_{member_id}_sessions",
                embedding_function=self.embeddings,
                persist_directory=str(user_db_path)
            )
            
            print(f"âœ… ì‚¬ìš©ì VectorDB ë¡œë“œ ì„±ê³µ: {member_id}")
            return vectorstore
            
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì VectorDB ë¡œë“œ ì‹¤íŒ¨: {member_id} - {e}")
            return None
    
    def search_user_sessions(self, member_id: str, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        ğŸ” ì‚¬ìš©ìì˜ ê³¼ê±° ì„¸ì…˜ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
        
        Args:
            member_id: ì‚¬ìš©ì ê³ ìœ  ID
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ê´€ë ¨ë„ ì ìˆ˜ í¬í•¨)
            
        ğŸ’¡ ê¸°ëŠ¥:
            - ê°œì¸í™”ëœ ê³¼ê±° ëŒ€í™” ë‚´ì—­ ê²€ìƒ‰
            - ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)
            - ê´€ë ¨ë„ ì ìˆ˜ ê¸°ë°˜ í’ˆì§ˆ í•„í„°ë§
        """
        try:
            vectorstore = self.get_user_vectorstore(member_id)
            
            if not vectorstore:
                return []
            
            # ğŸ” ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            results = vectorstore.similarity_search_with_relevance_scores(
                query=query,
                k=k
            )
            
            # ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°í™”
            search_results = []
            for doc, score in results:
                search_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": score,
                    "conversation_id": doc.metadata.get("conversation_id", "unknown"),
                    "session_summary": doc.metadata.get("summary", "ìš”ì•½ ì—†ìŒ")
                })
            
            print(f"   ğŸ” ì‚¬ìš©ì {member_id} ì„¸ì…˜ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ì„¸ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨: {member_id} - {e}")
            return []
    
    def get_user_session_stats(self, member_id: str) -> Dict[str, Any]:
        """
        ğŸ“Š ì‚¬ìš©ìë³„ ì„¸ì…˜ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Args:
            member_id: ì‚¬ìš©ì ê³ ìœ  ID
            
        Returns:
            Dict[str, Any]: ì„¸ì…˜ í†µê³„ ì •ë³´
            {
                'member_id': ì‚¬ìš©ì ID,
                'total_sessions': ì´ ì„¸ì…˜ ìˆ˜,
                'total_messages': ì´ ë©”ì‹œì§€ ìˆ˜,
                'recent_sessions': ìµœê·¼ ì„¸ì…˜ ì •ë³´,
                'last_activity': ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
            }
            
        ğŸ’¡ ìš©ë„:
            - ì‚¬ìš©ì ëŒ€í™” í™œë™ ë¶„ì„
            - ê°œì¸í™” ì„œë¹„ìŠ¤ ê°œì„  ë°ì´í„°
            - ì‚¬ìš© íŒ¨í„´ íŒŒì•…
        """
        try:
            index_file = self.storage_path / f"user_{member_id}_sessions" / "session_index.json"
            
            if not index_file.exists():
                return {
                    "member_id": member_id,
                    "total_sessions": 0,
                    "total_messages": 0,
                    "recent_sessions": [],
                    "message": "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"
                }
            
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            # ğŸ“Š í†µê³„ ê³„ì‚° ë° ìµœê·¼ ì„¸ì…˜ ì •ë³´ ì¶”ì¶œ
            sessions = index_data.get("sessions", {})
            total_messages = sum(session.get("message_count", 0) for session in sessions.values())
            
            # ğŸ“… ìµœê·¼ 5ê°œ ì„¸ì…˜ ì •ë³´ (ì‹œê°„ìˆœ ì •ë ¬)
            recent_sessions = []
            sorted_sessions = sorted(
                sessions.items(), 
                key=lambda x: x[1].get("created_at", ""), 
                reverse=True
            )[:5]
            
            for session_id, session_info in sorted_sessions:
                recent_sessions.append({
                    "session_id": session_id,
                    "summary": session_info.get("summary", "ìš”ì•½ ì—†ìŒ"),
                    "created_at": session_info.get("created_at"),
                    "message_count": session_info.get("message_count", 0)
                })
            
            return {
                "member_id": member_id,
                "total_sessions": index_data.get("total_sessions", 0),
                "total_messages": total_messages,
                "recent_sessions": recent_sessions,
                "first_session": index_data.get("created_at"),
                "last_activity": index_data.get("last_updated")
            }
            
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ì„¸ì…˜ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {member_id} - {e}")
            return {
                "member_id": member_id,
                "error": str(e)
            }


# ğŸŒ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ - ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ê³µìœ 
session_vectordb_builder = SessionVectorDBBuilder()
