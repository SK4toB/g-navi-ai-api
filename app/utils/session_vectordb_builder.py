# app/utils/session_vectordb_builder.py
"""
ğŸ—ƒï¸ ì‚¬ìš©ìë³„ ì±„íŒ… ì„¸ì…˜ ëŒ€í™”ë‚´ì—­ VectorDB êµ¬ì¶• ì‹œìŠ¤í…œ

ğŸ“‹ í•µì‹¬ ê¸°ëŠ¥:
1. ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì‚¬ìš©ìì˜ current_session_messagesë¥¼ VectorDBë¡œ ìë™ êµ¬ì¶•
2. ì‚¬ìš©ìë³„(member_id) ë¶„ë¦¬ëœ VectorDB ì €ì¥ â†’ ê°œì¸ì •ë³´ ë³´í˜¸ ë° ê°œì¸í™” ê²€ìƒ‰
3. OpenAI Embeddings + ChromaDBë¥¼ í™œìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
4. ê³¼ê±° ëŒ€í™” ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

ğŸ”„ ë™ì‘ í”Œë¡œìš°:
ì„¸ì…˜ ì¢…ë£Œ â†’ current_session_messages ìˆ˜ì§‘ â†’ LLM ìš”ì•½ â†’ VectorDB êµ¬ì¶• â†’ í–¥í›„ ê²€ìƒ‰ í™œìš©

ğŸ“ ì €ì¥ êµ¬ì¡°:
storage/vector_stores/user_{member_id}_sessions/
â”œâ”€â”€ chroma.sqlite3              # ChromaDB ë²¡í„° ì €ì¥ì†Œ
â”œâ”€â”€ session_index.json          # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤
â””â”€â”€ ê¸°íƒ€ ChromaDB íŒŒì¼ë“¤

âš ï¸ ì¤‘ìš” ì‚¬í•­:
- ê° ì‚¬ìš©ìë³„ë¡œ ì™„ì „íˆ ë¶„ë¦¬ëœ VectorDB ìƒì„± (íƒ€ ì‚¬ìš©ì ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€)
- ì„¸ì…˜ ì¢…ë£Œ ì‹œì—ë§Œ VectorDB êµ¬ì¶• (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì•„ë‹˜)
- ê²€ìƒ‰ ì‹œ ê´€ë ¨ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ í’ˆì§ˆ ë³´ì¥
"""

import os
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

class SessionVectorDBBuilder:
    """
    ğŸ—ƒï¸ ì‚¬ìš©ìë³„ ì±„íŒ… ì„¸ì…˜ ëŒ€í™”ë‚´ì—­ VectorDB êµ¬ì¶• ë° ê´€ë¦¬ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì±„íŒ… ì„¸ì…˜ì´ ì¢…ë£Œë  ë•Œ í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë‚´ì—­ì„
    ì‚¬ìš©ìë³„ë¡œ ë¶„ë¦¬ëœ VectorDBì— ì €ì¥í•˜ì—¬, í–¥í›„ ê°œì¸í™”ëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ì£¼ìš” ì±…ì„:
    - ì„¸ì…˜ ì¢…ë£Œ ì‹œ current_session_messages â†’ VectorDB ìë™ êµ¬ì¶•
    - ì‚¬ìš©ìë³„ VectorDB ë¶„ë¦¬ ê´€ë¦¬ (member_id ê¸°ì¤€)
    - ê³¼ê±° ëŒ€í™” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
    - ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ë° í†µê³„ ê´€ë¦¬
    """
    
    def __init__(self):
        """
        VectorDB êµ¬ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        ì„¤ì • ë‚´ìš©:
        - ì €ì¥ ê²½ë¡œ: app/storage/vector_stores/
        - ì„ë² ë”© ëª¨ë¸: OpenAI text-embedding-3-small
        - í…ìŠ¤íŠ¸ ë¶„í• : 1000ì ì²­í¬, 200ì ì˜¤ë²„ë©
        """
        # ğŸ“ VectorDB ì €ì¥ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìë³„ í´ë”ë¡œ ë¶„ë¦¬ë¨)
        self.storage_path = Path(__file__).parent.parent / "storage" / "vector_stores"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # ğŸ¤– OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"  # ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸
        )
        
        # âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì„¤ì • (ê¸´ ëŒ€í™”ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,    # ê° ì²­í¬ ìµœëŒ€ 1000ì
            chunk_overlap=200,  # ì²­í¬ ê°„ 200ì ì¤‘ë³µ (ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´)
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]  # ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í• ì 
        )
        
        print(f"SessionVectorDBBuilder ì´ˆê¸°í™” ì™„ë£Œ (ì €ì¥ê²½ë¡œ: {self.storage_path})")
    
    async def summarize_session_content(self, messages: List[Dict[str, Any]], user_name: str) -> str:
        """
        ğŸ“ ì„¸ì…˜ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
        
        Args:
            messages: ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ [{"role": "user/assistant", "content": "..."}]
            user_name: ì‚¬ìš©ì ì´ë¦„
            
        Returns:
            str: "ì‚¬ìš©ì {ì´ë¦„}ì˜ ëŒ€í™” ì„¸ì…˜ - ì´ Nê°œ ì§ˆë¬¸, Mê°œ ì‘ë‹µ | ì£¼ìš” ì£¼ì œ: í‚¤ì›Œë“œë“¤"
            
        ëª©ì :
            - VectorDB ê²€ìƒ‰ ì‹œ ë¹ ë¥¸ ì„¸ì…˜ ì‹ë³„
            - ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
            - ê´€ë¦¬ììš© ì„¸ì…˜ ê°œìš” ì œê³µ
        """
        try:
            # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = self._format_messages_to_text(messages)
            
            if not conversation_text.strip():
                return f"ì‚¬ìš©ì {user_name}ì˜ ë¹ˆ ëŒ€í™” ì„¸ì…˜"
            
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
            summary = await self._generate_summary_with_llm(conversation_text, user_name)
            
            return summary
            
        except Exception as e:
            print(f"ì„¸ì…˜ ë‚´ìš© ìš”ì•½ ì‹¤íŒ¨: {e}")
            return f"ì‚¬ìš©ì {user_name}ì˜ ëŒ€í™” ì„¸ì…˜ (ìš”ì•½ ì‹¤íŒ¨)"
    
    def _format_messages_to_text(self, messages: List[Dict[str, Any]]) -> str:
        """ë©”ì‹œì§€ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        for msg in messages:
            if isinstance(msg, dict):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                
                if role == 'user':
                    text_parts.append(f"ì‚¬ìš©ì: {content}")
                elif role == 'assistant':
                    text_parts.append(f"AI: {content}")
                elif role == 'system':
                    text_parts.append(f"ì‹œìŠ¤í…œ: {content}")
                else:
                    text_parts.append(f"{role}: {content}")
        
        return "\n".join(text_parts)
    
    async def _generate_summary_with_llm(self, conversation_text: str, user_name: str) -> str:
        """
        LLMì„ ì‚¬ìš©í•œ ëŒ€í™” ìš”ì•½ ìƒì„±
        (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” OpenAI APIë‚˜ ë‹¤ë¥¸ LLMì„ ì‚¬ìš©)
        """
        try:
            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìš”ì•½ (ì‹¤ì œë¡œëŠ” LLM API í˜¸ì¶œ)
            lines = conversation_text.split('\n')
            user_messages = [line for line in lines if line.startswith('ì‚¬ìš©ì:')]
            ai_messages = [line for line in lines if line.startswith('AI:')]
            
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(conversation_text)
            
            summary = f"ì‚¬ìš©ì {user_name}ì˜ ëŒ€í™” ì„¸ì…˜ - "
            summary += f"ì´ {len(user_messages)}ê°œ ì§ˆë¬¸, {len(ai_messages)}ê°œ ì‘ë‹µ"
            
            if keywords:
                summary += f" | ì£¼ìš” ì£¼ì œ: {', '.join(keywords[:5])}"
            
            return summary
            
        except Exception as e:
            print(f"LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì‚¬ìš©ì {user_name}ì˜ ëŒ€í™” ì„¸ì…˜ (ìš”ì•½ ìƒì„± ì‹¤íŒ¨)"
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©
        common_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'AI', 'ì‚¬ìš©ì', 'ì‹œìŠ¤í…œ'}
        
        words = text.split()
        keywords = []
        
        for word in words:
            if len(word) > 2 and word not in common_words:
                if word not in keywords:
                    keywords.append(word)
                if len(keywords) >= 10:
                    break
        
        return keywords
    
    async def build_vector_db(self, 
                            conversation_id: str, 
                            member_id: str, 
                            user_name: str, 
                            messages: List[Dict[str, Any]],
                            session_metadata: Dict[str, Any]) -> bool:
        """
        ğŸ—ƒï¸ ì„¸ì…˜ ëŒ€í™” ë‚´ì—­ì„ ì‚¬ìš©ìë³„ VectorDBì— ì €ì¥í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ
        
        Args:
            conversation_id: ëŒ€í™”ë°© ê³ ìœ  ID (ì˜ˆ: "chat_session_123")
            member_id: ì‚¬ìš©ì ê³ ìœ  ID (VectorDB ë¶„ë¦¬ ê¸°ì¤€)
            user_name: ì‚¬ìš©ì ì´ë¦„ (ë©”íƒ€ë°ì´í„°ìš©)
            messages: ì„¸ì…˜ì˜ ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ë“¤
            session_metadata: ì„¸ì…˜ ê¸°ë³¸ ì •ë³´ (ìƒì„±ì‹œê°„, ì§€ì†ì‹œê°„ ë“±)
            
        Returns:
            bool: VectorDB êµ¬ì¶• ì„±ê³µ ì—¬ë¶€
            
        ğŸ”„ ì²˜ë¦¬ ê³¼ì •:
        1. ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        2. ì‚¬ìš©ìë³„ VectorDB í´ë” ìƒì„±/ì ‘ê·¼
        3. ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹
        4. OpenAI Embeddingsë¡œ ë²¡í„°í™”
        5. ChromaDBì— ì €ì¥ + ë©”íƒ€ë°ì´í„° ì²¨ë¶€
        6. ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸
        
        ğŸ’¾ ì €ì¥ ìœ„ì¹˜: storage/vector_stores/user_{member_id}_sessions/
        """
        try:
            # âœ… 1ë‹¨ê³„: ë¹ˆ ì„¸ì…˜ ê²€ì¦
            if not messages:
                print(f"ë¹ˆ ë©”ì‹œì§€ ì„¸ì…˜ - VectorDB êµ¬ì¶• ìƒëµ: {conversation_id}")
                return False
            
            # ğŸ“ 2ë‹¨ê³„: ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´)
            summary = await self.summarize_session_content(messages, user_name)
            print(f"ì„¸ì…˜ ìš”ì•½ ìƒì„± ì™„ë£Œ: {conversation_id} - {summary}")
            
            # ğŸ“ 3ë‹¨ê³„: ì‚¬ìš©ìë³„ VectorDB ì €ì¥ ê²½ë¡œ ìƒì„±
            # ì¤‘ìš”: ê° ì‚¬ìš©ìë§ˆë‹¤ ì™„ì „íˆ ë¶„ë¦¬ëœ í´ë”ë¡œ ê°œì¸ì •ë³´ ë³´í˜¸
            user_db_path = self.storage_path / f"user_{member_id}_sessions"
            user_db_path.mkdir(parents=True, exist_ok=True)
            
            # ğŸ”¤ 4ë‹¨ê³„: ëŒ€í™” ë©”ì‹œì§€ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = self._format_messages_to_text(messages)
            
            # âœ‚ï¸ 5ë‹¨ê³„: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ì— ì í•©í•œ í¬ê¸°ë¡œ ë¶„í•  (ì²­í‚¹)
            chunks = self.text_splitter.split_text(conversation_text)
            
            if not chunks:
                print(f"ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ - VectorDB êµ¬ì¶• ìƒëµ: {conversation_id}")
                return False
            
            # ğŸ·ï¸ 6ë‹¨ê³„: ê° ì²­í¬ì— ì²¨ë¶€í•  ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "conversation_id": conversation_id,        # ì„¸ì…˜ ID
                "member_id": member_id,                   # ì‚¬ìš©ì ID (ê²€ìƒ‰ í•„í„°ë§ìš©)
                "user_name": user_name,                   # ì‚¬ìš©ì ì´ë¦„
                "summary": summary,                       # AI ìƒì„± ìš”ì•½
                "created_at": session_metadata.get("created_at", datetime.utcnow()).isoformat(),
                "session_duration_minutes": session_metadata.get("session_duration_minutes", 0),
                "message_count": len(messages),           # ì´ ë©”ì‹œì§€ ìˆ˜
                "indexed_at": datetime.utcnow().isoformat()  # VectorDB êµ¬ì¶• ì‹œì 
            }
            
            # ğŸ—ƒï¸ 7ë‹¨ê³„: ChromaDB VectorStore ì´ˆê¸°í™” (ì‚¬ìš©ìë³„ ì»¬ë ‰ì…˜)
            vectorstore = Chroma(
                collection_name=f"user_{member_id}_sessions",  # ì‚¬ìš©ìë³„ ì»¬ë ‰ì…˜ëª…
                embedding_function=self.embeddings,            # OpenAI ì„ë² ë”© í•¨ìˆ˜
                persist_directory=str(user_db_path)            # ì €ì¥ ê²½ë¡œ
            )
            
            # ğŸ“¦ 8ë‹¨ê³„: ê° ì²­í¬ì— ê³ ìœ  ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                chunk_metadata["chunk_index"] = i                   # ì²­í¬ ìˆœë²ˆ
                chunk_metadata["chunk_content"] = chunk[:100] + "..." if len(chunk) > 100 else chunk  # ë¯¸ë¦¬ë³´ê¸°
                metadatas.append(chunk_metadata)
            
            # VectorDBì— ì¶”ê°€
            vectorstore.add_texts(
                texts=chunks,
                metadatas=metadatas,
                ids=[f"{conversation_id}_chunk_{i}" for i in range(len(chunks))]
            )
            
            # ì˜ì†í™”
            vectorstore.persist()
            
            print(f"âœ… VectorDB êµ¬ì¶• ì™„ë£Œ: {conversation_id} (ì‚¬ìš©ì: {user_name}, ì²­í¬: {len(chunks)}ê°œ)")
            
            # 7. ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸
            await self._update_session_index(member_id, conversation_id, metadata)
            
            return True
            
        except Exception as e:
            print(f"âŒ VectorDB êµ¬ì¶• ì‹¤íŒ¨: {conversation_id} - {e}")
            return False
    
    async def _update_session_index(self, member_id: str, conversation_id: str, metadata: Dict[str, Any]):
        """ì‚¬ìš©ìë³„ ì„¸ì…˜ ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸"""
        try:
            index_file = self.storage_path / f"user_{member_id}_sessions" / "session_index.json"
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            else:
                index_data = {
                    "member_id": member_id,
                    "created_at": datetime.utcnow().isoformat(),
                    "sessions": {}
                }
            
            # ìƒˆ ì„¸ì…˜ ì •ë³´ ì¶”ê°€
            index_data["sessions"][conversation_id] = {
                "summary": metadata["summary"],
                "created_at": metadata["created_at"],
                "indexed_at": metadata["indexed_at"],
                "message_count": metadata["message_count"],
                "session_duration_minutes": metadata["session_duration_minutes"]
            }
            
            index_data["last_updated"] = datetime.utcnow().isoformat()
            index_data["total_sessions"] = len(index_data["sessions"])
            
            # ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
            print(f"ì„¸ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {member_id} - ì´ {len(index_data['sessions'])}ê°œ ì„¸ì…˜")
            
        except Exception as e:
            print(f"ì„¸ì…˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_user_vectorstore(self, member_id: str) -> Optional[Chroma]:
        """ì‚¬ìš©ìë³„ VectorDB ë°˜í™˜"""
        try:
            user_db_path = self.storage_path / f"user_{member_id}_sessions"
            
            if not user_db_path.exists():
                print(f"ì‚¬ìš©ì VectorDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {member_id}")
                return None
            
            vectorstore = Chroma(
                collection_name=f"user_{member_id}_sessions",
                embedding_function=self.embeddings,
                persist_directory=str(user_db_path)
            )
            
            return vectorstore
            
        except Exception as e:
            print(f"ì‚¬ìš©ì VectorDB ë¡œë“œ ì‹¤íŒ¨: {member_id} - {e}")
            return None
    
    def search_user_sessions(self, member_id: str, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìì˜ ê³¼ê±° ì„¸ì…˜ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
        try:
            vectorstore = self.get_user_vectorstore(member_id)
            
            if not vectorstore:
                return []
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            results = vectorstore.similarity_search_with_relevance_scores(
                query=query,
                k=k
            )
            
            search_results = []
            for doc, score in results:
                search_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": score
                })
            
            print(f"ì‚¬ìš©ì {member_id} ì„¸ì…˜ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì„¸ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨: {member_id} - {e}")
            return []
    
    def get_user_session_stats(self, member_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìë³„ ì„¸ì…˜ í†µê³„ ë°˜í™˜"""
        try:
            index_file = self.storage_path / f"user_{member_id}_sessions" / "session_index.json"
            
            if not index_file.exists():
                return {
                    "member_id": member_id,
                    "total_sessions": 0,
                    "message": "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"
                }
            
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            return {
                "member_id": member_id,
                "total_sessions": index_data.get("total_sessions", 0),
                "created_at": index_data.get("created_at"),
                "last_updated": index_data.get("last_updated"),
                "sessions": index_data.get("sessions", {})
            }
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì„¸ì…˜ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {member_id} - {e}")
            return {
                "member_id": member_id,
                "error": str(e)
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
session_vectordb_builder = SessionVectorDBBuilder()
