# csv_to_vector_and_docs.py
# csv íŒŒì¼ì„ ì½ì–´ ê²½ë ¥ ë°ì´í„°ë¥¼ ê·¸ë£¹í•‘í•˜ê³  docsê³¼ vectordbë¥¼ í•œë²ˆì— ìƒì„±í•˜ëŠ” ë„êµ¬

"""
ê¸°ì¡´ VectorDBì˜ ê·¸ë£¹í•‘ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³  ìˆ˜ì • (í†µí•© ìƒì„±ë„êµ¬)

ì´ ë„êµ¬ëŠ” ê²½ë ¥ ë°ì´í„°ë¥¼ ê°œì¸ë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³ , 
ê° ê°œì¸ì˜ ê²½ë ¥ íƒ€ì„ë¼ì¸ì„ í†µí•©í•˜ì—¬ ë¬¸ì„œë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ì´í›„, ìƒì„±ëœ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ë° ë¶„ì„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.

ì´ ë„êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤:
1. CSV íŒŒì¼ì—ì„œ ê²½ë ¥ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê°œì¸ë³„ë¡œ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤.
2. ê° ê°œì¸ì˜ ê²½ë ¥ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì—°ì†ì ì¸ íƒ€ì„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
3. ìƒì„±ëœ íƒ€ì„ë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ê´„ì ì´ê³  ì •í™•í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
4. ìƒì„±ëœ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.

ì£¼ì˜ : readonly ì˜¤ë¥˜ ë°œìƒì‹œ, vector_stores/career_data ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
"""

import os
import shutil
import json
import pandas as pd
import logging
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datetime import datetime
from dotenv import load_dotenv
load_dotenv()


class VectorDBGroupingFixer:
    """VectorDB ê·¸ë£¹í•‘ ë¬¸ì œ ìë™ ìˆ˜ì • ë„êµ¬"""
    
    def __init__(self, 
                 csv_path: str = "data/csv/career_history.csv",
                 skillset_csv_path: str = "data/csv/skill_set.csv",
                 persist_directory: str = "storage/vector_stores/career_data",
                 cache_directory: str = "storage/cache/embedding_cache",
                 docs_json_path: str = "storage/docs/career_history.json"):
        
        self.csv_path = os.path.abspath(csv_path)
        self.skillset_csv_path = os.path.abspath(skillset_csv_path)
        self.persist_directory = os.path.abspath(persist_directory)
        self.cache_directory = os.path.abspath(cache_directory)
        self.docs_json_path = os.path.abspath(docs_json_path)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì„ë² ë”© ì„¤ì •
        self.base_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            dimensions=1536
        )
        self.cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
            self.base_embeddings,
            LocalFileStore(cache_directory),
            namespace="career_embeddings"
        )
        
        # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=150,
            separators=["\n=== ê²½ë ¥ ", "\ní”„ë¡œì íŠ¸:", "\nì£¼ìš”ì—…ë¬´:", "\nì—­í• :", "\n\n", "\n", ": ", " "],
            length_function=len
        )
        
        # ìŠ¤í‚¬ì…‹ ë§¤í•‘
        self.skillset_mapping = {}
        self._load_skillset_mapping()
    
    def _load_skillset_mapping(self):
        """ìŠ¤í‚¬ì…‹ ë§¤í•‘ ë¡œë“œ"""
        try:
            if os.path.exists(self.skillset_csv_path):
                skill_df = pd.read_csv(self.skillset_csv_path, encoding='utf-8')
                mapping = {}
                for _, row in skill_df.iterrows():
                    if pd.notna(row.get('ì½”ë“œ')):
                        code = str(row['ì½”ë“œ']).strip()
                        mapping[code] = {
                            'skill_name': str(row.get('Skill set', '')).strip(),
                            'job_category': str(row.get('Skillset-ì§ë¬´ì—°ê³„', '')).strip(),
                            'description': str(row.get('Skill set', '')).strip()
                        }
                self.skillset_mapping = mapping
                self.logger.info(f"ìŠ¤í‚¬ì…‹ ë§¤í•‘ ì™„ë£Œ: {len(mapping)}ê°œ")
        except Exception as e:
            self.logger.warning(f"ìŠ¤í‚¬ì…‹ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def load_and_group_career_data(self) -> Dict[str, pd.DataFrame]:
        """ê²½ë ¥ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê°œì¸ë³„ë¡œ ì˜¬ë°”ë¥´ê²Œ ê·¸ë£¹í•‘"""
        try:
            # CSV ë¡œë“œ
            df = pd.read_csv(self.csv_path, encoding='utf-8')
            self.logger.info(f"ê²½ë ¥ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
            
            # ë°ì´í„° ì •ì œ
            df = df.dropna(subset=['ê³ ìœ ë²ˆí˜¸'])
            df['ê³ ìœ ë²ˆí˜¸'] = df['ê³ ìœ ë²ˆí˜¸'].astype(str)
            
            # ê°œì¸ë³„ ê·¸ë£¹í•‘ ë° ì—°ì°¨ìˆœ ì •ë ¬
            grouped = df.groupby('ê³ ìœ ë²ˆí˜¸')
            employee_groups = {}
            
            for emp_id, group_df in grouped:
                # ì—°ì°¨ë¡œ ì •ë ¬ (NaN ê°’ì€ ë§ˆì§€ë§‰ì—)
                if 'ì—°ì°¨' in group_df.columns:
                    # ì—°ì°¨ ì»¬ëŸ¼ì„ ìˆ«ìë¡œ ë³€í™˜
                    group_df = group_df.copy()
                    group_df['ì—°ì°¨_numeric'] = pd.to_numeric(group_df['ì—°ì°¨'], errors='coerce')
                    group_df = group_df.sort_values('ì—°ì°¨_numeric', na_position='last')
                    group_df = group_df.drop('ì—°ì°¨_numeric', axis=1)
                
                employee_groups[emp_id] = group_df.reset_index(drop=True)
                
                # ê·¸ë£¹í•‘ ê²°ê³¼ ë¡œê¹…
                if 'ì—°ì°¨' in group_df.columns:
                    years = group_df['ì—°ì°¨'].dropna()
                    if len(years) > 0:
                        self.logger.info(f"  {emp_id}: {len(group_df)}í–‰, {int(years.min())}-{int(years.max())}ë…„ì°¨")
            
            self.logger.info(f"ê°œì¸ë³„ ê·¸ë£¹í•‘ ì™„ë£Œ: {len(employee_groups)}ëª…")
            return employee_groups
            
        except Exception as e:
            self.logger.error(f"ê²½ë ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def create_integrated_career_timeline(self, emp_df: pd.DataFrame, emp_id: str) -> str:
        """ê°œì¸ì˜ ëª¨ë“  ê²½ë ¥ì„ í†µí•©í•œ ì—°ì†ì ì¸ íƒ€ì„ë¼ì¸ ìƒì„±"""
        
        # í—¤ë” ìƒì„±
        timeline_text = f"â–  ê³ ìœ ë²ˆí˜¸: {emp_id}\n"
        timeline_text += f"â–  ì´ ê²½ë ¥ ê¸°ë¡: {len(emp_df)}ê°œ\n"
        
        # ê²½ë ¥ ê¸°ê°„ ì •ë³´ (ìˆ˜ì •ëœ ë¡œì§)
        if 'ì—°ì°¨' in emp_df.columns:
            years = emp_df['ì—°ì°¨'].dropna()
            if not years.empty:
                min_year = int(years.min())
                max_year = int(years.max())
                total_years = max_year - min_year + 1
                timeline_text += f"â–  ê²½ë ¥ ê¸°ê°„: {min_year}ë…„ì°¨ â†’ {max_year}ë…„ì°¨ (ì´ {total_years}ë…„)\n"
                timeline_text += f"â–  ê²½ë ¥ ì—°ì°¨: {sorted(years.astype(int).tolist())}\n"
        
        # ì£¼ìš” ë„ë©”ì¸ ìš”ì•½
        if 'Domain ê²½í—˜' in emp_df.columns:
            domains = emp_df['Domain ê²½í—˜'].dropna().unique()
            if len(domains) > 0:
                timeline_text += f"â–  ì£¼ìš” ë„ë©”ì¸: {', '.join(domains)}\n"
        
        # ì£¼ìš” ì—­í•  ìš”ì•½
        if 'ì—­í• ' in emp_df.columns:
            roles = emp_df['ì—­í• '].dropna().unique()
            if len(roles) > 0:
                timeline_text += f"â–  ì£¼ìš” ì—­í• : {', '.join(roles)}\n"
        
        timeline_text += "\n"
        
        # ì—°ì°¨ë³„ ìƒì„¸ ì •ë³´ (ì‹œê°„ìˆœ)
        timeline_text += "=== ì—°ì°¨ë³„ ê²½ë ¥ ìƒì„¸ ===\n\n"
        
        for idx, (_, row) in enumerate(emp_df.iterrows(), 1):
            # ì—°ì°¨ ì •ë³´
            year_info = f"â–£ {int(row['ì—°ì°¨'])}ë…„ì°¨" if pd.notna(row.get('ì—°ì°¨')) else f"â–£ ê¸°ë¡ {idx}"
            timeline_text += year_info + "\n"
            
            # í”„ë¡œì íŠ¸/ì—…ë¬´ ì •ë³´
            if pd.notna(row.get('ì—…ë¬´')):
                timeline_text += f"  ğŸ“‹ ì—…ë¬´: {row['ì—…ë¬´']}\n"
            
            # ì—­í•  ì •ë³´
            if pd.notna(row.get('ì—­í• ')):
                timeline_text += f"  ğŸ‘¤ ì—­í• : {row['ì—­í• ']}\n"
            
            # ë„ë©”ì¸ ì •ë³´
            if pd.notna(row.get('Domain ê²½í—˜')):
                timeline_text += f"  ğŸ¢ ë„ë©”ì¸: {row['Domain ê²½í—˜']}\n"
            
            # í”„ë¡œì íŠ¸ ê·œëª¨
            if pd.notna(row.get('í”„ë¡œì íŠ¸ ê·œëª¨(M/M)')):
                timeline_text += f"  ğŸ“Š ê·œëª¨: {row['í”„ë¡œì íŠ¸ ê·œëª¨(M/M)']}M/M\n"
            
            # ìŠ¤í‚¬ì…‹ ì •ë³´
            skills = self._extract_skills_from_row(row)
            if skills:
                resolved_skills = self._resolve_skill_codes(skills)
                if resolved_skills['skill_names']:
                    timeline_text += f"  ğŸ”§ í™œìš© ê¸°ìˆ : {', '.join(resolved_skills['skill_names'][:5])}\n"
            
            # ì¤‘ìš” ê²½ë ¥ í¬ì¸íŠ¸
            if self._is_important_career_point(row):
                impact_desc = row.get('í° ì˜í–¥ì„ ë°›ì€ ì—…ë¬´/ì‹œê¸°ì— ëŒ€í•œ ì„¤ëª…')
                if pd.notna(impact_desc):
                    timeline_text += f"  â­ í•µì‹¬ ê²½ë ¥: {impact_desc}\n"
            
            timeline_text += "\n"
        
        return timeline_text
    
    def _extract_skills_from_row(self, row: pd.Series) -> List[str]:
        """í–‰ì—ì„œ ìŠ¤í‚¬ ì½”ë“œë“¤ ì¶”ì¶œ"""
        skills = []
        for col in row.index:
            if 'Skill set' in col and pd.notna(row[col]):
                skill_code = str(row[col]).strip()
                if skill_code:
                    skills.append(skill_code)
        return skills
    
    def _resolve_skill_codes(self, skill_codes: List[str]) -> Dict[str, Any]:
        """ìŠ¤í‚¬ ì½”ë“œë¥¼ ì‹¤ì œ ìŠ¤í‚¬ëª…ìœ¼ë¡œ ë³€í™˜"""
        resolved_skills = {
            'skill_names': [],
            'job_categories': [],
            'technical_skills': [],
            'management_skills': []
        }
        
        for code in skill_codes:
            if code and code in self.skillset_mapping:
                skill_info = self.skillset_mapping[code]
                skill_name = skill_info['skill_name']
                job_category = skill_info['job_category']
                
                resolved_skills['skill_names'].append(skill_name)
                resolved_skills['job_categories'].append(job_category)
                
                # ê¸°ìˆ /ê´€ë¦¬ ìŠ¤í‚¬ ë¶„ë¥˜
                if any(keyword in skill_name.lower() for keyword in 
                      ['dev', 'eng', 'architect', 'database', 'system', 'network']):
                    resolved_skills['technical_skills'].append(skill_name)
                elif any(keyword in skill_name.lower() for keyword in 
                        ['pm', 'management', 'ê´€ë¦¬', 'lead']):
                    resolved_skills['management_skills'].append(skill_name)
        
        # ì¤‘ë³µ ì œê±°
        for key in resolved_skills:
            resolved_skills[key] = list(set(resolved_skills[key]))
        
        return resolved_skills
    
    def _is_important_career_point(self, row: pd.Series) -> bool:
        """ì¤‘ìš”í•œ ê²½ë ¥ í¬ì¸íŠ¸ì¸ì§€ í™•ì¸"""
        impact_col = 'ì»¤ë¦¬ì–´ í˜•ì„±ì— í° ì˜í–¥ì„ ë°›ì€ ì—…ë¬´ë‚˜ ì‹œê¸°'
        return pd.notna(row.get(impact_col)) and str(row.get(impact_col)).upper() == 'TRUE'
    
    def create_comprehensive_metadata(self, emp_id: str, emp_df: pd.DataFrame) -> Dict[str, Any]:
        """í¬ê´„ì ì´ê³  ì •í™•í•œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'employee_id': emp_id,
            'total_career_records': len(emp_df),
            'source_file': self.csv_path,
            'processing_timestamp': datetime.now().isoformat(),
            'processing_method': 'integrated_career_timeline'
        }
        
        # ì—°ì°¨ ì •ë³´ (ì •í™•í•œ ê³„ì‚°)
        if 'ì—°ì°¨' in emp_df.columns:
            years = emp_df['ì—°ì°¨'].dropna()
            if not years.empty:
                min_year = int(years.min())
                max_year = int(years.max())
                metadata.update({
                    'career_start_year': min_year,
                    'career_end_year': max_year,
                    'total_experience_years': max_year - min_year + 1,
                    'experience_level': self._categorize_experience_level(max_year),
                    'year_list': sorted(years.astype(int).tolist())
                })
        
        # ë„ë©”ì¸ ë¶„ì„
        if 'Domain ê²½í—˜' in emp_df.columns:
            domains = emp_df['Domain ê²½í—˜'].dropna().unique()
            if len(domains) > 0:
                metadata.update({
                    'domains': list(domains),
                    'primary_domain': domains[0],
                    'domain_diversity': len(domains),
                    'is_domain_specialist': len(domains) <= 2
                })
        
        # ì—­í•  ë¶„ì„
        if 'ì—­í• ' in emp_df.columns:
            roles = emp_df['ì—­í• '].dropna().unique()
            if len(roles) > 0:
                metadata.update({
                    'roles': list(roles),
                    'role_progression_complexity': len(roles),
                    'is_leadership_track': any('PM' in str(role) or 'ê´€ë¦¬' in str(role) or 'Lead' in str(role) 
                                             for role in roles)
                })
        
        # ìŠ¤í‚¬ì…‹ ì¢…í•© ë¶„ì„
        all_skills = []
        for _, row in emp_df.iterrows():
            row_skills = self._extract_skills_from_row(row)
            all_skills.extend(row_skills)
        
        if all_skills:
            resolved_skills = self._resolve_skill_codes(list(set(all_skills)))
            metadata.update({
                'total_skill_codes': len(set(all_skills)),
                'skill_names': resolved_skills['skill_names'][:10],  # ìƒìœ„ 10ê°œë§Œ
                'job_categories': list(set(resolved_skills['job_categories'])),
                'technical_skills': resolved_skills['technical_skills'][:5],
                'management_skills': resolved_skills['management_skills'][:5],
                'skill_diversity_score': len(set(resolved_skills['job_categories']))
            })
        
        # ì¤‘ìš” ê²½ë ¥ í¬ì¸íŠ¸ ìˆ˜
        important_points = sum(1 for _, row in emp_df.iterrows() if self._is_important_career_point(row))
        metadata['critical_career_points'] = important_points
        
        return metadata
    
    def _categorize_experience_level(self, max_year: int) -> str:
        """ê²½ë ¥ ë ˆë²¨ ë¶„ë¥˜"""
        if max_year <= 3:
            return "junior"
        elif max_year <= 7:
            return "mid-level"
        elif max_year <= 15:
            return "senior"
        else:
            return "expert"
    
    def create_fixed_documents(self, employee_groups: Dict[str, pd.DataFrame]) -> List[Document]:
        """ìˆ˜ì •ëœ ê·¸ë£¹í•‘ ë¡œì§ìœ¼ë¡œ Document ìƒì„±"""
        documents = []
        
        for emp_id, emp_df in employee_groups.items():
            try:
                # í†µí•©ëœ ê²½ë ¥ íƒ€ì„ë¼ì¸ í…ìŠ¤íŠ¸ ìƒì„±
                timeline_text = self.create_integrated_career_timeline(emp_df, emp_id)
                
                # í¬ê´„ì  ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = self.create_comprehensive_metadata(emp_id, emp_df)
                
                # ë¬¸ì„œ í¬ê¸° ì²´í¬ ë° ë¶„í• 
                if len(timeline_text) <= 1500:
                    # ë‹¨ì¼ ë¬¸ì„œ
                    doc = Document(
                        page_content=timeline_text,
                        metadata=metadata
                    )
                    documents.append(doc)
                    
                else:
                    # í° ë¬¸ì„œëŠ” ë¶„í•  (í•˜ì§€ë§Œ ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€)
                    split_texts = self.text_splitter.split_text(timeline_text)
                    self.logger.info(f"{emp_id}: {len(split_texts)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
                    
                    for i, split_text in enumerate(split_texts):
                        chunk_metadata = metadata.copy()
                        chunk_metadata.update({
                            'chunk_index': i,
                            'total_chunks': len(split_texts),
                            'chunk_size': len(split_text),
                            'original_document_size': len(timeline_text)
                        })
                        
                        chunk_doc = Document(
                            page_content=split_text,
                            metadata=chunk_metadata
                        )
                        documents.append(chunk_doc)
                
            except Exception as e:
                self.logger.error(f"{emp_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì´ {len(documents)}ê°œ Document ìƒì„± ì™„ë£Œ")
        return documents
    
    def rebuild_vectorstore_with_fixed_grouping(self) -> Dict[str, Any]:
        """ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ VectorStore ì¬êµ¬ì¶•"""
        result = {
            "steps_completed": [],
            "errors": [],
            "success": False,
            "build_summary": {}
        }
        
        try:
            # 1. ê¸°ì¡´ VectorStore ë° ìºì‹œ ì‚­ì œ
            self.logger.info("ê¸°ì¡´ VectorStore ì‚­ì œ ì¤‘...")
            if os.path.exists(self.persist_directory):
                shutil.rmtree(self.persist_directory)
            if os.path.exists(self.cache_directory):
                shutil.rmtree(self.cache_directory)
            result["steps_completed"].append("ê¸°ì¡´ ë°ì´í„° ì‚­ì œ")
            
            # 2. ë””ë ‰í† ë¦¬ ì¬ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            os.makedirs(self.cache_directory, exist_ok=True)
            os.makedirs(os.path.dirname(self.docs_json_path), exist_ok=True)
            
            # 3. ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
            self.logger.info("ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            employee_groups = self.load_and_group_career_data()
            result["steps_completed"].append("ê°œì¸ë³„ ë°ì´í„° ê·¸ë£¹í•‘")
            
            # 4. ìˆ˜ì •ëœ Document ìƒì„±
            self.logger.info("í†µí•© Document ìƒì„± ì¤‘...")
            documents = self.create_fixed_documents(employee_groups)
            result["steps_completed"].append("í†µí•© Document ìƒì„±")
            
            # 5. JSON íŒŒì¼ ì €ì¥
            self.logger.info("docs JSON íŒŒì¼ ì €ì¥ ì¤‘...")
            with open(self.docs_json_path, 'w', encoding='utf-8') as f:
                json_docs = [
                    {"page_content": doc.page_content, "metadata": doc.metadata}
                    for doc in documents
                ]
                json.dump(json_docs, f, ensure_ascii=False, indent=2)
            result["steps_completed"].append("docs JSON ì €ì¥")
            
            # 6. ChromaDB êµ¬ì¶•
            self.logger.info("ChromaDB êµ¬ì¶• ì¤‘...")
            
            # ë©”íƒ€ë°ì´í„° ì •ì œ (ChromaDB í˜¸í™˜ì„±)
            filtered_documents = []
            for doc in documents:
                clean_metadata = {}
                for key, value in doc.metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        clean_metadata[key] = value
                    elif isinstance(value, list) and all(isinstance(item, str) for item in value):
                        clean_metadata[key] = ", ".join(value[:5])
                    elif value is not None:
                        clean_metadata[key] = str(value)
                
                filtered_doc = Document(
                    page_content=doc.page_content,
                    metadata=clean_metadata
                )
                filtered_documents.append(filtered_doc)
            
            # VectorStore ìƒì„±
            vector_store = Chroma.from_documents(
                documents=filtered_documents,
                embedding=self.cached_embeddings,
                persist_directory=self.persist_directory,
                collection_name="career_history"
            )
            
            vector_store.persist()
            result["steps_completed"].append("ChromaDB êµ¬ì¶•")
            
            # 7. êµ¬ì¶• ì •ë³´ ìš”ì•½
            result["build_summary"] = {
                'total_employees': len(employee_groups),
                'total_documents': len(filtered_documents),
                'persist_directory': self.persist_directory,
                'build_timestamp': datetime.now().isoformat(),
                'grouping_fixed': True,
                'employee_sample': {
                    emp_id: {
                        'rows': len(group),
                        'years': f"{int(group['ì—°ì°¨'].min())}-{int(group['ì—°ì°¨'].max())}" if 'ì—°ì°¨' in group.columns and not group['ì—°ì°¨'].isna().all() else "N/A"
                    } for emp_id, group in list(employee_groups.items())[:3]
                }
            }
            
            result["success"] = True
            self.logger.info("VectorStore ì¬êµ¬ì¶• ì™„ë£Œ!")
            
        except Exception as e:
            error_msg = f"VectorStore ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}"
            result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        return result
    
    def verify_fix(self, target_user: str = "ì´ì¬ì›") -> Dict[str, Any]:
        """ìˆ˜ì • ê²°ê³¼ ê²€ì¦"""
        verification = {
            "target_user": target_user,
            "vectorstore_test": {},
            "grouping_verification": {},
            "fix_success": False
        }
        
        try:
            # VectorStore ë¡œë“œ í…ŒìŠ¤íŠ¸
            if os.path.exists(self.persist_directory):
                vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.cached_embeddings,
                    collection_name="career_history"
                )
                
                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = vectorstore.similarity_search(target_user, k=2)
                verification["vectorstore_test"] = {
                    "search_successful": True,
                    "results_count": len(results),
                    "results_analysis": []
                }
                
                for i, doc in enumerate(results):
                    # ì—°ì°¨ ì •ë³´ ì¶”ì¶œ
                    year_mentions = []
                    content = doc.page_content
                    import re
                    year_pattern = r'(\d+)ë…„ì°¨'
                    matches = re.findall(year_pattern, content)
                    year_mentions = [int(match) for match in matches if match.isdigit()]
                    
                    analysis = {
                        "document_index": i,
                        "content_length": len(content),
                        "year_mentions": year_mentions,
                        "year_range": f"{min(year_mentions)}-{max(year_mentions)}" if year_mentions else "ì—†ìŒ",
                        "total_years": max(year_mentions) - min(year_mentions) + 1 if year_mentions else 0,
                        "appears_integrated": "ì´ ê²½ë ¥ ê¸°ë¡:" in content and content.count("â–£") > 1,
                        "metadata_quality": {
                            "employee_id": doc.metadata.get("employee_id", "ì—†ìŒ"),
                            "total_experience_years": doc.metadata.get("total_experience_years", "ì—†ìŒ"),
                            "domains": doc.metadata.get("domains", "ì—†ìŒ"),
                            "roles": doc.metadata.get("roles", "ì—†ìŒ")
                        }
                    }
                    verification["vectorstore_test"]["results_analysis"].append(analysis)
                
                # ê·¸ë£¹í•‘ ê²€ì¦
                if results:
                    first_result = verification["vectorstore_test"]["results_analysis"][0]
                    verification["grouping_verification"] = {
                        "single_document_covers_multiple_years": first_result["total_years"] > 1,
                        "metadata_properly_extracted": first_result["metadata_quality"]["employee_id"] != "ì—†ìŒ",
                        "content_appears_integrated": first_result["appears_integrated"],
                        "year_range_reasonable": first_result["total_years"] >= 5  # 5ë…„ ì´ìƒ ê²½ë ¥
                    }
                    
                    # ì „ì²´ì  ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                    verification["fix_success"] = all([
                        verification["grouping_verification"]["single_document_covers_multiple_years"],
                        verification["grouping_verification"]["metadata_properly_extracted"],
                        verification["grouping_verification"]["content_appears_integrated"]
                    ])
            
        except Exception as e:
            verification["error"] = f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
        
        return verification

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ VectorDB ê·¸ë£¹í•‘ ë¬¸ì œ ìë™ ìˆ˜ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    fixer = VectorDBGroupingFixer()
    
    # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
    print("\n1ï¸âƒ£ í˜„ì¬ VectorDB ìƒíƒœ í™•ì¸...")
    current_verification = fixer.verify_fix()
    
    if current_verification.get("fix_success"):
        print("âœ… VectorDBê°€ ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return
    
    # 2. VectorStore ì¬êµ¬ì¶•
    print("\n2ï¸âƒ£ ìˆ˜ì •ëœ ê·¸ë£¹í•‘ìœ¼ë¡œ VectorStore ì¬êµ¬ì¶• ì¤‘...")
    rebuild_result = fixer.rebuild_vectorstore_with_fixed_grouping()
    
    # 3. ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì¬êµ¬ì¶• ê²°ê³¼:")
    print(f"   - ì„±ê³µ ì—¬ë¶€: {'âœ…' if rebuild_result['success'] else 'âŒ'}")
    print(f"   - ì™„ë£Œëœ ë‹¨ê³„: {len(rebuild_result['steps_completed'])}ê°œ")
    
    for step in rebuild_result['steps_completed']:
        print(f"     âœ“ {step}")
    
    if rebuild_result['errors']:
        print(f"   - ì˜¤ë¥˜: {len(rebuild_result['errors'])}ê°œ")
        for error in rebuild_result['errors']:
            print(f"     âŒ {error}")
    
    if rebuild_result['success']:
        build_summary = rebuild_result['build_summary']
        print(f"\nğŸ“ˆ êµ¬ì¶• ìš”ì•½:")
        print(f"   - ì²˜ë¦¬ëœ ì‚¬ìš©ì: {build_summary['total_employees']}ëª…")
        print(f"   - ìƒì„±ëœ ë¬¸ì„œ: {build_summary['total_documents']}ê°œ")
        print(f"   - ê·¸ë£¹í•‘ ìˆ˜ì •: {'ì˜ˆ' if build_summary['grouping_fixed'] else 'ì•„ë‹ˆì˜¤'}")
        
        # ìƒ˜í”Œ ì‚¬ìš©ì ì •ë³´
        if build_summary['employee_sample']:
            print(f"   - ìƒ˜í”Œ ì‚¬ìš©ì:")
            for emp_id, info in build_summary['employee_sample'].items():
                print(f"     â€¢ {emp_id}: {info['rows']}í–‰ â†’ {info['years']}ë…„ì°¨")
    
    # 4. ìˆ˜ì • ê²°ê³¼ ê²€ì¦
    if rebuild_result['success']:
        print(f"\n3ï¸âƒ£ ìˆ˜ì • ê²°ê³¼ ê²€ì¦ ì¤‘...")
        verification = fixer.verify_fix("EMP-100001")
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼:")
        if verification.get("fix_success"):
            print(f"   âœ… ê·¸ë£¹í•‘ ë¬¸ì œê°€ ì„±ê³µì ìœ¼ë¡œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ìƒì„¸ ê²°ê³¼
            if verification["vectorstore_test"]["results_analysis"]:
                result = verification["vectorstore_test"]["results_analysis"][0]
                print(f"   ğŸ“Š 'EMP-100001' ê²€ìƒ‰ ê²°ê³¼:")
                print(f"     - ì—°ì°¨ ë²”ìœ„: {result['year_range']}")
                print(f"     - ì´ ê²½ë ¥ ë…„ìˆ˜: {result['total_years']}ë…„")
                print(f"     - í†µí•©ëœ ë¬¸ì„œ: {'ì˜ˆ' if result['appears_integrated'] else 'ì•„ë‹ˆì˜¤'}")
                print(f"     - ë©”íƒ€ë°ì´í„° í’ˆì§ˆ: {result['metadata_quality']['employee_id']}")
        else:
            print(f"   âŒ ì¼ë¶€ ë¬¸ì œê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            grouping = verification.get("grouping_verification", {})
            for key, value in grouping.items():
                status = "âœ…" if value else "âŒ"
                print(f"     {status} {key}: {value}")

if __name__ == "__main__":
    main()