# app/graphs/nodes/output_node.py

from typing import Dict, Any
import os
from app.graphs.state import ChatState

class OutputNode:
    """ìµœì¢… AI ì‘ë‹µ ìƒì„± ë…¸ë“œ"""
    
    def __init__(self):
        self.openai_client = None
        self._init_openai()
    
    def _init_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.openai_client = AsyncOpenAI(api_key=api_key)
                self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
                self.max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1000"))
                self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
                print("OutputNode OpenAI ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            print(f"OutputNode OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def process(self, state: ChatState) -> ChatState:
        """ì‹¤ì œ AI ì‘ë‹µ ìƒì„±"""
        print(f"ğŸ“ OutputNode AI ì‘ë‹µ ìƒì„± ì‹œì‘")
        print(f"ğŸ” State ë‚´ìš©: {list(state.keys())}")
        
        user_message = state.get("user_message", "")
        intent = state.get("intent", "general")
        profiling_data = state.get("profiling_data", {})
        memory_results = state.get("memory_results", [])
        connection_suggestions = state.get("connection_suggestions", [])
        
        print(f"ğŸ“¨ ì‚¬ìš©ì ë©”ì‹œì§€: '{user_message}'")
        print(f"ğŸ¯ ì˜ë„: {intent}")
        print(f"ğŸ”— OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ: {self.openai_client is not None}")
        
        # ì‚¬ìš©ì ì •ë³´ ì¶”ì¶œ
        user_info = state.get("user_info", {})
        user_name = user_info.get("name", "ì‚¬ìš©ì")
        projects = user_info.get("projects", [])
        
        print(f"ğŸ‘¤ ì‚¬ìš©ìëª…: {user_name}")
        print(f"ğŸ“ í”„ë¡œì íŠ¸ ìˆ˜: {len(projects)}")
        
        if not user_message:
            # ë¹ˆ ë©”ì‹œì§€ì¸ ê²½ìš° (ì´ˆê¸° ìƒíƒœ)
            print("âš ï¸ ë¹ˆ ë©”ì‹œì§€ - ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
            state["final_response"] = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            return state
        
        # AI ì‘ë‹µ ìƒì„±
        try:
            print("ğŸ¤– _generate_ai_response í˜¸ì¶œ ì‹œì‘")
            ai_response = await self._generate_ai_response(
                user_message=user_message,
                user_name=user_name,
                projects=projects,
                intent=intent,
                memory_results=memory_results,
                connection_suggestions=connection_suggestions
            )
            
            state["final_response"] = ai_response
            print(f"âœ… OutputNode AI ì‘ë‹µ ìƒì„± ì™„ë£Œ: {ai_response[:100]}...")
            
        except Exception as e:
            print(f"âŒ OutputNode AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {type(e).__name__}: {e}")
            import traceback
            print(f"ğŸ“‹ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            state["final_response"] = f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ '{user_message}'ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        return state
    
    async def _generate_ai_response(
        self, 
        user_message: str, 
        user_name: str, 
        projects: list,
        intent: str,
        memory_results: list,
        connection_suggestions: list
    ) -> str:
        """OpenAIë¥¼ í™œìš©í•œ ì‹¤ì œ AI ì‘ë‹µ ìƒì„±"""
        
        if not self.openai_client:
            return f"ì•ˆë…•í•˜ì„¸ìš” {user_name}ë‹˜! '{user_message}'ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. (OpenAI ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤)"
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_info = self._build_context(user_name, projects, memory_results, connection_suggestions)
        
        # ì˜ë„ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = self._get_system_prompt(intent)
        user_prompt = self._build_user_prompt(user_message, context_info)
        
        try:
            print("ğŸ¤– OpenAI ì‘ë‹µ ìƒì„± ì¤‘...")
            response = await self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            ai_response = response.choices[0].message.content.strip()
            print(f"âœ… OpenAI ì‘ë‹µ ì™„ë£Œ: {len(ai_response)}ì")
            return ai_response
            
        except Exception as e:
            print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤, {user_name}ë‹˜. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ìƒì„¸í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ë§ì”€ë“œë¦¬ë©´ '{user_message}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    def _build_context(self, user_name: str, projects: list, memory_results: list, connection_suggestions: list) -> str:
        """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±"""
        context_parts = [f"ì‚¬ìš©ìëª…: {user_name}"]
        
        # í”„ë¡œì íŠ¸ ê²½í—˜ ìš”ì•½
        if projects:
            domains = [p.get("domain", "") for p in projects]
            roles = [p.get("role", "") for p in projects]
            context_parts.append(f"ì£¼ìš” ê²½í—˜ ë„ë©”ì¸: {', '.join(set(domains))}")
            context_parts.append(f"ì£¼ìš” ì—­í• : {', '.join(set(roles))}")
            context_parts.append(f"ì´ í”„ë¡œì íŠ¸ ìˆ˜: {len(projects)}ê°œ")
        
        # ê³¼ê±° ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (TODO: ì‹¤ì œ êµ¬í˜„ ì‹œ ì¶”ê°€)
        if memory_results:
            context_parts.append(f"ê³¼ê±° ëŒ€í™” ì°¸ê³ ì‚¬í•­: {len(memory_results)}ê±´")
        
        # ì—°ê²° ì œì•ˆì‚¬í•­
        if connection_suggestions:
            context_parts.append(f"ì—°ê²° ì œì•ˆ: {', '.join(connection_suggestions)}")
        
        return '\n'.join(context_parts)
    
    def _get_system_prompt(self, intent: str) -> str:
        """ì˜ë„ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        base_prompt = """ë‹¹ì‹ ì€ SK AXì˜ ì „ë¬¸ ì»¤ë¦¬ì–´ ìƒë‹´ì‚¬ 'G.Navi'ì…ë‹ˆë‹¤. 
ì‚¬ë‚´ êµ¬ì„±ì›ë“¤ì˜ ì»¤ë¦¬ì–´ ì„±ì¥ê³¼ ë°œì „ì„ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ê¸°ë³¸ ì›ì¹™:
1. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ìƒë‹´
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ
3. ì‚¬ìš©ìì˜ ê²½í—˜ê³¼ ë°°ê²½ì„ ê³ ë ¤í•œ ê°œì¸í™”ëœ ë‹µë³€
4. 2-3ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë‹µë³€ (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€"""

        intent_specific = {
            "career_consultation": "\níŠ¹íˆ ì»¤ë¦¬ì–´ ìƒë‹´ì— ì§‘ì¤‘í•˜ì—¬ ì„±ì¥ ë°©í–¥, ìŠ¤í‚¬ ê°œë°œ, ì—­í•  ì „í™˜ ë“±ì— ëŒ€í•´ ì¡°ì–¸í•´ì£¼ì„¸ìš”.",
            "skill_development": "\níŠ¹íˆ ê¸°ìˆ  ë° ì—­ëŸ‰ ê°œë°œì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµ ë°©í–¥ê³¼ ë°©ë²•ì— ëŒ€í•´ ì¡°ì–¸í•´ì£¼ì„¸ìš”.",
            "project_advice": "\níŠ¹íˆ í”„ë¡œì íŠ¸ ê´€ë ¨ ì¡°ì–¸ì— ì§‘ì¤‘í•˜ì—¬ ê²½í—˜ í™œìš©ê³¼ í–¥í›„ ë°©í–¥ì— ëŒ€í•´ ì¡°ì–¸í•´ì£¼ì„¸ìš”.",
            "general": "\nì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ê°€ì¥ ë„ì›€ì´ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
        
        return base_prompt + intent_specific.get(intent, intent_specific["general"])
    
    def _build_user_prompt(self, user_message: str, context_info: str) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë°°ê²½ ì •ë³´ì…ë‹ˆë‹¤:

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{user_message}

=== ì‚¬ìš©ì ë°°ê²½ ì •ë³´ ===
{context_info}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
_output_node_instance = OutputNode()

# ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜
async def process(state: ChatState) -> ChatState:
    """ê¸°ì¡´ ë…¸ë“œ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ë˜í¼ í•¨ìˆ˜"""
    return await _output_node_instance.process(state)