# app/graphs/agents/retriever.py
"""
* @className : CareerEnsembleRetrieverAgent
* @description : ì»¤ë¦¬ì–´ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì—ì´ì „íŠ¸ ëª¨ë“ˆ
*                Vector Storeì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.
*                BM25 + OpenAI ì„ë² ë”© ì•™ìƒë¸” ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³ ,
*                ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ê°œì¸í™”ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
*
*                ì£¼ìš” ê¸°ëŠ¥:
*                1. BM25 + OpenAI ì„ë² ë”© ì•™ìƒë¸” ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
*                2. ì»¤ë¦¬ì–´ ì‚¬ë¡€ì™€ êµìœ¡ê³¼ì • ë°ì´í„° í†µí•© ê²€ìƒ‰
*                3. ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ê°œì¸í™”ëœ ê²€ìƒ‰ ê²°ê³¼ ì œê³µ
*                4. ChromaDBë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰
*
*                ê²€ìƒ‰ ëŒ€ìƒ:
*                - ì»¤ë¦¬ì–´ ì‚¬ë¡€: ê²½ë ¥ ì „í™˜, ì„±ì¥ ìŠ¤í† ë¦¬, ì§ë¬´ ê²½í—˜ë‹´
*                - êµìœ¡ê³¼ì •: AI/ë°ì´í„° ë¶„ì•¼ ê°•ì˜, ì‹¤ë¬´ êµìœ¡ í”„ë¡œê·¸ë¨
*                - í•™ìŠµ ê²½ë¡œ: ë‹¨ê³„ë³„ ì„±ì¥ ë¡œë“œë§µ
*
*                ì£¼ìš” ê¸°ìˆ :
*                - Ensemble Retriever (BM25 + Vector Search)
*                - OpenAI Embeddings with Cache
*                - ChromaDB Persistent Storage
*                - Query Optimization & Filtering
"""

import os
import json
import re
import requests
import logging
import chromadb
from typing import Dict, List, Any
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.schema import Document
from datetime import datetime, timedelta
from .k8s_chroma_adapter import K8sChromaRetriever

from dotenv import load_dotenv
load_dotenv()


# ==================== ê²½ë¡œ ì„¤ì • (ìˆ˜ì • í•„ìš”ì‹œ ì—¬ê¸°ë§Œ ë³€ê²½) ====================
class PathConfig:
    """
    ëª¨ë“  ê²½ë¡œ ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    K8s í™˜ê²½ì—ì„œëŠ” PVC ë§ˆìš´íŠ¸ ê²½ë¡œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # app ë””ë ‰í† ë¦¬
    
    @classmethod
    def _get_k8s_pvc_path(cls) -> str:
        """K8s PVC ë§ˆìš´íŠ¸ ê²½ë¡œ ë°˜í™˜"""
        return os.environ.get('APP_STORAGE_PVC_PATH', '/mnt/gnavi')
    
    @classmethod
    def _is_k8s_environment(cls) -> bool:
        """K8s í™˜ê²½ì¸ì§€ í™•ì¸"""
        pvc_path = cls._get_k8s_pvc_path()
        return os.path.exists(pvc_path)
    
    @classmethod
    def _get_app_root_dir(cls) -> str:
        """app ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë°˜í™˜ (graphs/agentsì—ì„œ appê¹Œì§€ ì˜¬ë¼ê°€ê¸°)"""
        # í˜„ì¬ íŒŒì¼ì´ app/graphs/agents/retriever.py ë¼ë©´
        # app ë””ë ‰í† ë¦¬ê¹Œì§€ ì˜¬ë¼ê°€ì•¼ í•¨
        current_dir = os.path.dirname(__file__)  # app/graphs/agents/
        app_dir = os.path.dirname(os.path.dirname(current_dir))  # app/
        return app_dir
    
    @classmethod
    def _get_smart_docs_path(cls, filename: str) -> str:
        """K8s í™˜ê²½ì´ë©´ PVC ê²½ë¡œ, ì•„ë‹ˆë©´ ë¡œì»¬ app/storage/docs ê²½ë¡œ ë°˜í™˜"""
        if cls._is_k8s_environment():
            # K8s í™˜ê²½: /mnt/gnavi/docs/filename
            k8s_path = os.path.join(cls._get_k8s_pvc_path(), 'docs', filename)
            if os.path.exists(k8s_path):
                return k8s_path
            # K8s í™˜ê²½ì´ì§€ë§Œ PVCì— íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì»¬ í´ë°±
            local_fallback = os.path.join(cls._get_app_root_dir(), 'storage', 'docs', filename)
            if os.path.exists(local_fallback):
                return local_fallback
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ K8s ê²½ë¡œ ë°˜í™˜ (ì›ë˜ ì˜ë„ëŒ€ë¡œ)
            return k8s_path
        else:
            # ë¡œì»¬ í™˜ê²½: app/storage/docs/filename  
            return os.path.join(cls._get_app_root_dir(), 'storage', 'docs', filename)
    
    # ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ (Chroma DB ì €ì¥ì†Œ) - ê¸°ì¡´ ë°©ì‹ ìœ ì§€
    CAREER_VECTOR_STORE = "../../storage/vector_stores/career_data"
    EDUCATION_VECTOR_STORE = "../../storage/vector_stores/education_courses"
    NEWS_VECTOR_STORE = "../../storage/vector_stores/news_data"
    
    # ìºì‹œ ê²½ë¡œ (ì„ë² ë”© ìºì‹œ) - ê¸°ì¡´ ë°©ì‹ ìœ ì§€  
    CAREER_EMBEDDING_CACHE = "../../storage/cache/embedding_cache"
    EDUCATION_EMBEDDING_CACHE = "../../storage/cache/education_embedding_cache"
    
    # ë¬¸ì„œ ê²½ë¡œ (JSON ë°ì´í„° íŒŒì¼ë“¤) - ìŠ¤ë§ˆíŠ¸ ê²½ë¡œ ì ìš© (ê¸°ì¡´ ì†ì„±ëª… ìœ ì§€)
    @classmethod
    def _init_paths(cls):
        """ê²½ë¡œ ì´ˆê¸°í™” - ëª¨ë“ˆ ë¡œë“œ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰"""
        cls.CAREER_DOCS = cls._get_smart_docs_path("career_history.json")
        cls.EDUCATION_DOCS = cls._get_smart_docs_path("education_courses.json") 
        cls.SKILL_MAPPING = cls._get_smart_docs_path("skill_education_mapping.json")
        cls.COURSE_DEDUPLICATION = cls._get_smart_docs_path("course_deduplication_index.json")
        cls.COMPANY_VISION = cls._get_smart_docs_path("company_vision.json")
        cls.MYSUNI_DETAILED = cls._get_smart_docs_path("mysuni_courses_detailed.json")
        cls.COLLEGE_DETAILED = cls._get_smart_docs_path("college_courses_detailed.json")
    
    @classmethod
    def get_abs_path(cls, relative_path: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        return os.path.abspath(os.path.join(os.path.dirname(__file__), relative_path))
    
    @classmethod
    def log_current_environment(cls):
        """í˜„ì¬ í™˜ê²½ ì •ë³´ ë¡œê·¸ ì¶œë ¥"""
        env_type = "K8s PVC" if cls._is_k8s_environment() else "ë¡œì»¬"
        print(f" [PathConfig] í™˜ê²½ ê°ì§€: {env_type}")
        print(f"ğŸ“ [PathConfig] App ë£¨íŠ¸ ë””ë ‰í† ë¦¬: {cls._get_app_root_dir()}")
        if cls._is_k8s_environment():
            print(f"ğŸ“ [PathConfig] PVC ê²½ë¡œ: {cls._get_k8s_pvc_path()}")
        print(f" [PathConfig] ì»¤ë¦¬ì–´ ë¬¸ì„œ: {cls.CAREER_DOCS}")
        print(f" [PathConfig] êµìœ¡ê³¼ì • ë¬¸ì„œ: {cls.EDUCATION_DOCS}")
        print(f" [PathConfig] ìŠ¤í‚¬ ë§¤í•‘: {cls.SKILL_MAPPING}")
        print(f" [PathConfig] ì¤‘ë³µì œê±° ì¸ë±ìŠ¤: {cls.COURSE_DEDUPLICATION}")
        print(f" [PathConfig] íšŒì‚¬ ë¹„ì „: {cls.COMPANY_VISION}")
        print(f"ğŸ“ [PathConfig] mySUNI ìƒì„¸: {cls.MYSUNI_DETAILED}")
        print(f"ğŸ« [PathConfig] College ìƒì„¸: {cls.COLLEGE_DETAILED}")
        return env_type
    
    @classmethod
    def check_files_exist(cls):
        """ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        files_to_check = {
            "ì»¤ë¦¬ì–´ ë¬¸ì„œ": cls.CAREER_DOCS,
            "êµìœ¡ê³¼ì • ë¬¸ì„œ": cls.EDUCATION_DOCS,
            "ìŠ¤í‚¬ ë§¤í•‘": cls.SKILL_MAPPING,
            "ì¤‘ë³µì œê±° ì¸ë±ìŠ¤": cls.COURSE_DEDUPLICATION,
            "íšŒì‚¬ ë¹„ì „": cls.COMPANY_VISION,
            "mySUNI ìƒì„¸": cls.MYSUNI_DETAILED,
            "College ìƒì„¸": cls.COLLEGE_DETAILED
        }
        
        missing_files = []
        existing_files = []
        
        for name, path in files_to_check.items():
            if os.path.exists(path):
                existing_files.append(f"{name}: {path}")
            else:
                missing_files.append(f"{name}: {path}")
        
        print(" [PathConfig] íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸:")
        for file_info in existing_files:
            print(f"  {file_info}")
        for file_info in missing_files:
            print(f"  {file_info}")
        
        return len(missing_files) == 0

# í´ë˜ìŠ¤ ë¡œë“œ ì‹œ ê²½ë¡œ ì´ˆê¸°í™” ì‹¤í–‰
PathConfig._init_paths()

# ==================== ğŸ“‚ ê²½ë¡œ ì„¤ì • ë ====================

class CareerEnsembleRetrieverAgent:
    """
    ì»¤ë¦¬ì–´ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì—ì´ì „íŠ¸
    
    BM25 + LLM ì„ë² ë”© ì•™ìƒë¸”ì„ í™œìš©í•˜ì—¬ ì»¤ë¦¬ì–´ ì‚¬ë¡€ì™€ êµìœ¡ê³¼ì •
    
     ê²€ìƒ‰ ê²°ê³¼:
    - ì»¤ë¦¬ì–´ ì‚¬ë¡€: ìµœëŒ€ 2ê°œê¹Œì§€ ê²€ìƒ‰
    - êµìœ¡ê³¼ì •: ìµœëŒ€ 2ê°œê¹Œì§€ ê²€ìƒ‰
    """
    def __init__(self, persist_directory: str = None, cache_directory: str = None):
        """
        CareerEnsembleRetrieverAgent ì´ˆê¸°í™”
        
        Args:
            persist_directory: ì»¤ë¦¬ì–´ ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ (ê¸°ë³¸ê°’: PathConfig.CAREER_VECTOR_STORE)
            cache_directory: ì»¤ë¦¬ì–´ ì„ë² ë”© ìºì‹œ ê²½ë¡œ (ê¸°ë³¸ê°’: PathConfig.CAREER_EMBEDDING_CACHE)
        """
        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = logging.getLogger(__name__)
        
        # í™˜ê²½ ì •ë³´ ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        env_type = PathConfig.log_current_environment()
        self.is_k8s = PathConfig._is_k8s_environment()
        print(f"[CareerRetrieverAgent] í™˜ê²½: {env_type}, K8s: {self.is_k8s}")
        
        # ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)
        self.persist_directory = PathConfig.get_abs_path(
            persist_directory or PathConfig.CAREER_VECTOR_STORE
        )
        self.career_cache_directory = PathConfig.get_abs_path(
            cache_directory or PathConfig.CAREER_EMBEDDING_CACHE
        )
        self.base_dir = PathConfig.BASE_DIR
        
        # ë””ë ‰í† ë¦¬ ìƒì„± (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
        if not self.is_k8s:
            os.makedirs(self.persist_directory, exist_ok=True)
            os.makedirs(self.career_cache_directory, exist_ok=True)

        # ì»¤ë¦¬ì–´ ì „ìš© ì„ë² ë”© ì„¤ì •
        self.base_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            dimensions=1536
        )
        
        # ìºì‹œ ì„¤ì • (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
        if not self.is_k8s:
            self.career_cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
                self.base_embeddings,
                LocalFileStore(self.career_cache_directory),
                namespace="career_embeddings"
            )
        else:
            # K8s í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì—†ì´ ì§ì ‘ ì„ë² ë”© ì‚¬ìš©
            self.career_cached_embeddings = self.base_embeddings
        
        # êµìœ¡ê³¼ì • ì „ìš© ì„ë² ë”© ì„¤ì •
        if not self.is_k8s:
            self.education_cache_directory = PathConfig.get_abs_path(PathConfig.EDUCATION_EMBEDDING_CACHE)
            os.makedirs(self.education_cache_directory, exist_ok=True)
            self.education_cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
                self.base_embeddings,
                LocalFileStore(self.education_cache_directory),
                namespace="education_embeddings"
            )
        else:
            # K8s í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì—†ì´ ì§ì ‘ ì„ë² ë”© ì‚¬ìš©
            self.education_cached_embeddings = self.base_embeddings
        
        self.vectorstore = None
        self.ensemble_retriever = None
        
        # êµìœ¡ê³¼ì • ê´€ë ¨ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)
        if not self.is_k8s:
            self.education_persist_dir = PathConfig.get_abs_path(PathConfig.EDUCATION_VECTOR_STORE)
        self.education_docs_path = PathConfig.EDUCATION_DOCS
        self.skill_mapping_path = PathConfig.SKILL_MAPPING
        self.deduplication_index_path = PathConfig.COURSE_DEDUPLICATION
        
        # íšŒì‚¬ ë¹„ì „ ê´€ë ¨ ê²½ë¡œ ì„¤ì •
        self.company_vision_path = PathConfig.COMPANY_VISION
        
        # ì§€ì—° ë¡œë”© ì†ì„±
        self.education_vectorstore = None
        self.skill_education_mapping = None
        self.course_deduplication_index = None
        
        self._load_vectorstore_and_retriever()

    def _load_vectorstore_and_retriever(self):
        """ë²¡í„°ìŠ¤í† ì–´ì™€ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ë¡œë“œ (í™˜ê²½ë³„ ë¶„ê¸°)"""
        if self.is_k8s:
            self._load_k8s_vectorstore_and_retriever()
        else:
            self._load_local_vectorstore_and_retriever()

    def _load_k8s_vectorstore_and_retriever(self):
        """K8s í™˜ê²½: ì™¸ë¶€ ChromaDB ì‚¬ìš©"""
        
        # í†µí•© K8sChromaRetriever ì‚¬ìš©
        self.vectorstore = K8sChromaRetriever("career_history", self.career_cached_embeddings, k=3)
        # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection_info = self.vectorstore.get_collection_info()
        if collection_info.get("status") == "success":
            print(f"[K8s ChromaDB] ì—°ê²° ì„±ê³µ: {collection_info.get('document_count')}ê°œ ë¬¸ì„œ")
        else:
            print(f"K8s ChromaDB] ì—°ê²° ì‹¤íŒ¨: {collection_info.get('message')}")
        # LLM ì„ë² ë”© ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰ ê²°ê³¼ë¥¼ 3ê°œë¡œ ì œí•œ)
        embedding_retriever = self.vectorstore
        
        # BM25ìš© docs ë¡œë“œ (JSON íŒŒì¼ì€ ì—¬ì „íˆ ì‚¬ìš©)
        docs_path = PathConfig.CAREER_DOCS
        all_docs = []
        try:
            with open(docs_path, 'r', encoding='utf-8') as f:
                json_docs = json.load(f)
                all_docs = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in json_docs]
            self.logger.info(f"BM25ìš© career_docs.json ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)}) - ê²½ë¡œ: {docs_path}")
        except Exception as e:
            self.logger.warning(f"BM25ìš© career_docs.json ë¡œë“œ ì‹¤íŒ¨: {e} - ê²½ë¡œ: {docs_path}")
        
        # ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±
        retrievers = [embedding_retriever]
        weights = [1.0]
        if all_docs:
            bm25_retriever = BM25Retriever.from_documents(all_docs)
            bm25_retriever.k = 3  # BM25ë„ 3ê°œë¡œ ì œí•œ
            retrievers.append(bm25_retriever)
            weights = [0.3, 0.7]  # K8s ChromaDB: 30%, BM25: 70%
        
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=retrievers,
            weights=weights
        )
        self.logger.info(f"K8s Career ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ (JSON ë¬¸ì„œ ìˆ˜: {len(all_docs)})")
        print(f" [K8s ì»¤ë¦¬ì–´ ì‚¬ë¡€ VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_local_vectorstore_and_retriever(self):
        """ë¡œì»¬ í™˜ê²½: ê¸°ì¡´ ë¡œì»¬ ChromaDB ì‚¬ìš©"""
        
        # Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.career_cached_embeddings,
            collection_name="career_history"
        )
        # LLM ì„ë² ë”© ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰ ê²°ê³¼ë¥¼ 2ê°œë¡œ ì œí•œ)
        embedding_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 2}
        )
        # BM25ìš© docs ë¡œë“œ
        docs_path = PathConfig.CAREER_DOCS
        all_docs = []
        try:
            with open(docs_path, 'r', encoding='utf-8') as f:
                json_docs = json.load(f)
                all_docs = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in json_docs]
            self.logger.info(f"BM25ìš© career_docs.json ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)}) - ê²½ë¡œ: {docs_path}")
        except Exception as e:
            self.logger.warning(f"BM25ìš© career_docs.json ë¡œë“œ ì‹¤íŒ¨: {e} - ê²½ë¡œ: {docs_path}")
        
        retrievers = [embedding_retriever]
        weights = [1.0]
        if all_docs:
            bm25_retriever = BM25Retriever.from_documents(all_docs)
            bm25_retriever.k = 2  # BM25ë„ 2ê°œë¡œ ì œí•œ
            retrievers.append(bm25_retriever)
            weights = [0.3, 0.7]
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=retrievers,
            weights=weights
        )
        self.logger.info(f"ë¡œì»¬ Career ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)})")
        print(f"[ë¡œì»¬ ì»¤ë¦¬ì–´ ì‚¬ë¡€ VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")

    def retrieve(self, query: str, k: int = 3):
        """ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¡œ ê²€ìƒ‰"""
        print(f" [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        
        if not self.ensemble_retriever:
            print(f"[ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ê°€ ì—†ìŒ")
            return []
        
        # ë™ì ìœ¼ë¡œ k ê°’ ì„¤ì •
        search_k = max(k * 2, 10)  # ìš”ì²­ëœ ê°œìˆ˜ì˜ 2ë°° ë˜ëŠ” ìµœì†Œ 10ê°œ
        
        # Chroma ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²°ê³¼ ê²€ìƒ‰
        embedding_docs = self.vectorstore.similarity_seaerch(query, k=search_k)
        print(f"DEBUG - ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼: {len(embedding_docs)}ê°œ")
        
        # BM25 ê²€ìƒ‰ë„ ë” ë§ì€ ê²°ê³¼ ë°˜í™˜
        bm25_docs = []
        if hasattr(self.ensemble_retriever, 'retrievers') and len(self.ensemble_retriever.retrievers) > 1:
            try:
                # BM25 ë¦¬íŠ¸ë¦¬ë²„ì˜ k ê°’ì„ ë™ì ìœ¼ë¡œ ì„¤ì •
                bm25_retriever = self.ensemble_retriever.retrievers[1]
                original_k = bm25_retriever.k
                bm25_retriever.k = search_k
                bm25_docs = bm25_retriever.invoke(query)
                bm25_retriever.k = original_k  # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
                print(f"DEBUG - BM25 ê²€ìƒ‰ ê²°ê³¼: {len(bm25_docs)}ê°œ")
            except Exception as e:
                print(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê²°í•©
        doc_scores = {} 
        RRF_CONSTANT = 60

        # ì„ë² ë”© ê²°ê³¼ (ê°€ì¤‘ì¹˜ 0.3)
        for rank, doc in enumerate(embedding_docs):
            content_hash = hash(doc.page_content)
            rrf_score = 1.0 / (rank + RRF_CONSTANT)
            weighted_score = rrf_score * 0.3  # Vector Search ê°€ì¤‘ì¹˜

            if content_hash in doc_scores:
                # ì´ë¯¸ ìˆëŠ” ë¬¸ì„œë©´ ì ìˆ˜ ëˆ„ì  (ì—¬ëŸ¬ retrieverì—ì„œ ë‚˜ì˜¨ ê²½ìš°)
                doc_scores[content_hash] = (
                    doc_scores[content_hash][0] + weighted_score,
                    doc_scores[content_hash][1]
                )
            else:
                doc_scores[content_hash] = (weighted_score, doc)

        # BM25 ê²°ê³¼ (ê°€ì¤‘ì¹˜ 0.7)
        for rank, doc in enumerate(bm25_docs):
            content_hash = hash(doc.page_content)
            rrf_score = 1.0 / (rank + RRF_CONSTANT)
            weighted_score = rrf_score * 0.7  # BM25 ê°€ì¤‘ì¹˜

            if content_hash in doc_scores:
                # ì´ë¯¸ ìˆëŠ” ë¬¸ì„œë©´ ì ìˆ˜ ëˆ„ì 
                doc_scores[content_hash] = (
                    doc_scores[content_hash][0] + weighted_score,
                    doc_scores[content_hash][1]
                )
            else:
                doc_scores[content_hash] = (weighted_score, doc)

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœì¢… ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        sorted_docs = sorted(doc_scores.values(), key=lambda x: x[0], reverse=True)
        all_docs = [doc for score, doc in sorted_docs]

        print(f"DEBUG - RRF ê²°í•© ê²°ê³¼: {len(all_docs)}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
        
        # ìµœê·¼ í‚¤ì›Œë“œ ê°ì§€ ë° ì—°ë„ ì¶”ì¶œ
        recent_keywords = ['ìµœê·¼', 'ìµœì‹ ', 'recent', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ìƒˆë¡œìš´', 'ì‹ ê·œ', 'íŠ¸ë Œë“œ']
        is_recent_query = any(keyword in query.lower() for keyword in recent_keywords)
        
        # ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ
        years_info = self._extract_years_from_query(query)
        
        # "ì‹ ì…" ë˜ëŠ” "ì…ì‚¬" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹œì‘ ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        new_hire_keywords = ['ì‹ ì…', 'ì…ì‚¬', 'ìƒˆë¡œ', 'ì‹ ê·œ', 'ì‹œì‘', 'ì²˜ìŒ']
        focus_on_start_year = any(keyword in query.lower() for keyword in new_hire_keywords)
        
        if is_recent_query or years_info.get('n_years') or years_info.get('specific_year'):
            current_year = datetime.now().year
            
            # ì—°ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ 3ë…„
            if years_info.get('n_years'):
                min_year = current_year - years_info['n_years']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ì—°ë„: ìµœê·¼ {years_info['n_years']}ë…„ ({min_year}ë…„ ì´í›„)")
            elif years_info.get('specific_year'):
                min_year = years_info['specific_year']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ íŠ¹ì • ì—°ë„: {min_year}ë…„ ì´í›„")
            else:
                min_year = current_year - 3  # ê¸°ë³¸ê°’: ìµœê·¼ 3ë…„
                self.logger.info(f"ê¸°ë³¸ ì„¤ì •: ìµœê·¼ 3ë…„ ({min_year}ë…„ ì´í›„)")
            
            if focus_on_start_year:
                # ì‹ ì…/ì…ì‚¬ ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš°: ì‹œì‘ ì—°ë„ ê¸°ì¤€
                filtered_docs = []
                for doc in all_docs:
                    try:
                        metadata = doc.metadata or {}
                        start_year = metadata.get('activity_start_year')
                        
                        if start_year and isinstance(start_year, int) and start_year >= min_year:
                            filtered_docs.append(doc)
                            self.logger.debug(f"í¬í•¨: {start_year}ë…„ ì‹œì‘ í™œë™ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                        else:
                            self.logger.debug(f"ì œì™¸: {start_year}ë…„ ì‹œì‘ í™œë™ (ìµœì†Œ ê¸°ì¤€: {min_year}ë…„ ì´í›„ ì‹œì‘) (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                    except Exception as e:
                        self.logger.warning(f"ë¬¸ì„œ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
            else:
                # ì¼ë°˜ ìµœê·¼ ì¿¼ë¦¬ì¸ ê²½ìš°: ìµœê·¼ í™œë™ì´ ìˆì—ˆë˜ ì§ì›ë“¤ ì¤‘ì—ì„œ
                self.logger.info(f"ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì‹œì‘: {min_year}ë…„ ì´í›„ **í™œë™ì´ ìˆì—ˆë˜** ë°ì´í„° ê²€ìƒ‰...")
                filtered_docs = []
                for doc in all_docs:
                    try:
                        metadata = doc.metadata or {}
                        
                        # í™œë™ ì—°ë„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§€ì •ëœ ê¸°ê°„ ë‚´ í™œë™ì´ ìˆëŠ”ì§€ í™•ì¸
                        activity_years = metadata.get('activity_years_list', [])
                        if activity_years and isinstance(activity_years, list):
                            recent_activity_years = [year for year in activity_years 
                                                   if isinstance(year, int) and year >= min_year]
                            if recent_activity_years:
                                filtered_docs.append(doc)
                                self.logger.debug(f"í¬í•¨: ìµœê·¼ í™œë™ ì—°ë„ {recent_activity_years} (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                                continue
                        
                        # í´ë°±: ì¢…ë£Œ ì—°ë„ê°€ ìµœê·¼ì¸ì§€ í™•ì¸
                        end_year = metadata.get('activity_end_year')
                        if end_year and isinstance(end_year, int) and end_year >= min_year:
                            filtered_docs.append(doc)
                            self.logger.debug(f"í¬í•¨: {end_year}ë…„ ì¢…ë£Œ í™œë™ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                        else:
                            self.logger.debug(f"ì œì™¸: ìµœê·¼ í™œë™ ì—†ìŒ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                    except Exception as e:
                        self.logger.warning(f"ë¬¸ì„œ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
            
            final_docs = filtered_docs[:k]
        else:
            final_docs = all_docs[:k]
        
        # íšŒì‚¬ ë¹„ì „ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€ (ì»¤ë¦¬ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°)
        career_keywords = ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì„±ì¥', 'ë°œì „', 'ëª©í‘œ', 'ë°©í–¥', 'ê³„íš', 'ë¹„ì „', 'ë¯¸ë˜', 'íšŒì‚¬', 'ì¡°ì§']
        if any(keyword in query.lower() for keyword in career_keywords):
            try:
                company_vision_context = self.get_company_vision_context()
                if company_vision_context:
                    # íšŒì‚¬ ë¹„ì „ì„ Document í˜•íƒœë¡œ ì¶”ê°€
                    vision_doc = Document(
                        page_content=company_vision_context,
                        metadata={"type": "company_vision", "source": "company_vision.json"}
                    )
                    final_docs.append(vision_doc)
                    self.logger.info("íšŒì‚¬ ë¹„ì „ ì •ë³´ê°€ ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                self.logger.warning(f"íšŒì‚¬ ë¹„ì „ ì •ë³´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        
        return final_docs
    
    def _extract_years_from_query(self, query: str) -> dict:
        """ì¿¼ë¦¬ì—ì„œ ì—°ë„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        years_info = {'n_years': None, 'specific_year': None}
        
        # "ìµœê·¼ Në…„" íŒ¨í„´ ë§¤ì¹­
        year_patterns = [
            r'ìµœê·¼\s*(\d+)\s*ë…„',  # ìµœê·¼ 5ë…„
            r'ì§€ë‚œ\s*(\d+)\s*ë…„',  # ì§€ë‚œ 5ë…„
            r'ê³¼ê±°\s*(\d+)\s*ë…„',  # ê³¼ê±° 5ë…„
            r'(\d+)\s*ë…„\s*ë™ì•ˆ',  # 5ë…„ ë™ì•ˆ
            r'(\d+)\s*ë…„\s*ê°„',    # 5ë…„ê°„
            r'(\d+)\s*ë…„\s*ì´ë‚´',  # 5ë…„ ì´ë‚´
            r'(\d+)\s*ë…„\s*ì‚¬ì´',  # 5ë…„ ì‚¬ì´
        ]
        
        for pattern in year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    n_years = int(match.group(1))
                    if 1 <= n_years <= 50:  # ìœ íš¨í•œ ë²”ìœ„
                        years_info['n_years'] = n_years
                        break
                except ValueError:
                    continue
        
        # íŠ¹ì • ì—°ë„ íŒ¨í„´ ë§¤ì¹­ (ì˜ˆ: "2020ë…„ ì´í›„", "2023ë…„ë¶€í„°")
        specific_year_patterns = [
            r'(\d{4})\s*ë…„\s*ì´í›„',  # 2020ë…„ ì´í›„
            r'(\d{4})\s*ë…„\s*ë¶€í„°',  # 2020ë…„ë¶€í„°
            r'(\d{4})\s*ë…„\s*ì´ìƒ',  # 2020ë…„ ì´ìƒ
            r'(\d{4})\s*ì´í›„',      # 2020 ì´í›„
            r'(\d{4})\s*ë¶€í„°',      # 2020 ë¶€í„°
        ]
        
        for pattern in specific_year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    year = int(match.group(1))
                    if 2000 <= year <= datetime.now().year:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        years_info['specific_year'] = year
                        break
                except ValueError:
                    continue
        
        return years_info
    
    def _get_latest_year_from_doc(self, doc: Document) -> int:
        """ë¬¸ì„œì—ì„œ ê°€ì¥ ìµœì‹  ì—°ë„ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        metadata = doc.metadata or {}
        
        # 1. í™œë™ ì¢…ë£Œ ì—°ë„ ìš°ì„  í™•ì¸ (ê°€ì¥ ì‹ ë¢°í•  ë§Œí•œ ì •ë³´)
        end_year = metadata.get('activity_end_year')
        if end_year and isinstance(end_year, int) and 2000 <= end_year <= 2030:
            return end_year
        
        # 2. í™œë™ ì—°ë„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœì‹  ì—°ë„ í™•ì¸
        activity_years = metadata.get('activity_years_list', [])
        if activity_years and isinstance(activity_years, list):
            try:
                valid_years = [year for year in activity_years if isinstance(year, int) and 2000 <= year <= 2030]
                if valid_years:
                    return max(valid_years)
            except:
                pass
        
        # 3. í™œë™ ì‹œì‘ ì—°ë„ í™•ì¸ (ì¢…ë£Œ ì—°ë„ê°€ ì—†ëŠ” ê²½ìš°)
        start_year = metadata.get('activity_start_year')
        if start_year and isinstance(start_year, int) and 2000 <= start_year <= 2030:
            return start_year
        
        # 4. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
        return self._extract_year_from_doc(doc)
    
    def _extract_year_from_doc(self, doc: Document) -> int:
        """ë¬¸ì„œì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ì°¾ê¸°
        metadata = doc.metadata or {}
        
        # ì§ì ‘ì ì¸ ì—°ë„ í•„ë“œë“¤ í™•ì¸
        year_fields = ['year', 'start_year', 'end_year', 'graduation_year', 'project_year']
        for field in year_fields:
            if field in metadata and metadata[field]:
                try:
                    year = int(metadata[field])
                    if 1980 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        return year
                except:
                    continue
        
        # ë‚ ì§œ í˜•ì‹ì—ì„œ ì—°ë„ ì¶”ì¶œ
        date_fields = ['date', 'start_date', 'end_date', 'created_at', 'updated_at']
        for field in date_fields:
            if field in metadata and metadata[field]:
                try:
                    date_str = str(metadata[field])
                    # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD í˜•ì‹ ì²˜ë¦¬
                    year_match = re.search(r'(\d{4})', date_str)
                    if year_match:
                        year = int(year_match.group(1))
                        if 1980 <= year <= 2030:
                            return year
                except:
                    continue
        
        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì—°ë„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        content = doc.page_content or ""
        # "2023ë…„", "2024ë…„" ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
        year_patterns = [
            r'(\d{4})ë…„',  # 2023ë…„
            r'(\d{4})\s*-\s*(\d{4})',  # 2022-2024
            r'(\d{4})/(\d{1,2})',  # 2023/12
            r'(\d{4})\.(\d{1,2})'   # 2023.12
        ]
        
        years = []
        for pattern in year_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        # ì—¬ëŸ¬ ê·¸ë£¹ì´ ìˆëŠ” ê²½ìš° ëª¨ë“  ì—°ë„ ìˆ˜ì§‘
                        for group in match:
                            year = int(group)
                            if 1980 <= year <= 2030:
                                years.append(year)
                    else:
                        year = int(match)
                        if 1980 <= year <= 2030:
                            years.append(year)
                except:
                    continue
        
        # ê°€ì¥ ìµœê·¼ ì—°ë„ ë°˜í™˜
        return max(years) if years else None

    def _load_education_resources(self):
        """êµìœ¡ê³¼ì • ë¦¬ì†ŒìŠ¤ ì§€ì—° ë¡œë”©"""
        if self.education_vectorstore is None:
            self._initialize_education_vectorstore()
        if self.skill_education_mapping is None:
            self._load_skill_education_mapping()
        if self.course_deduplication_index is None:
            self._load_deduplication_index()
    
    def _initialize_education_vectorstore(self):
        """êµìœ¡ê³¼ì • VectorDB ì´ˆê¸°í™” (í™˜ê²½ë³„ ë¶„ê¸°)"""
        if self.is_k8s:
            self._initialize_k8s_education_vectorstore()
        else:
            self._initialize_local_education_vectorstore()
    
    def _initialize_k8s_education_vectorstore(self):
        """K8s í™˜ê²½: ì™¸ë¶€ êµìœ¡ê³¼ì • ChromaDB ì´ˆê¸°í™”"""
        try:
            print(" [K8s êµìœ¡ê³¼ì • ChromaDB] ì™¸ë¶€ ChromaDB ì—°ê²° ì¤‘...")
            self.education_vectorstore = K8sChromaRetriever("education_courses", self.education_cached_embeddings, k=3)
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = self.education_vectorstore.get_collection_info()
            if collection_info.get("status") == "success":
                print(f" [K8s êµìœ¡ê³¼ì • ChromaDB] ì—°ê²° ì„±ê³µ: {collection_info.get('document_count')}ê°œ ë¬¸ì„œ")
            else:
                print(f" [K8s êµìœ¡ê³¼ì • ChromaDB] ì—°ê²° ì‹¤íŒ¨: {collection_info.get('message')}")
        except Exception as e:
            self.logger.error(f"K8s êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f" [K8s êµìœ¡ê³¼ì • ChromaDB] ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.education_vectorstore = None
    
    def _initialize_local_education_vectorstore(self):
        """ë¡œì»¬ í™˜ê²½: ê¸°ì¡´ ë¡œì»¬ êµìœ¡ê³¼ì • ChromaDB ì´ˆê¸°í™”"""
        try:
            if os.path.exists(self.education_persist_dir):
                self.education_vectorstore = Chroma(
                    persist_directory=self.education_persist_dir,
                    embedding_function=self.education_cached_embeddings,
                    collection_name="education_courses"
                )
                self.logger.info("ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì™„ë£Œ")
                print(f" [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("ë¡œì»¬ êµìœ¡ê³¼ì • VectorDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"  [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ì—†ìŒ - JSON íŒŒì¼ë¡œ í´ë°± ê²€ìƒ‰ ì§„í–‰")
        except Exception as e:
            self.logger.error(f"ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f" [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.education_vectorstore = None
    
    def _load_skill_education_mapping(self):
        """ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ"""
        try:
            if os.path.exists(self.skill_mapping_path):
                with open(self.skill_mapping_path, "r", encoding="utf-8") as f:
                    self.skill_education_mapping = json.load(f)
                self.logger.info(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.skill_education_mapping)}ê°œ ìŠ¤í‚¬")
            else:
                self.skill_education_mapping = {}
                self.logger.warning("ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.skill_education_mapping = {}
    
    def _load_deduplication_index(self):
        """ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if os.path.exists(self.deduplication_index_path):
                with open(self.deduplication_index_path, "r", encoding="utf-8") as f:
                    self.course_deduplication_index = json.load(f)
                self.logger.info(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.course_deduplication_index)}ê°œ ê·¸ë£¹")
            else:
                self.course_deduplication_index = {}
                self.logger.warning("ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.course_deduplication_index = {}
    
    def search_education_courses(self, query: str, user_profile: Dict, intent_analysis: Dict, max_results: int = 15) -> Dict:
        """êµìœ¡ê³¼ì • ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜ - ì§€ì •ëœ ê°œìˆ˜ê¹Œì§€ ê²€ìƒ‰"""
        print(f" [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì‹œì‘ - '{query}' (ìµœëŒ€ {max_results}ê°œ)")
        print(f" [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        self._load_education_resources()
        
        try:
            # ì‚¬ìš©ìì˜ êµìœ¡ê³¼ì • ì†ŒìŠ¤ ì„ í˜¸ë„ í™•ì¸
            preferred_source = self._get_preferred_education_source(query, user_profile, intent_analysis)
            
            # 1ë‹¨ê³„: ìŠ¤í‚¬ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°ë§
            skill_based_courses = self._skill_based_course_filter(user_profile, intent_analysis)
            
            # 2ë‹¨ê³„: VectorDB ì˜ë¯¸ì  ê²€ìƒ‰ (VectorDBê°€ ì—†ìœ¼ë©´ JSON í´ë°±)
            semantic_matches = self._semantic_course_search(query, skill_based_courses, max_results)
            
            # 3ë‹¨ê³„: ì„ í˜¸ë„ì— ë”°ë¥¸ ì†ŒìŠ¤ í•„í„°ë§
            if preferred_source:
                semantic_matches = self._filter_by_preferred_source(semantic_matches, preferred_source)
            
            # 4ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            deduplicated_courses = self._deduplicate_courses(semantic_matches)
            
            # ì§€ì •ëœ ê°œìˆ˜ê¹Œì§€ë§Œ ì œí•œ
            deduplicated_courses = deduplicated_courses[:max_results]
            
            # 5ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° í•™ìŠµ ê²½ë¡œ ìƒì„±
            course_analysis = self._analyze_course_recommendations(deduplicated_courses)
            learning_path = self._generate_learning_path(deduplicated_courses)
            
            self.logger.info(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì™„ë£Œ: ìµœì¢… {len(deduplicated_courses)}ê°œ ê³¼ì • ë°˜í™˜")
            print(f" [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì™„ë£Œ: {len(deduplicated_courses)}ê°œ ê³¼ì • ë°˜í™˜")
            
            return {
                "recommended_courses": deduplicated_courses,
                "course_analysis": course_analysis,
                "learning_path": learning_path
            }
        except Exception as e:
            self.logger.error(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f" [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "recommended_courses": [],
                "course_analysis": {"message": f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"},
                "learning_path": []
            }
    
    def _skill_based_course_filter(self, user_profile: Dict, intent_analysis: Dict) -> List[Dict]:
        """ìŠ¤í‚¬ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ - JSON ì¸ë±ìŠ¤ í™œìš©"""
        filtered_courses = []
        
        # ì‚¬ìš©ì í˜„ì¬ ìŠ¤í‚¬ ì¶”ì¶œ
        current_skills = self._extract_user_skills(user_profile)
        
        # ì˜ë„ ë¶„ì„ì—ì„œ ëª©í‘œ ìŠ¤í‚¬ ì¶”ì¶œ
        target_skills = intent_analysis.get("career_history", [])
        
        # ê²€ìƒ‰í•  ìŠ¤í‚¬ ëª©ë¡ ìƒì„±
        search_skills = list(set(current_skills + target_skills))
        
        for skill_code in search_skills:
            if skill_code in self.skill_education_mapping:
                skill_courses = self.skill_education_mapping[skill_code]
                
                # College ê³¼ì • - ì„¸ë¶„í™” ë ˆë²¨ë³„ ì¶”ê°€
                for course_type in ["specialized", "recommended", "common_required"]:
                    if course_type in skill_courses.get("college", {}):
                        courses = skill_courses["college"][course_type]
                        for course in courses:
                            course_info = course.copy()
                            course_info["source"] = "college"
                            course_info["skill_relevance"] = course_type
                            course_info["target_skill"] = skill_code
                            filtered_courses.append(course_info)
                
                # mySUNI ê³¼ì • ì¶”ê°€
                if "mysuni" in skill_courses:
                    for course in skill_courses["mysuni"]:
                        course_info = course.copy()
                        course_info["source"] = "mysuni"
                        course_info["skill_relevance"] = "general"
                        course_info["target_skill"] = skill_code
                        filtered_courses.append(course_info)
        
        self.logger.info(f"ìŠ¤í‚¬ ê¸°ë°˜ í•„í„°ë§ ê²°ê³¼: {len(filtered_courses)}ê°œ ê³¼ì •")
        return filtered_courses
    
    def _extract_user_skills(self, user_profile: Dict) -> List[str]:
        """ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ìŠ¤í‚¬ ì¶”ì¶œ"""
        skills = []
        
        # ì§ì ‘ì ì¸ ìŠ¤í‚¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if "skills" in user_profile:
            skills.extend(user_profile["skills"])
        
        # ê²½ë ¥ ì •ë³´ì—ì„œ ìŠ¤í‚¬ ì¶”ì¶œ
        if "career_history" in user_profile:
            for career in user_profile["career_history"]:
                if "skills" in career:
                    skills.extend(career["skills"])
        
        return list(set(skills))
    
    def _semantic_course_search(self, query: str, filtered_courses: List[Dict], max_results: int = 15) -> List[Dict]:
        """VectorDBë¥¼ í™œìš©í•œ ì˜ë¯¸ì  ê²€ìƒ‰ (VectorDBê°€ ì—†ìœ¼ë©´ JSONì—ì„œ ê²€ìƒ‰) - ì§€ì •ëœ ê°œìˆ˜ê¹Œì§€ ê²€ìƒ‰"""
        if not self.education_vectorstore:
            # VectorDBê°€ ì—†ìœ¼ë©´ JSON íŒŒì¼ì—ì„œ ì§ì ‘ ê²€ìƒ‰
            self.logger.info("VectorDB ì—†ìŒ - JSON íŒŒì¼ì—ì„œ ê²€ìƒ‰")
            return self._search_from_json_documents(query, filtered_courses, max_results)
            
        if not filtered_courses:
            # í•„í„°ë§ëœ ê³¼ì •ì´ ì—†ìœ¼ë©´ ì „ì²´ VectorDBì—ì„œ ê²€ìƒ‰
            docs = self.education_vectorstore.similarity_search(query, k=max_results)
            courses = [self._doc_to_course_dict(doc) for doc in docs]
            # ì›ë³¸ ë°ì´í„°ë¡œ ìƒì„¸ ì •ë³´ ë³´ê°•
            courses = [self._enrich_course_with_original_data(course) for course in courses]
        else:
            # í•„í„°ë§ëœ ê³¼ì •ë“¤ì˜ course_idë¡œ VectorDBì—ì„œ ìƒì„¸ ê²€ìƒ‰
            course_ids = [course.get("course_id") for course in filtered_courses if course.get("course_id")]
            courses = self._search_by_course_ids(course_ids, query, max_results)
            
            # í•„í„°ë§ ì •ë³´ë¥¼ VectorDB ê²°ê³¼ì— ë³‘í•©
            for course in courses:
                for filtered_course in filtered_courses:
                    if course.get("course_id") == filtered_course.get("course_id"):
                        course.update(filtered_course)
                        break
            
            # ì›ë³¸ ë°ì´í„°ë¡œ ìƒì„¸ ì •ë³´ ë³´ê°•
            courses = [self._enrich_course_with_original_data(course) for course in courses]
        
        # ê²°ê³¼ë¥¼ ì§€ì •ëœ ê°œìˆ˜ë¡œ ì œí•œ
        courses = courses[:max_results]
        self.logger.info(f"ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {len(courses)}ê°œ ê³¼ì • (ìµœëŒ€ {max_results}ê°œ)")
        return courses
    
    def _search_from_json_documents(self, query: str, filtered_courses: List[Dict], max_results: int = 15) -> List[Dict]:
        """JSON ë¬¸ì„œì—ì„œ ì§ì ‘ ê²€ìƒ‰ (VectorDB ëŒ€ì•ˆ) - ì§€ì •ëœ ê°œìˆ˜ê¹Œì§€ ê²€ìƒ‰"""
        try:
            with open(self.education_docs_path, "r", encoding="utf-8") as f:
                all_docs = json.load(f)
        except FileNotFoundError:
            self.logger.warning("êµìœ¡ê³¼ì • ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # í•„í„°ë§ëœ ê³¼ì •ì´ë¼ë„ ë°˜í™˜í•˜ì
            return filtered_courses[:max_results] if filtered_courses else []
        
        # í•„í„°ë§ëœ ê³¼ì •ì´ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
        if filtered_courses:
            # filtered_coursesì˜ course_idë“¤ê³¼ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œë“¤ ì°¾ê¸°
            filtered_course_ids = {course.get("course_id") for course in filtered_courses}
            matching_docs = []
            
            for doc in all_docs:
                metadata = doc.get("metadata", {})
                course_id = metadata.get("course_id")
                
                if course_id in filtered_course_ids:
                    course_dict = self._doc_to_course_dict_from_json(doc)
                    # í•„í„°ë§ ì •ë³´ ë³‘í•©
                    for filtered_course in filtered_courses:
                        if course_dict.get("course_id") == filtered_course.get("course_id"):
                            course_dict.update(filtered_course)
                            break
                    matching_docs.append(course_dict)
            
            if matching_docs:
                # ì§€ì •ëœ ê°œìˆ˜ë¡œ ì œí•œ
                matching_docs = matching_docs[:max_results]
                self.logger.info(f"í•„í„°ë§ëœ ê³¼ì • ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(matching_docs)}ê°œ (ìµœëŒ€ {max_results}ê°œ)")
                return matching_docs
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_keywords = query.lower().split()
        matching_docs = []
        
        for doc in all_docs:
            content = doc.get("page_content", "").lower()
            metadata = doc.get("metadata", {})
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in query_keywords:
                if keyword in content:
                    score += 1
            
            if score > 0:
                course_dict = self._doc_to_course_dict_from_json(doc)
                course_dict["match_score"] = score
                matching_docs.append(course_dict)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        matching_docs.sort(key=lambda x: x.get("match_score", 0), reverse=True)
        
        # ì§€ì •ëœ ê°œìˆ˜ë¡œ ì œí•œ
        matching_docs = matching_docs[:max_results]
        self.logger.info(f"í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(matching_docs)}ê°œ (ìµœëŒ€ {max_results}ê°œ)")
        return matching_docs
    
    def _doc_to_course_dict_from_json(self, doc_data: Dict) -> Dict:
        """JSON ë¬¸ì„œ ë°ì´í„°ë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc_data.get("metadata", {})
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc_data.get("page_content", ""),
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")  # URL í•„ë“œ ì¶”ê°€
        }
    
    def _search_by_course_ids(self, course_ids: List[str], query: str, max_results: int = 15) -> List[Dict]:
        """íŠ¹ì • ê³¼ì • IDë“¤ì— ëŒ€í•œ VectorDB ê²€ìƒ‰ - 2ê°œê¹Œì§€ë§Œ ê²€ìƒ‰"""
        if not course_ids:
            return []
        
        # ê° course_idì— ëŒ€í•´ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ í†µí•©
        all_docs = []
        for course_id in course_ids[:10]:  # ê²€ìƒ‰í•  course_idëŠ” ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            try:
                # ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰
                docs = self.education_vectorstore.similarity_search(
                    query, 
                    k=1,  # ê° ê³¼ì •ë‹¹ 1ê°œì”©ë§Œ ê°€ì ¸ì™€ì„œ ì „ì²´ì ìœ¼ë¡œ 3ê°œ ì œí•œ ìœ ì§€
                    filter={"course_id": course_id}
                )
                all_docs.extend(docs)
                # ì´ë¯¸ 2ê°œê°€ ë˜ë©´ ì¤‘ë‹¨
                if len(all_docs) >= 2:
                    break
            except Exception as e:
                self.logger.warning(f"Course ID {course_id} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì¼ë°˜ ê²€ìƒ‰ë„ ìˆ˜í–‰ (ë°±ì—…) - 2ê°œë¡œ ì œí•œ
        if not all_docs:
            all_docs = self.education_vectorstore.similarity_search(query, k=2)
        
        # ê²°ê³¼ë¥¼ 2ê°œë¡œ ì œí•œ
        all_docs = all_docs[:2]
        return [self._doc_to_course_dict(doc) for doc in all_docs]
    
    def _doc_to_course_dict(self, doc: Document) -> Dict:
        """VectorDB Documentë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc.metadata or {}
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc.page_content,
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")  # URL í•„ë“œ ì¶”ê°€
        }
    
    def _deduplicate_courses(self, courses: List[Dict]) -> List[Dict]:
        """Collegeì™€ mySUNI ê°„ ì¤‘ë³µ ê³¼ì • ì œê±° (mySUNI ë©”íƒ€ë°ì´í„° ë³´ì¡´)"""
        if not courses:
            return []
        
        deduplicated = []
        seen_courses = set()
        
        # ìš°ì„ ìˆœìœ„: College > mySUNI (Collegeê°€ ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ)
        def sort_priority(course):
            source_priority = 0 if course.get("source") == "college" else 1
            
            if course.get("source") == "college":
                relevance = course.get("skill_relevance", "")
                if relevance == "specialized":
                    relevance_priority = 0
                elif relevance == "recommended":
                    relevance_priority = 1
                else:  # common_required
                    relevance_priority = 2
            else:
                # mySUNIëŠ” í‰ì  ê¸°ì¤€ ì •ë ¬
                rating = course.get("í‰ì ", 0)
                try:
                    rating = float(rating) if rating else 0
                except:
                    rating = 0
                relevance_priority = 5 - rating  # í‰ì ì´ ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ
            
            return (source_priority, relevance_priority)
        
        sorted_courses = sorted(courses, key=sort_priority)
        
        for course in sorted_courses:
            course_signature = self._generate_course_signature(course)
            
            if course_signature not in seen_courses:
                # ì¤‘ë³µ ê³¼ì •ì´ ìˆëŠ” ê²½ìš° mySUNI ë°ì´í„°ë¥¼ College ê³¼ì •ì— í†µí•©
                if course_signature in self.course_deduplication_index:
                    duplicate_info = self.course_deduplication_index[course_signature]
                    
                    # College ê³¼ì •ì´ ìš°ì„ ì´ë¯€ë¡œ mySUNI ë°ì´í„°ë¥¼ ì¶”ê°€ ì •ë³´ë¡œ ë³‘í•©
                    if course.get("source") == "college":
                        mysuni_data = self._find_mysuni_duplicate(duplicate_info, courses)
                        if mysuni_data:
                            course["mysuni_alternative"] = {
                                "available": True,
                                "card_name": mysuni_data.get("card_name"),
                                "í‰ì ": mysuni_data.get("í‰ì "),
                                "ì´ìˆ˜ììˆ˜": mysuni_data.get("ì´ìˆ˜ììˆ˜"),
                                "ë‚œì´ë„": mysuni_data.get("ë‚œì´ë„"),
                                "ì¸ì •í•™ìŠµì‹œê°„": mysuni_data.get("ì¸ì •í•™ìŠµì‹œê°„"),
                                "ì¹´í…Œê³ ë¦¬ëª…": mysuni_data.get("ì¹´í…Œê³ ë¦¬ëª…"),
                                "ì±„ë„ëª…": mysuni_data.get("ì±„ë„ëª…")
                            }
                        else:
                            course["mysuni_alternative"] = {"available": False}
                    else:
                        # mySUNI ê³¼ì •ì¸ ê²½ìš° ì›ë³¸ ë°ì´í„° ìœ ì§€
                        course["mysuni_alternative"] = {"available": False}
                    
                    course["alternative_platforms"] = duplicate_info.get("platforms", [])
                else:
                    # ì¤‘ë³µì´ ì—†ëŠ” ê³¼ì •ì¸ ê²½ìš°
                    if course.get("source") == "mysuni":
                        course["mysuni_alternative"] = {"available": False}
                
                deduplicated.append(course)
                seen_courses.add(course_signature)
        
        self.logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(courses)}ê°œ â†’ {len(deduplicated)}ê°œ")
        return deduplicated
    
    def _generate_course_signature(self, course: Dict) -> str:
        """ê³¼ì • ì¤‘ë³µ íŒë³„ì„ ìœ„í•œ ì‹œê·¸ë‹ˆì²˜ ìƒì„±"""
        name = course.get("course_name", course.get("card_name", "")).lower().strip()
        skills = sorted(course.get("target_skills", []))
        
        # ìœ ì‚¬í•œ ê³¼ì •ëª… ì •ê·œí™”
        normalized_name = re.sub(r'[^\w\s]', '', name)
        normalized_name = re.sub(r'\s+', ' ', normalized_name)
        
        return f"{normalized_name}_{','.join(skills)}"
    
    def _find_mysuni_duplicate(self, duplicate_info: Dict, all_courses: List[Dict]) -> Dict:
        """ì¤‘ë³µ ì •ë³´ì—ì„œ mySUNI ê³¼ì • ì°¾ê¸°"""
        mysuni_course_info = None
        for course_info in duplicate_info.get("courses", []):
            if course_info.get("platform") == "mySUNI":
                course_id = course_info.get("course_id")
                # ì „ì²´ ê³¼ì • ë¦¬ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ mySUNI ê³¼ì • ì°¾ê¸°
                for course in all_courses:
                    if (course.get("source") == "mysuni" and 
                        course.get("course_id") == course_id):
                        mysuni_course_info = course
                        break
                break
        
        return mysuni_course_info
    
    def _analyze_course_recommendations(self, courses: List[Dict]) -> Dict:
        """ì¶”ì²œ ê³¼ì • ë¶„ì„ ê²°ê³¼ ìƒì„± (mySUNI ë°ì´í„° í¬í•¨)"""
        if not courses:
            return {"message": "ì¶”ì²œí•  êµìœ¡ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        college_courses = [c for c in courses if c.get("source") == "college"]
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        
        # College ê³¼ì • ì„¸ë¶„í™” ë¶„ì„
        specialized_count = len([c for c in college_courses if c.get("skill_relevance") == "specialized"])
        recommended_count = len([c for c in college_courses if c.get("skill_relevance") == "recommended"])
        required_count = len([c for c in college_courses if c.get("skill_relevance") == "common_required"])
        
        # mySUNI ëŒ€ì•ˆ ì •ë³´ ë¶„ì„
        college_with_mysuni_alt = len([c for c in college_courses 
                                      if c.get("mysuni_alternative", {}).get("available")])
        
        # mySUNI ê³¼ì • í‰ì  ë¶„ì„
        mysuni_ratings = []
        for c in mysuni_courses:
            rating = c.get("í‰ì ", 0)
            try:
                rating = float(rating) if rating else 0
                if rating > 0:
                    mysuni_ratings.append(rating)
            except:
                continue
        
        avg_mysuni_rating = sum(mysuni_ratings) / len(mysuni_ratings) if mysuni_ratings else 0
        
        # ì´ìˆ˜ì ìˆ˜ í•©ê³„
        total_enrollments = 0
        for course in mysuni_courses:
            enrollments_str = str(course.get("ì´ìˆ˜ììˆ˜", "0"))
            try:
                enrollments = int(enrollments_str.replace(",", "")) if enrollments_str.replace(",", "").isdigit() else 0
                total_enrollments += enrollments
            except:
                continue
        
        return {
            "total_courses": len(courses),
            "college_courses": len(college_courses),
            "mysuni_courses": len(mysuni_courses),
            "skill_depth_analysis": {
                "specialized": specialized_count,
                "recommended": recommended_count, 
                "common_required": required_count
            },
            "learning_platforms": {
                "college_available": len(college_courses) > 0,
                "mysuni_available": len(mysuni_courses) > 0,
                "college_with_mysuni_alternatives": college_with_mysuni_alt
            },
            "mysuni_quality_metrics": {
                "average_rating": round(avg_mysuni_rating, 1),
                "total_enrollments": total_enrollments,
                "high_rated_courses": len([r for r in mysuni_ratings if r >= 4.5])
            }
        }
    
    def _generate_learning_path(self, courses: List[Dict]) -> List[Dict]:
        """í•™ìŠµ ê²½ë¡œ ì œì•ˆ ìƒì„±"""
        if not courses:
            return []
        
        path = []
        
        # 1ë‹¨ê³„: ê³µí†µ í•„ìˆ˜ ê³¼ì •
        required_courses = [c for c in courses if c.get("skill_relevance") == "common_required"]
        if required_courses:
            path.append({
                "step": 1,
                "level": "ê¸°ì´ˆ/í•„ìˆ˜",
                "courses": required_courses[:2],  # ìµœëŒ€ 2ê°œ
                "description": "ê¸°ë³¸ ì§€ì‹ ìŠµë“ì„ ìœ„í•œ í•„ìˆ˜ ê³¼ì •"
            })
        
        # 2ë‹¨ê³„: ì¶”ì²œ ê³¼ì •
        recommended_courses = [c for c in courses if c.get("skill_relevance") == "recommended"]
        if recommended_courses:
            path.append({
                "step": 2,
                "level": "í™•ì¥/ì‘ìš©",
                "courses": recommended_courses[:3],  # ìµœëŒ€ 3ê°œ
                "description": "ê´€ë ¨ ê¸°ìˆ  í™•ì¥ì„ ìœ„í•œ ì¶”ì²œ ê³¼ì •"
            })
        
        # 3ë‹¨ê³„: ì „ë¬¸í™” ê³¼ì •
        specialized_courses = [c for c in courses if c.get("skill_relevance") == "specialized"]
        if specialized_courses:
            path.append({
                "step": 3,
                "level": "ì „ë¬¸/ì‹¬í™”",
                "courses": specialized_courses[:2],  # ìµœëŒ€ 2ê°œ
                "description": "ì „ë¬¸ì„± ê°•í™”ë¥¼ ìœ„í•œ íŠ¹í™” ê³¼ì •"
            })
        
        # mySUNI ê³¼ì •ì€ ë³´ì™„/ëŒ€ì•ˆìœ¼ë¡œ ì œì‹œ
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        if mysuni_courses:
            path.append({
                "step": "ë³´ì™„",
                "level": "ì˜¨ë¼ì¸/ììœ¨",
                "courses": mysuni_courses[:3],  # ìµœëŒ€ 3ê°œ
                "description": "ì˜¨ë¼ì¸ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ë³´ì™„ ê³¼ì •"
            })
        
        return path

    def _load_original_course_data(self):
        """ì›ë³¸ êµìœ¡ê³¼ì • ìƒì„¸ ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)"""
        if not hasattr(self, 'original_mysuni_data'):
            try:
                mysuni_path = PathConfig.MYSUNI_DETAILED
                with open(mysuni_path, "r", encoding="utf-8") as f:
                    self.original_mysuni_data = json.load(f)
                self.logger.info(f"mySUNI ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_mysuni_data)}ê°œ - ê²½ë¡œ: {mysuni_path}")
            except FileNotFoundError:
                self.logger.warning(f"mySUNI ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. - ê²½ë¡œ: {PathConfig.MYSUNI_DETAILED}")
                self.original_mysuni_data = []
                
        if not hasattr(self, 'original_college_data'):
            try:
                college_path = PathConfig.COLLEGE_DETAILED
                with open(college_path, "r", encoding="utf-8") as f:
                    self.original_college_data = json.load(f)
                self.logger.info(f"College ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_college_data)}ê°œ - ê²½ë¡œ: {college_path}")
            except FileNotFoundError:
                self.logger.warning(f"College ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. - ê²½ë¡œ: {PathConfig.COLLEGE_DETAILED}")
                self.original_college_data = []

    def _enrich_course_with_original_data(self, course: Dict) -> Dict:
        """VectorDB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ë³´ê°•"""
        self._load_original_course_data()
        
        course_id = course.get("course_id")
        source = course.get("source")
        
        if not course_id:
            return course
            
        # mySUNI ê³¼ì •ì¸ ê²½ìš°
        if source == "mysuni":
            for original in self.original_mysuni_data:
                if original.get("course_id") == course_id:
                    # ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    course.update({
                        "ì¹´í…Œê³ ë¦¬ëª…": original.get("ì¹´í…Œê³ ë¦¬ëª…"),
                        "ì±„ë„ëª…": original.get("ì±„ë„ëª…"),
                        "íƒœê·¸ëª…": original.get("íƒœê·¸ëª…"),
                        "ë‚œì´ë„": original.get("ë‚œì´ë„"),
                        "í‰ì ": original.get("í‰ì "),
                        "ì´ìˆ˜ììˆ˜": original.get("ì´ìˆ˜ììˆ˜"),
                        "ì§ë¬´": original.get("ì§ë¬´", []),
                        "skillset": original.get("skillset", []),
                        "url": original.get("url")
                    })
                    break
                    
        # College ê³¼ì •ì¸ ê²½ìš°
        elif source == "college":
            for original in self.original_college_data:
                if original.get("course_id") == course_id:
                    # ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    course.update({
                        "í•™ë¶€": original.get("í•™ë¶€"),
                        "í‘œì¤€ê³¼ì •": original.get("í‘œì¤€ê³¼ì •"),
                        "ì‚¬ì—…ë³„êµìœ¡ì²´ê³„": original.get("ì‚¬ì—…ë³„êµìœ¡ì²´ê³„"),
                        "êµìœ¡ìœ í˜•": original.get("êµìœ¡ìœ í˜•"),
                        "í•™ìŠµìœ í˜•": original.get("í•™ìŠµìœ í˜•"),
                        "ê³µê°œì—¬ë¶€": original.get("ê³µê°œì—¬ë¶€"),
                        "íŠ¹í™”ì§ë¬´": original.get("íŠ¹í™”ì§ë¬´", []),
                        "ì¶”ì²œì§ë¬´": original.get("ì¶”ì²œì§ë¬´", []),
                        "ê³µí†µí•„ìˆ˜ì§ë¬´": original.get("ê³µí†µí•„ìˆ˜ì§ë¬´", []),
                        "url": original.get("url")
                    })
                    break
        
        return course
    
    def _get_preferred_education_source(self, query: str, user_profile: Dict, intent_analysis: Dict) -> str:
        """ì‚¬ìš©ìì˜ êµìœ¡ê³¼ì • ì†ŒìŠ¤ ì„ í˜¸ë„ ê°ì§€"""
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ëª…ì‹œì  ì–¸ê¸‰ í™•ì¸
        query_lower = query.lower()
        if 'mysuni' in query_lower or 'my suni' in query_lower:
            return 'mysuni'
        elif 'college' in query_lower or 'ì»¬ë¦¬ì§€' in query_lower:
            return 'college'
        
        # 2. ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ì„ í˜¸ë„ í™•ì¸
        preferred_source = user_profile.get('preferred_education_source', '')
        if preferred_source in ['mysuni', 'college']:
            return preferred_source
        
        # 3. ì˜ë„ ë¶„ì„ì—ì„œ ì„ í˜¸ë„ í™•ì¸
        intent_preferred = intent_analysis.get('preferred_source', '')
        if intent_preferred in ['mysuni', 'college']:
            return intent_preferred
        
        # ê¸°ë³¸ê°’: ì„ í˜¸ë„ ì—†ìŒ
        return ''
    
    def _filter_by_preferred_source(self, courses: List[Dict], preferred_source: str) -> List[Dict]:
        """ì„ í˜¸í•˜ëŠ” êµìœ¡ê³¼ì • ì†ŒìŠ¤ë¡œ í•„í„°ë§"""
        if not preferred_source or not courses:
            return courses
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ë“¤ ë¨¼ì € ì¶”ì¶œ
        preferred_courses = [course for course in courses if course.get('source') == preferred_source]
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ì´ ì¶©ë¶„íˆ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ë°˜í™˜ (ìµœì†Œ 2ê°œ)
        if len(preferred_courses) >= 2:
            self.logger.info(f"{preferred_source} ê³¼ì • {len(preferred_courses)}ê°œë¡œ í•„í„°ë§")
            return preferred_courses[:2]  # 2ê°œë¡œ ì œí•œ
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ì†ŒìŠ¤ë„ í¬í•¨í•˜ë˜ ì„ í˜¸ ì†ŒìŠ¤ ìš°ì„  ì •ë ¬
        other_courses = [course for course in courses if course.get('source') != preferred_source]
        result = preferred_courses + other_courses[:2-len(preferred_courses)]  # ìµœëŒ€ 2ê°œê¹Œì§€
        
        self.logger.info(f"{preferred_source} ìš°ì„  í•„í„°ë§: {len(preferred_courses)}ê°œ + ê¸°íƒ€ {len(result)-len(preferred_courses)}ê°œ")
        return result[:2]  # ìµœì¢…ì ìœ¼ë¡œ 2ê°œ ì œí•œ

    def get_company_vision_context(self) -> str:
        """íšŒì‚¬ ë¹„ì „ ì •ë³´ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        try:
            import os
            import json
            
            # íšŒì‚¬ ë¹„ì „ íŒŒì¼ ê²½ë¡œ
            vision_path = os.path.abspath(os.path.join(
                os.path.dirname(__file__), 
                "../../storage/docs/company_vision.json"
            ))
            
            if not os.path.exists(vision_path):
                return ""
            
            with open(vision_path, "r", encoding="utf-8") as f:
                vision_data = json.load(f)
            
            if not vision_data:
                return ""
            
            sections = []
            sections.append(" **íšŒì‚¬ ë¹„ì „ ë° ê°€ì¹˜ (ì»¤ë¦¬ì–´ ê°€ì´ë“œì— ë°˜ì˜)**:")
            sections.append("")
            
            # íšŒì‚¬ ê¸°ë³¸ ì •ë³´
            if vision_data.get('company_name'):
                sections.append(f"**íšŒì‚¬ëª…**: {vision_data['company_name']}")
            
            # ë¹„ì „
            if vision_data.get('vision'):
                vision = vision_data['vision']
                sections.append(f"**ë¹„ì „**: {vision.get('title', '')}")
                if vision.get('description'):
                    sections.append(f"*{vision['description']}*")
            
            sections.append("")
            
            # í•µì‹¬ ê°€ì¹˜
            if vision_data.get('core_values'):
                sections.append("**í•µì‹¬ ê°€ì¹˜**:")
                for value in vision_data['core_values']:
                    sections.append(f"- **{value.get('name', '')}**: {value.get('description', '')}")
                sections.append("")
            
            # ì „ëµ ë°©í–¥
            if vision_data.get('strategic_directions'):
                sections.append("**ì „ëµ ë°©í–¥**:")
                for direction in vision_data['strategic_directions']:
                    sections.append(f"- **{direction.get('category', '')}**: {direction.get('description', '')}")
                sections.append("")
            
            # ì¸ì¬ ê°œë°œ
            if vision_data.get('talent_development'):
                talent = vision_data['talent_development']
                sections.append(f"**ì¸ì¬ ê°œë°œ ì² í•™**: {talent.get('philosophy', '')}")
                if talent.get('focus_areas'):
                    sections.append("**ì—­ëŸ‰ ê°œë°œ ì¤‘ì  ì˜ì—­**:")
                    for area in talent['focus_areas']:
                        sections.append(f"- **{area.get('area', '')}**: {area.get('description', '')}")
                sections.append("")
            
            # ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™
            if vision_data.get('career_guidance_principles'):
                sections.append("**ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™**:")
                for principle in vision_data['career_guidance_principles']:
                    sections.append(f"- **{principle.get('principle', '')}**: {principle.get('description', '')}")
                sections.append("")
            
            # ì ìš© ê°€ì´ë“œë¼ì¸
            sections.append("** ì¤‘ìš”: íšŒì‚¬ ë¹„ì „ í™œìš© ì§€ì¹¨**")
            sections.append("- ì»¤ë¦¬ì–´ ìƒë‹´ ì‹œ ê°œì¸ì˜ ëª©í‘œì™€ AI Powered ITS ë¹„ì „ì„ ì—°ê²°í•˜ì—¬ ì¡°ì–¸")
            sections.append("- í•µì‹¬ ê°€ì¹˜(ì‚¬ëŒ ì¤‘ì‹¬, Digital í˜ì‹ , Identity ììœ¨í™”, Business í˜ì‹ , ìµœê³ ì˜ Delivery)ì™€ ì¼ì¹˜í•˜ëŠ” ë°©í–¥ ì œì‹œ")
            sections.append("- Multi-Skill Setì„ í†µí•œ ê¸€ë¡œë²Œ ìˆ˜ì¤€ì˜ ì „ë¬¸ê°€ ìœ¡ì„± ê°•ì¡°")
            sections.append("- IT â†’ Digital â†’ AIë¡œì˜ ê¸°ìˆ  ì§„í™”ì— ëŠ¥ë™ì  ì ì‘ê³¼ ìê¸°ì£¼ë„ì  ì„±ì¥ ê°•ì¡°")
            sections.append("- Process í˜ì‹ ê³¼ ì—…ë¬´ ìë™í™”/ì§€ëŠ¥í™”ë¥¼ ë°˜ì˜í•œ ì»¤ë¦¬ì–´ ë°©í–¥ ì œì•ˆ")
            sections.append("- Offshoring ëŒ€ì‘ì„ ìœ„í•œ ê¸€ë¡œë²Œ ê²½ìŸë ¥ í™•ë³´ ë°©ì•ˆ ì œì‹œ")
            
            return "\n".join(sections)
            
        except Exception as e:
            self.logger.error(f"íšŒì‚¬ ë¹„ì „ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""


class NewsRetrieverAgent:
    """
    ë‰´ìŠ¤ ê²€ìƒ‰ ì—ì´ì „íŠ¸
    
    AI, ê¸ˆìœµ, ë°˜ë„ì²´, ì œì¡° ë„ë©”ì¸ë³„ ìµœì‹  ë‰´ìŠ¤ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬
    ì—…ê³„ íŠ¸ë Œë“œì™€ ì±„ìš© ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ë„ë©”ì¸ë³„ ë‰´ìŠ¤ ë¶„ë¥˜ ë° ê²€ìƒ‰
    - ì˜ë„ ë¶„ì„ ê¸°ë°˜ ë§ì¶¤í˜• ë‰´ìŠ¤ ì¶”ì²œ
    - ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
    - ìµœì‹  ì—…ê³„ íŠ¸ë Œë“œ ë° ì±„ìš© ì •ë³´ ì œê³µ
    - ëŸ°íƒ€ì„ì—ì„œ ì§ì ‘ ChromaDB ì ‘ê·¼ (NewsDataProcessor ë¹„ì˜ì¡´)
    
    ê²€ìƒ‰ ëŒ€ìƒ:
    - AI ë„ë©”ì¸: AI ê°œë°œì ì±„ìš©, ìƒì„±í˜• AI, ì˜ë£Œ AI ë“±
    - ê¸ˆìœµ ë„ë©”ì¸: í•€í…Œí¬, ë¸”ë¡ì²´ì¸, ë””ì§€í„¸ ê¸ˆìœµ ë“±
    - ë°˜ë„ì²´ ë„ë©”ì¸: ë°˜ë„ì²´ ì„¤ê³„, ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ë“±
    - ì œì¡° ë„ë©”ì¸: ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬, IoT, ë°°í„°ë¦¬ ê´€ë¦¬ ë“±
    """
    
    def __init__(self):
        """
        NewsRetrieverAgent ì´ˆê¸°í™”
        - ëŸ°íƒ€ì„ì—ì„œ ì§ì ‘ ChromaDBì— ì ‘ê·¼
        - NewsDataProcessorì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
        """
        self.logger = logging.getLogger(__name__)
        
        # ë‰´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ ì„¤ì •
        self.news_vector_store_path = PathConfig.get_abs_path(PathConfig.NEWS_VECTOR_STORE)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self.chroma_client = None
        self.news_collection = None
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        self.domain_keywords = {
            "AI": ["AI", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìƒì„±í˜•", "ChatGPT", "LLM", "ìì—°ì–´ì²˜ë¦¬", "NLP", "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸"],
            "ê¸ˆìœµ": ["í•€í…Œí¬", "ë¸”ë¡ì²´ì¸", "ë””ì§€í„¸ê¸ˆìœµ", "DeFi", "ìŠ¤ë§ˆíŠ¸ì»¨íŠ¸ë™íŠ¸", "ì•”í˜¸í™”í", "í† ìŠ¤", "ì¹´ì¹´ì˜¤í˜ì´"],
            "ë°˜ë„ì²´": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "DRAM", "NAND", "ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "ì„¤ê³„", "ì—”ì§€ë‹ˆì–´", "ì¹©"],
            "ì œì¡°": ["ì œì¡°", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "IoT", "ìë™ì°¨", "ë°°í„°ë¦¬", "ì „ê¸°ì°¨", "BMS", "í˜„ëŒ€ìë™ì°¨", "LG"]
        }
    
    def _initialize_vectorstore(self) -> bool:
        """
        ë‰´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        NewsDataProcessorì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ChromaDB í´ë¼ì´ì–¸íŠ¸ì— ì§ì ‘ ì ‘ê·¼í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        if self.chroma_client is None or self.news_collection is None:
            try:
                import chromadb
                from chromadb.config import Settings
                
                # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì´ˆê¸°í™” (NewsDataProcessorì™€ ë™ì¼í•œ ë°©ì‹)
                self.chroma_client = chromadb.PersistentClient(
                    path=self.news_vector_store_path,
                    settings=Settings(
                        allow_reset=True,
                        anonymized_telemetry=False
                    )
                )
                
                # ë‰´ìŠ¤ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
                self.news_collection = self.chroma_client.get_collection("news_articles")
                
                self.logger.info(f"ë‰´ìŠ¤ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ: {self.news_vector_store_path}")
                return True
                
            except Exception as e:
                self.logger.error(f"ë‰´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        return True
    
    def search_relevant_news(self, query: str, intent_analysis: dict = None, n_results: int = 2) -> list:
        """
        ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ChromaDB í´ë¼ì´ì–¸íŠ¸ì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            intent_analysis: ì˜ë„ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 2)
            
        Returns:
            list: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "title": "ë‰´ìŠ¤ ì œëª©",
                        "domain": "ë„ë©”ì¸ (AI/ê¸ˆìœµ/ë°˜ë„ì²´/ì œì¡°)",
                        "category": "ì¹´í…Œê³ ë¦¬",
                        "content": "ë‰´ìŠ¤ ë‚´ìš© (300ì ì œí•œ)",
                        "published_date": "ë°œí–‰ì¼",
                        "source": "ì¶œì²˜",
                        "similarity_score": "ìœ ì‚¬ë„ ì ìˆ˜"
                    }
                ]
        """
        try:
            # ë‰´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            if not self._initialize_vectorstore():
                return []
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            search_query = self._optimize_search_query(query, intent_analysis)
            
            #  ChromaDB ì»¬ë ‰ì…˜ì—ì„œ ì§ì ‘ ê²€ìƒ‰ ìˆ˜í–‰
            results = self.news_collection.query(
                query_texts=[search_query],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ê°€ê³µ
            processed_news = self._process_chromadb_results(results)
            
            self.logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(processed_news)}ê°œ (ì¿¼ë¦¬: {search_query[:50]}...)")
            return processed_news
            
        except Exception as e:
            self.logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _optimize_search_query(self, query: str, intent_analysis: dict = None) -> str:
        """
        ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        
        Args:
            query: ì›ë³¸ ì§ˆì˜
            intent_analysis: ì˜ë„ ë¶„ì„ ê²°ê³¼
            
        Returns:
            str: ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬
        """
        search_query = query
        
        if intent_analysis:
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¶”ê°€
            keywords = []
            
            # ì»¤ë¦¬ì–´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
            if intent_analysis.get("career_history"):
                keywords.extend(intent_analysis["career_history"][:2])
            
            # ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì¶”ê°€
            if intent_analysis.get("interests"):
                keywords.extend(intent_analysis["interests"][:2])
            
            # ë„ë©”ì¸ ê´€ë ¨ í‚¤ì›Œë“œ ê°•í™”
            detected_domain = self._detect_domain_from_query(query)
            if detected_domain and detected_domain in self.domain_keywords:
                domain_keywords = self.domain_keywords[detected_domain][:2]
                keywords.extend(domain_keywords)
            
            # ìµœì¢… ì¿¼ë¦¬ êµ¬ì„±
            if keywords:
                search_query = f"{query} {' '.join(keywords)}"
        
        return search_query
    
    def _detect_domain_from_query(self, query: str) -> str:
        """
        ì¿¼ë¦¬ì—ì„œ ë„ë©”ì¸ì„ ê°ì§€í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            
        Returns:
            str: ê°ì§€ëœ ë„ë©”ì¸ (AI/ê¸ˆìœµ/ë°˜ë„ì²´/ì œì¡°) ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
        """
        query_lower = query.lower()
        
        for domain, keywords in self.domain_keywords.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    return domain
        
        return ""
    
    def _process_chromadb_results(self, results: dict) -> list:
        """
        ChromaDB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ê³µí•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            results: ChromaDB query ê²°ê³¼
            
        Returns:
            list: ê°€ê³µëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        processed_news = []
        
        if results['documents'] and results['documents'][0]:
            for i in range(len(results['documents'][0])):
                try:
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    
                    # ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
                    similarity_score = max(0, 1 - distance) if distance <= 1 else 0
                    
                    # ë‰´ìŠ¤ ì •ë³´ ì¬êµ¬ì„±
                    news_info = {
                        "title": metadata.get('title', ''),
                        "domain": metadata.get('domain', ''),
                        "category": metadata.get('category', ''),
                        "content": self._extract_content_from_document(results['documents'][0][i]),
                        "published_date": metadata.get('published_date', ''),
                        "source": metadata.get('source', ''),
                        "similarity_score": round(similarity_score, 3)
                    }
                    
                    # ê¸°ë³¸ í’ˆì§ˆ í•„í„°ë§ (ì œëª©ì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ)
                    if news_info["title"]:
                        processed_news.append(news_info)
                        
                except Exception as e:
                    self.logger.warning(f"ë‰´ìŠ¤ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        return processed_news
    
    def _extract_content_from_document(self, document: str) -> str:
        """
        ì„ë² ë”©ëœ ë¬¸ì„œì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            document: ì„ë² ë”©ëœ ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸
            
        Returns:
            str: ì¶”ì¶œëœ ë‰´ìŠ¤ ë‚´ìš© (300ì ì œí•œ)
        """
        # "ë‚´ìš©:" ì´í›„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if "ë‚´ìš©:" in document:
            content = document.split("ë‚´ìš©:")[-1].strip()
        else:
            content = document
        
        # ê¸¸ì´ ì œí•œ (300ì)
        if len(content) > 300:
            content = content[:300] + "..."
        
        return content
    
    def get_news_by_domain(self, domain: str, n_results: int = 2) -> list:
        """
        íŠ¹ì • ë„ë©”ì¸ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ChromaDB í´ë¼ì´ì–¸íŠ¸ì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ë„ë©”ì¸ í•„í„°ë§ëœ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            domain: ë„ë©”ì¸ (AI/ê¸ˆìœµ/ë°˜ë„ì²´/ì œì¡°)
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            list: í•´ë‹¹ ë„ë©”ì¸ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if domain not in self.domain_keywords:
            self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸: {domain}")
            return []
        
        try:
            # ë‰´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            if not self._initialize_vectorstore():
                return []
            
            # ë„ë©”ì¸ë³„ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            domain_query = " ".join(self.domain_keywords[domain][:3])
            
            # ChromaDBì—ì„œ ë„ë©”ì¸ í•„í„°ë§ ê²€ìƒ‰
            results = self.news_collection.query(
                query_texts=[domain_query],
                n_results=n_results * 2,  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
                where={"domain": domain},  # ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° í•„í„°ë§
                include=['documents', 'metadatas', 'distances']
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ê°€ê³µ
            processed_news = self._process_chromadb_results(results)
            
            # ê²°ê³¼ ìˆ˜ ì œí•œ
            return processed_news[:n_results]
            
        except Exception as e:
            self.logger.error(f"ë„ë©”ì¸ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            domain_query = " ".join(self.domain_keywords[domain][:3])
            return self.search_relevant_news(domain_query, n_results=n_results)
    
    def get_latest_industry_trends(self, user_profile: dict = None) -> dict:
        """
        ì‚¬ìš©ì í”„ë¡œí•„ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  ì—…ê³„ íŠ¸ë Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        
        Args:
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´
            
        Returns:
            dict: ë„ë©”ì¸ë³„ ìµœì‹  íŠ¸ë Œë“œ ë‰´ìŠ¤
        """
        trends = {}
        
        # ì‚¬ìš©ì ê´€ì‹¬ ë„ë©”ì¸ íŒŒì•…
        interested_domains = self._extract_interested_domains(user_profile)
        
        # ê° ë„ë©”ì¸ë³„ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
        for domain in interested_domains:
            domain_news = self.get_news_by_domain(domain, n_results=2)
            if domain_news:
                trends[domain] = domain_news
        
        return trends
    
    def _extract_interested_domains(self, user_profile: dict = None) -> list:
        """
        ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ê´€ì‹¬ ë„ë©”ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´
            
        Returns:
            list: ê´€ì‹¬ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        """
        if not user_profile:
            return ["AI", "ê¸ˆìœµ", "ë°˜ë„ì²´", "ì œì¡°"]  # ê¸°ë³¸ ëª¨ë“  ë„ë©”ì¸
        
        interested_domains = []
        
        # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ë‚˜ ê²½ë ¥ì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        interests = user_profile.get("interests", [])
        career = user_profile.get("career", "")
        
        combined_text = " ".join(interests) + " " + career
        
        for domain in self.domain_keywords.keys():
            domain_keywords = self.domain_keywords[domain]
            if any(keyword.lower() in combined_text.lower() for keyword in domain_keywords):
                interested_domains.append(domain)
        
        # ê´€ì‹¬ ë„ë©”ì¸ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë„ë©”ì¸ ë°˜í™˜
        return interested_domains if interested_domains else ["AI", "ê¸ˆìœµ", "ë°˜ë„ì²´", "ì œì¡°"]