# app/graphs/agents/retriever.py

import os
import json
import re
import requests
import logging
from typing import Dict, List, Any, Optional
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from datetime import datetime, timedelta

from dotenv import load_dotenv
load_dotenv()

# ==================== ğŸ“‚ ê²½ë¡œ ì„¤ì • (ìˆ˜ì • í•„ìš”ì‹œ ì—¬ê¸°ë§Œ ë³€ê²½) ====================
class PathConfig:
    """
    ëª¨ë“  ê²½ë¡œ ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    ê²½ë¡œ ë³€ê²½ì´ í•„ìš”í•  ë•ŒëŠ” ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    """
    BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # app ë””ë ‰í† ë¦¬
    
    # ğŸ“„ ë¬¸ì„œ ê²½ë¡œ (JSON ë°ì´í„° íŒŒì¼ë“¤) - í´ë°±ìš©
    CAREER_DOCS = "../../storage/docs/career_history.json"                    # ì»¤ë¦¬ì–´ íˆìŠ¤í† ë¦¬ ì›ë³¸ ë°ì´í„°
    EDUCATION_DOCS = "../../storage/docs/education_courses.json"              # êµìœ¡ê³¼ì • ë¬¸ì„œ ë°ì´í„°
    SKILL_MAPPING = "../../storage/docs/skill_education_mapping.json"         # ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ í…Œì´ë¸”
    COURSE_DEDUPLICATION = "../../storage/docs/course_deduplication_index.json"  # ê³¼ì • ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤
    COMPANY_VISION = "../../storage/docs/company_vision.json"                 # íšŒì‚¬ ë¹„ì „ ë° ê°€ì¹˜ ë°ì´í„°
    MYSUNI_DETAILED = "../../storage/docs/mysuni_courses_detailed.json"       # mySUNI ê³¼ì • ìƒì„¸ ì •ë³´
    COLLEGE_DETAILED = "../../storage/docs/college_courses_detailed.json"     # College ê³¼ì • ìƒì„¸ ì •ë³´
    
    @classmethod
    def get_abs_path(cls, relative_path: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        return os.path.abspath(os.path.join(os.path.dirname(__file__), relative_path))

# ==================== ğŸ“‚ ê²½ë¡œ ì„¤ì • ë ====================

class ChromaK8sClient:
    """K8s ë‚´ë¶€ ChromaDB í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, use_external: bool = False):
        """
        ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            use_external: Trueë©´ ì™¸ë¶€ ì ‘ì†, Falseë©´ k8s ë‚´ë¶€ ì ‘ì†
        """
        self.use_external = use_external
        
        if use_external:
            # ì™¸ë¶€ ì ‘ì† (ê°œë°œí™˜ê²½)
            self.base_url = "https://sk-gnavi4.skala25a.project.skala-ai.com/vector/api/v2"
        else:
            # k8s ë‚´ë¶€ ì ‘ì† (ìš´ì˜í™˜ê²½)
            self.base_url = "http://chromadb-1.sk-team-04.svc.cluster.local:8000/api/v2"
        
        self.tenant = "default_tenant"
        self.database = "default_database"
        self.collections_url = f"{self.base_url}/tenants/{self.tenant}/databases/{self.database}/collections"
        
        # ì»¬ë ‰ì…˜ ì •ë³´
        self.career_collection_name = "gnavi4_career_history_prod"
        self.education_collection_name = "gnavi4_education_prod"
        self.career_collection_id = None
        self.education_collection_id = None
        
        # ì„ë² ë”© ì„¤ì •
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            dimensions=1536
        )
        
        # í—¤ë” ì„¤ì •
        self.headers = {"Content-Type": "application/json"}
        self.logger = logging.getLogger(__name__)
        
        # ì´ˆê¸°í™”
        self._init_collections()
    
    def _init_collections(self):
        """ì»¬ë ‰ì…˜ ID ì´ˆê¸°í™”"""
        try:
            collections = self._get_collections_list()
            if collections:
                for collection in collections:
                    name = collection.get('name')
                    collection_id = collection.get('id')
                    
                    if name == self.career_collection_name:
                        self.career_collection_id = collection_id
                        self.logger.info(f"ê²½ë ¥ ì»¬ë ‰ì…˜ ID ë¡œë“œ: {collection_id}")
                    elif name == self.education_collection_name:
                        self.education_collection_id = collection_id
                        self.logger.info(f"êµìœ¡ê³¼ì • ì»¬ë ‰ì…˜ ID ë¡œë“œ: {collection_id}")
            
            connection_mode = "ì™¸ë¶€" if self.use_external else "k8s ë‚´ë¶€"
            print(f"âœ… [ChromaDB {connection_mode} ì ‘ì†] ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"âŒ [ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨] {e}")
    
    def _get_collections_list(self) -> Optional[List[Dict]]:
        """ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(
                self.collections_url,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def search_career_documents(self, query: str, k: int = 3) -> List[Document]:
        """ê²½ë ¥ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.career_collection_id:
            self.logger.warning("ê²½ë ¥ ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)
            
            # ê²€ìƒ‰ ìš”ì²­
            search_data = {
                "query_embeddings": [query_embedding],
                "n_results": k,
                "include": ["documents", "metadatas"]
            }
            
            search_url = f"{self.collections_url}/{self.career_collection_id}/query"
            response = requests.post(search_url, headers=self.headers, json=search_data, timeout=30)
            
            if response.status_code == 200:
                results = response.json()
                documents = results.get('documents', [[]])
                metadatas = results.get('metadatas', [[]])
                
                # Document ê°ì²´ë¡œ ë³€í™˜
                docs = []
                for i, doc_content in enumerate(documents[0] if documents else []):
                    metadata = metadatas[0][i] if metadatas and metadatas[0] and i < len(metadatas[0]) else {}
                    docs.append(Document(page_content=doc_content, metadata=metadata))
                
                return docs
            else:
                self.logger.error(f"ê²½ë ¥ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            self.logger.error(f"ê²½ë ¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def search_education_documents(self, query: str, k: int = 3) -> List[Document]:
        """êµìœ¡ê³¼ì • ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.education_collection_id:
            self.logger.warning("êµìœ¡ê³¼ì • ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)
            
            # ê²€ìƒ‰ ìš”ì²­
            search_data = {
                "query_embeddings": [query_embedding],
                "n_results": k,
                "include": ["documents", "metadatas"]
            }
            
            search_url = f"{self.collections_url}/{self.education_collection_id}/query"
            response = requests.post(search_url, headers=self.headers, json=search_data, timeout=30)
            
            if response.status_code == 200:
                results = response.json()
                documents = results.get('documents', [[]])
                metadatas = results.get('metadatas', [[]])
                
                # Document ê°ì²´ë¡œ ë³€í™˜
                docs = []
                for i, doc_content in enumerate(documents[0] if documents else []):
                    metadata = metadatas[0][i] if metadatas and metadatas[0] and i < len(metadatas[0]) else {}
                    docs.append(Document(page_content=doc_content, metadata=metadata))
                
                return docs
            else:
                self.logger.error(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            self.logger.error(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_collection_count(self, collection_type: str = "career") -> Optional[int]:
        """ì»¬ë ‰ì…˜ ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ"""
        if collection_type == "career":
            collection_id = self.career_collection_id
        elif collection_type == "education":
            collection_id = self.education_collection_id
        else:
            return None
        
        if not collection_id:
            return None
        
        try:
            count_url = f"{self.collections_url}/{collection_id}/count"
            response = requests.get(count_url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"{collection_type} ì»¬ë ‰ì…˜ ì¹´ìš´íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"{collection_type} ì»¬ë ‰ì…˜ ì¹´ìš´íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

class CareerEnsembleRetrieverAgent:
    """
    ğŸ” ì»¤ë¦¬ì–´ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì—ì´ì „íŠ¸ (K8s ChromaDB ì—°ë™)
    
    K8s ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ChromaDBì— ì—°ê²°í•˜ì—¬ ì»¤ë¦¬ì–´ ì‚¬ë¡€ì™€ êµìœ¡ê³¼ì •ì„
    íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    ğŸ“Š ê²€ìƒ‰ ê²°ê³¼:
    - ì»¤ë¦¬ì–´ ì‚¬ë¡€: ìµœëŒ€ 3ê°œê¹Œì§€ ê²€ìƒ‰
    - êµìœ¡ê³¼ì •: ìµœëŒ€ 3ê°œê¹Œì§€ ê²€ìƒ‰
    """
    def __init__(self, use_external_chroma: bool = None):
        """
        CareerEnsembleRetrieverAgent ì´ˆê¸°í™”
        
        Args:
            use_external_chroma: Trueë©´ ì™¸ë¶€ ì ‘ì†, Falseë©´ k8s ë‚´ë¶€ ì ‘ì†, Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ í™•ì¸
        """
        # í™˜ê²½ë³€ìˆ˜ë¡œ ì ‘ì† ë°©ì‹ ê²°ì •
        if use_external_chroma is None:
            use_external_chroma = os.getenv("CHROMA_USE_EXTERNAL", "false").lower() == "true"
        
        self.logger = logging.getLogger(__name__)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.chroma_client = ChromaK8sClient(use_external=use_external_chroma)
        
        # ë¬¸ì„œ ê²½ë¡œ ì„¤ì • (í´ë°±ìš©)
        self.education_docs_path = PathConfig.get_abs_path(PathConfig.EDUCATION_DOCS)
        self.skill_mapping_path = PathConfig.get_abs_path(PathConfig.SKILL_MAPPING)
        self.deduplication_index_path = PathConfig.get_abs_path(PathConfig.COURSE_DEDUPLICATION)
        self.company_vision_path = PathConfig.get_abs_path(PathConfig.COMPANY_VISION)
        
        # ì§€ì—° ë¡œë”© ì†ì„±
        self.skill_education_mapping = None
        self.course_deduplication_index = None
        self.company_vision_data = None
        self.original_mysuni_data = None
        self.original_college_data = None
        
        connection_mode = "ì™¸ë¶€" if use_external_chroma else "k8s ë‚´ë¶€"
        print(f"âœ… [Retriever Agent {connection_mode} ëª¨ë“œ] ì´ˆê¸°í™” ì™„ë£Œ")

    def retrieve(self, query: str, k: int = 3):
        """ChromaDBì—ì„œ ê²½ë ¥ ì‚¬ë¡€ ê²€ìƒ‰ + ì‹œê°„ ê¸°ë°˜ í•„í„°ë§"""
        print(f"ğŸ” [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        
        # ChromaDBì—ì„œ ê²€ìƒ‰
        docs = self.chroma_client.search_career_documents(query, k=k*2)  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
        
        if not docs:
            print(f"âŒ [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ê²°ê³¼ ì—†ìŒ")
            return self._fallback_career_search(query, k)
        
        # ìµœê·¼ í‚¤ì›Œë“œ ê°ì§€ ë° ì—°ë„ ì¶”ì¶œ
        recent_keywords = ['ìµœê·¼', 'ìµœì‹ ', 'recent', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ìƒˆë¡œìš´', 'ì‹ ê·œ', 'íŠ¸ë Œë“œ']
        is_recent_query = any(keyword in query.lower() for keyword in recent_keywords)
        
        # ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ
        years_info = self._extract_years_from_query(query)
        
        # "ì‹ ì…" ë˜ëŠ” "ì…ì‚¬" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹œì‘ ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        new_hire_keywords = ['ì‹ ì…', 'ì…ì‚¬', 'ìƒˆë¡œ', 'ì‹ ê·œ', 'ì‹œì‘', 'ì²˜ìŒ']
        focus_on_start_year = any(keyword in query.lower() for keyword in new_hire_keywords)
        
        if is_recent_query or years_info.get('n_years') or years_info.get('specific_year'):
            current_year = datetime.now().year
            
            # ì—°ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ 3ë…„
            if years_info.get('n_years'):
                min_year = current_year - years_info['n_years']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ì—°ë„: ìµœê·¼ {years_info['n_years']}ë…„ ({min_year}ë…„ ì´í›„)")
            elif years_info.get('specific_year'):
                min_year = years_info['specific_year']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ íŠ¹ì • ì—°ë„: {min_year}ë…„ ì´í›„")
            else:
                min_year = current_year - 3  # ê¸°ë³¸ê°’: ìµœê·¼ 3ë…„
                self.logger.info(f"ê¸°ë³¸ ì„¤ì •: ìµœê·¼ 3ë…„ ({min_year}ë…„ ì´í›„)")
            
            # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§
            filtered_docs = self._filter_docs_by_time(docs, min_year, focus_on_start_year)
            final_docs = filtered_docs[:k]
        else:
            final_docs = docs[:k]
        
        # íšŒì‚¬ ë¹„ì „ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€ (ì»¤ë¦¬ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°)
        career_keywords = ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì„±ì¥', 'ë°œì „', 'ëª©í‘œ', 'ë°©í–¥', 'ê³„íš', 'ë¹„ì „', 'ë¯¸ë˜', 'íšŒì‚¬', 'ì¡°ì§']
        if any(keyword in query.lower() for keyword in career_keywords):
            company_vision = self._load_company_vision()
            if company_vision:
                vision_content = self._format_company_vision_for_context(company_vision)
                vision_doc = Document(
                    page_content=vision_content,
                    metadata={"type": "company_vision", "source": "company_vision.json"}
                )
                final_docs.append(vision_doc)
                self.logger.info("íšŒì‚¬ ë¹„ì „ ì •ë³´ê°€ ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì™„ë£Œ: {len(final_docs)}ê°œ ê²°ê³¼ ë°˜í™˜")
        return final_docs
    
    def _filter_docs_by_time(self, docs: List[Document], min_year: int, focus_on_start_year: bool) -> List[Document]:
        """ì‹œê°„ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§"""
        filtered_docs = []
        
        for doc in docs:
            try:
                metadata = doc.metadata or {}
                
                if focus_on_start_year:
                    # ì‹ ì…/ì…ì‚¬ ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš°: ì‹œì‘ ì—°ë„ ê¸°ì¤€
                    start_year = metadata.get('activity_start_year')
                    if start_year and isinstance(start_year, int) and start_year >= min_year:
                        filtered_docs.append(doc)
                        self.logger.debug(f"í¬í•¨: {start_year}ë…„ ì‹œì‘ í™œë™")
                else:
                    # ì¼ë°˜ ìµœê·¼ ì¿¼ë¦¬ì¸ ê²½ìš°: ìµœê·¼ í™œë™ì´ ìˆì—ˆë˜ ì§ì›ë“¤ ì¤‘ì—ì„œ
                    activity_years = metadata.get('activity_years_list', [])
                    if activity_years and isinstance(activity_years, list):
                        recent_activity_years = [year for year in activity_years 
                                               if isinstance(year, int) and year >= min_year]
                        if recent_activity_years:
                            filtered_docs.append(doc)
                            self.logger.debug(f"í¬í•¨: ìµœê·¼ í™œë™ ì—°ë„ {recent_activity_years}")
                            continue
                    
                    # í´ë°±: ì¢…ë£Œ ì—°ë„ê°€ ìµœê·¼ì¸ì§€ í™•ì¸
                    end_year = metadata.get('activity_end_year')
                    if end_year and isinstance(end_year, int) and end_year >= min_year:
                        filtered_docs.append(doc)
                        self.logger.debug(f"í¬í•¨: {end_year}ë…„ ì¢…ë£Œ í™œë™")
                        
            except Exception as e:
                self.logger.warning(f"ë¬¸ì„œ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.info(f"ì‹œê°„ í•„í„°ë§ ì™„ë£Œ: ì „ì²´ {len(docs)}ê°œ â†’ í•„í„°ë§ëœ {len(filtered_docs)}ê°œ ë¬¸ì„œ")
        return filtered_docs
    
    def _fallback_career_search(self, query: str, k: int = 3) -> List[Document]:
        """ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ê²€ìƒ‰"""
        self.logger.warning("ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨, JSON íŒŒì¼ í´ë°± ê²€ìƒ‰ ì‹œë„")
        
        try:
            career_docs_path = PathConfig.get_abs_path(PathConfig.CAREER_DOCS)
            with open(career_docs_path, 'r', encoding='utf-8') as f:
                json_docs = json.load(f)
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            query_keywords = query.lower().split()
            matching_docs = []
            
            for doc_data in json_docs:
                content = doc_data.get('page_content', '').lower()
                score = sum(1 for keyword in query_keywords if keyword in content)
                
                if score > 0:
                    doc = Document(
                        page_content=doc_data['page_content'],
                        metadata=doc_data.get('metadata', {})
                    )
                    matching_docs.append((doc, score))
            
            # ì ìˆ˜ìˆœ ì •ë ¬ í›„ kê°œ ë°˜í™˜
            matching_docs.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in matching_docs[:k]]
            
        except Exception as e:
            self.logger.error(f"í´ë°± ê²€ìƒ‰ë„ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_years_from_query(self, query: str) -> dict:
        """ì¿¼ë¦¬ì—ì„œ ì—°ë„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        years_info = {'n_years': None, 'specific_year': None}
        
        # "ìµœê·¼ Në…„" íŒ¨í„´ ë§¤ì¹­
        year_patterns = [
            r'ìµœê·¼\s*(\d+)\s*ë…„',  # ìµœê·¼ 5ë…„
            r'ì§€ë‚œ\s*(\d+)\s*ë…„',  # ì§€ë‚œ 5ë…„
            r'ê³¼ê±°\s*(\d+)\s*ë…„',  # ê³¼ê±° 5ë…„
            r'(\d+)\s*ë…„\s*ë™ì•ˆ',  # 5ë…„ ë™ì•ˆ
            r'(\d+)\s*ë…„\s*ê°„',    # 5ë…„ê°„
            r'(\d+)\s*ë…„\s*ì´ë‚´',  # 5ë…„ ì´ë‚´
            r'(\d+)\s*ë…„\s*ì‚¬ì´',  # 5ë…„ ì‚¬ì´
        ]
        
        for pattern in year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    n_years = int(match.group(1))
                    if 1 <= n_years <= 50:  # ìœ íš¨í•œ ë²”ìœ„
                        years_info['n_years'] = n_years
                        break
                except ValueError:
                    continue
        
        # íŠ¹ì • ì—°ë„ íŒ¨í„´ ë§¤ì¹­
        specific_year_patterns = [
            r'(\d{4})\s*ë…„\s*ì´í›„',  # 2020ë…„ ì´í›„
            r'(\d{4})\s*ë…„\s*ë¶€í„°',  # 2020ë…„ë¶€í„°
            r'(\d{4})\s*ë…„\s*ì´ìƒ',  # 2020ë…„ ì´ìƒ
            r'(\d{4})\s*ì´í›„',      # 2020 ì´í›„
            r'(\d{4})\s*ë¶€í„°',      # 2020 ë¶€í„°
        ]
        
        for pattern in specific_year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    year = int(match.group(1))
                    if 2000 <= year <= datetime.now().year:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        years_info['specific_year'] = year
                        break
                except ValueError:
                    continue
        
        return years_info

    def search_education_courses(self, query: str, user_profile: Dict, intent_analysis: Dict) -> Dict:
        """êµìœ¡ê³¼ì • ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜ - ChromaDB ìš°ì„ , í´ë°±ìœ¼ë¡œ JSON ê²€ìƒ‰"""
        print(f"ğŸ” [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        
        try:
            # ChromaDBì—ì„œ ê²€ìƒ‰ ì‹œë„
            docs = self.chroma_client.search_education_documents(query, k=6)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            
            if docs:
                # ChromaDB ê²°ê³¼ë¥¼ Dict í˜•íƒœë¡œ ë³€í™˜
                courses = [self._doc_to_course_dict(doc) for doc in docs]
                
                # ì›ë³¸ ë°ì´í„°ë¡œ ìƒì„¸ ì •ë³´ ë³´ê°•
                courses = [self._enrich_course_with_original_data(course) for course in courses]
                
                # ì‚¬ìš©ì ì„ í˜¸ë„ ì ìš©
                preferred_source = self._get_preferred_education_source(query, user_profile, intent_analysis)
                if preferred_source:
                    courses = self._filter_by_preferred_source(courses, preferred_source)
                
                # ì¤‘ë³µ ì œê±°
                courses = self._deduplicate_courses(courses)
                
                # ìµœì¢… 3ê°œë¡œ ì œí•œ
                courses = courses[:3]
                
                print(f"âœ… [êµìœ¡ê³¼ì • ê²€ìƒ‰] ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {len(courses)}ê°œ ê³¼ì • ë°˜í™˜")
                
            else:
                # ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°±
                print(f"âš ï¸ [êµìœ¡ê³¼ì • ê²€ìƒ‰] ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨, JSON í´ë°± ê²€ìƒ‰")
                courses = self._fallback_education_search(query, user_profile, intent_analysis)
            
            # ê²°ê³¼ ë¶„ì„ ë° í•™ìŠµ ê²½ë¡œ ìƒì„±
            course_analysis = self._analyze_course_recommendations(courses)
            learning_path = self._generate_learning_path(courses)
            
            return {
                "recommended_courses": courses,
                "course_analysis": course_analysis,
                "learning_path": learning_path
            }
            
        except Exception as e:
            self.logger.error(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "recommended_courses": [],
                "course_analysis": {"message": f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"},
                "learning_path": []
            }
    
    def _fallback_education_search(self, query: str, user_profile: Dict, intent_analysis: Dict) -> List[Dict]:
        """ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ JSON í´ë°± ê²€ìƒ‰"""
        try:
            # ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ë¦¬ì†ŒìŠ¤ ë¡œë“œ
            self._load_education_resources()
            
            # ìŠ¤í‚¬ ê¸°ë°˜ í•„í„°ë§
            skill_based_courses = self._skill_based_course_filter(user_profile, intent_analysis)
            
            # JSONì—ì„œ ì˜ë¯¸ì  ê²€ìƒ‰
            semantic_matches = self._search_from_json_documents(query, skill_based_courses)
            
            # ì„ í˜¸ë„ í•„í„°ë§
            preferred_source = self._get_preferred_education_source(query, user_profile, intent_analysis)
            if preferred_source:
                semantic_matches = self._filter_by_preferred_source(semantic_matches, preferred_source)
            
            # ì¤‘ë³µ ì œê±° ë° 3ê°œë¡œ ì œí•œ
            deduplicated_courses = self._deduplicate_courses(semantic_matches)[:3]
            
            return deduplicated_courses
            
        except Exception as e:
            self.logger.error(f"í´ë°± êµìœ¡ê³¼ì • ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _doc_to_course_dict(self, doc: Document) -> Dict:
        """ChromaDB Documentë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc.metadata or {}
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc.page_content,
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")
        }
    
    # ê¸°ì¡´ ë©”ì†Œë“œë“¤ ìœ ì§€ (ìŠ¤í‚¬ ê¸°ë°˜ í•„í„°ë§, ì¤‘ë³µ ì œê±° ë“±)
    def _load_education_resources(self):
        """êµìœ¡ê³¼ì • ë¦¬ì†ŒìŠ¤ ì§€ì—° ë¡œë”©"""
        if self.skill_education_mapping is None:
            self._load_skill_education_mapping()
        if self.course_deduplication_index is None:
            self._load_deduplication_index()
    
    def _load_skill_education_mapping(self):
        """ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ"""
        try:
            if os.path.exists(self.skill_mapping_path):
                with open(self.skill_mapping_path, "r", encoding="utf-8") as f:
                    self.skill_education_mapping = json.load(f)
                self.logger.info(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.skill_education_mapping)}ê°œ ìŠ¤í‚¬")
            else:
                self.skill_education_mapping = {}
                self.logger.warning("ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.skill_education_mapping = {}
    
    def _load_deduplication_index(self):
        """ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if os.path.exists(self.deduplication_index_path):
                with open(self.deduplication_index_path, "r", encoding="utf-8") as f:
                    self.course_deduplication_index = json.load(f)
                self.logger.info(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.course_deduplication_index)}ê°œ ê·¸ë£¹")
            else:
                self.course_deduplication_index = {}
                self.logger.warning("ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.course_deduplication_index = {}
    
    def _load_company_vision(self):
        """íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ"""
        if self.company_vision_data is not None:
            return self.company_vision_data
            
        try:
            if os.path.exists(self.company_vision_path):
                with open(self.company_vision_path, "r", encoding="utf-8") as f:
                    self.company_vision_data = json.load(f)
                self.logger.info("íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            else:
                self.company_vision_data = {}
                self.logger.warning("íšŒì‚¬ ë¹„ì „ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.company_vision_data = {}
        
        return self.company_vision_data
    
    def _format_company_vision_for_context(self, vision_data: Dict) -> str:
        """íšŒì‚¬ ë¹„ì „ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not vision_data:
            return ""
        
        sections = []
        
        # íšŒì‚¬ ê¸°ë³¸ ì •ë³´
        if vision_data.get('company_name'):
            sections.append(f"íšŒì‚¬ëª…: {vision_data['company_name']}")
        
        # ë¹„ì „
        if vision_data.get('vision'):
            vision = vision_data['vision']
            sections.append(f"ë¹„ì „: {vision.get('title', '')}")
            if vision.get('description'):
                sections.append(f"ë¹„ì „ ì„¤ëª…: {vision['description']}")
        
        # í•µì‹¬ ê°€ì¹˜
        if vision_data.get('core_values'):
            values_text = []
            for value in vision_data['core_values']:
                values_text.append(f"- {value.get('name', '')}: {value.get('description', '')}")
            if values_text:
                sections.append("í•µì‹¬ ê°€ì¹˜:\n" + "\n".join(values_text))
        
        # ì „ëµ ë°©í–¥
        if vision_data.get('strategic_directions'):
            strategy_text = []
            for direction in vision_data['strategic_directions']:
                strategy_text.append(f"- {direction.get('category', '')}: {direction.get('description', '')}")
            if strategy_text:
                sections.append("ì „ëµ ë°©í–¥:\n" + "\n".join(strategy_text))
        
        # ì¸ì¬ ê°œë°œ
        if vision_data.get('talent_development'):
            talent = vision_data['talent_development']
            sections.append(f"ì¸ì¬ ê°œë°œ ì² í•™: {talent.get('philosophy', '')}")
            if talent.get('focus_areas'):
                focus_text = []
                for area in talent['focus_areas']:
                    focus_text.append(f"- {area.get('area', '')}: {area.get('description', '')}")
                if focus_text:
                    sections.append("ì—­ëŸ‰ ê°œë°œ ì¤‘ì  ì˜ì—­:\n" + "\n".join(focus_text))
        
        # ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™
        if vision_data.get('career_guidance_principles'):
            principles_text = []
            for principle in vision_data['career_guidance_principles']:
                principles_text.append(f"- {principle.get('principle', '')}: {principle.get('description', '')}")
            if principles_text:
                sections.append("ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™:\n" + "\n".join(principles_text))
        
        return "\n\n".join(sections)
    
    def _skill_based_course_filter(self, user_profile: Dict, intent_analysis: Dict) -> List[Dict]:
        """ìŠ¤í‚¬ ê¸°ë°˜ 1ì°¨ í•„í„°ë§"""
        filtered_courses = []
        
        # ì‚¬ìš©ì í˜„ì¬ ìŠ¤í‚¬ ì¶”ì¶œ
        current_skills = self._extract_user_skills(user_profile)
        
        # ì˜ë„ ë¶„ì„ì—ì„œ ëª©í‘œ ìŠ¤í‚¬ ì¶”ì¶œ
        target_skills = intent_analysis.get("career_history", [])
        
        # ê²€ìƒ‰í•  ìŠ¤í‚¬ ëª©ë¡ ìƒì„±
        search_skills = list(set(current_skills + target_skills))
        
        for skill_code in search_skills:
            if skill_code in self.skill_education_mapping:
                skill_courses = self.skill_education_mapping[skill_code]
                
                # College ê³¼ì • ì¶”ê°€
                for course_type in ["specialized", "recommended", "common_required"]:
                    if course_type in skill_courses.get("college", {}):
                        courses = skill_courses["college"][course_type]
                        for course in courses:
                            course_info = course.copy()
                            course_info["source"] = "college"
                            course_info["skill_relevance"] = course_type
                            course_info["target_skill"] = skill_code
                            filtered_courses.append(course_info)
                
                # mySUNI ê³¼ì • ì¶”ê°€
                if "mysuni" in skill_courses:
                    for course in skill_courses["mysuni"]:
                        course_info = course.copy()
                        course_info["source"] = "mysuni"
                        course_info["skill_relevance"] = "general"
                        course_info["target_skill"] = skill_code
                        filtered_courses.append(course_info)
        
        self.logger.info(f"ìŠ¤í‚¬ ê¸°ë°˜ í•„í„°ë§ ê²°ê³¼: {len(filtered_courses)}ê°œ ê³¼ì •")
        return filtered_courses
    
    def _extract_user_skills(self, user_profile: Dict) -> List[str]:
        """ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ìŠ¤í‚¬ ì¶”ì¶œ"""
        skills = []
        
        if "skills" in user_profile:
            skills.extend(user_profile["skills"])
        
        if "career_history" in user_profile:
            for career in user_profile["career_history"]:
                if "skills" in career:
                    skills.extend(career["skills"])
        
        return list(set(skills))
    
    def _search_from_json_documents(self, query: str, filtered_courses: List[Dict]) -> List[Dict]:
        """JSON ë¬¸ì„œì—ì„œ ì§ì ‘ ê²€ìƒ‰"""
        try:
            with open(self.education_docs_path, "r", encoding="utf-8") as f:
                all_docs = json.load(f)
        except FileNotFoundError:
            self.logger.warning("êµìœ¡ê³¼ì • ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return filtered_courses[:3] if filtered_courses else []
        
        # í•„í„°ë§ëœ ê³¼ì •ì´ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
        if filtered_courses:
            filtered_course_ids = {course.get("course_id") for course in filtered_courses}
            matching_docs = []
            
            for doc in all_docs:
                metadata = doc.get("metadata", {})
                course_id = metadata.get("course_id")
                
                if course_id in filtered_course_ids:
                    course_dict = self._doc_to_course_dict_from_json(doc)
                    # í•„í„°ë§ ì •ë³´ ë³‘í•©
                    for filtered_course in filtered_courses:
                        if course_dict.get("course_id") == filtered_course.get("course_id"):
                            course_dict.update(filtered_course)
                            break
                    matching_docs.append(course_dict)
            
            if matching_docs:
                return matching_docs[:3]
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_keywords = query.lower().split()
        matching_docs = []
        
        for doc in all_docs:
            content = doc.get("page_content", "").lower()
            score = sum(1 for keyword in query_keywords if keyword in content)
            
            if score > 0:
                course_dict = self._doc_to_course_dict_from_json(doc)
                course_dict["match_score"] = score
                matching_docs.append(course_dict)
        
        matching_docs.sort(key=lambda x: x.get("match_score", 0), reverse=True)
        return matching_docs[:3]
    
    def _doc_to_course_dict_from_json(self, doc_data: Dict) -> Dict:
        """JSON ë¬¸ì„œ ë°ì´í„°ë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc_data.get("metadata", {})
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc_data.get("page_content", ""),
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")
        }
    
    def _get_preferred_education_source(self, query: str, user_profile: Dict, intent_analysis: Dict) -> str:
        """ì‚¬ìš©ìì˜ êµìœ¡ê³¼ì • ì†ŒìŠ¤ ì„ í˜¸ë„ ê°ì§€"""
        query_lower = query.lower()
        if 'mysuni' in query_lower or 'my suni' in query_lower:
            return 'mysuni'
        elif 'college' in query_lower or 'ì»¬ë¦¬ì§€' in query_lower:
            return 'college'
        
        preferred_source = user_profile.get('preferred_education_source', '')
        if preferred_source in ['mysuni', 'college']:
            return preferred_source
        
        intent_preferred = intent_analysis.get('preferred_source', '')
        if intent_preferred in ['mysuni', 'college']:
            return intent_preferred
        
        return ''
    
    def _filter_by_preferred_source(self, courses: List[Dict], preferred_source: str) -> List[Dict]:
        """ì„ í˜¸í•˜ëŠ” êµìœ¡ê³¼ì • ì†ŒìŠ¤ë¡œ í•„í„°ë§"""
        if not preferred_source or not courses:
            return courses
        
        preferred_courses = [course for course in courses if course.get('source') == preferred_source]
        
        if len(preferred_courses) >= 3:
            self.logger.info(f"{preferred_source} ê³¼ì • {len(preferred_courses)}ê°œë¡œ í•„í„°ë§")
            return preferred_courses
        
        other_courses = [course for course in courses if course.get('source') != preferred_source]
        result = preferred_courses + other_courses[:7-len(preferred_courses)]
        
        self.logger.info(f"{preferred_source} ìš°ì„  í•„í„°ë§: {len(preferred_courses)}ê°œ + ê¸°íƒ€ {len(result)-len(preferred_courses)}ê°œ")
        return result
    
    def _deduplicate_courses(self, courses: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ê³¼ì • ì œê±°"""
        if not courses:
            return []
        
        deduplicated = []
        seen_courses = set()
        
        def sort_priority(course):
            source_priority = 0 if course.get("source") == "college" else 1
            
            if course.get("source") == "college":
                relevance = course.get("skill_relevance", "")
                if relevance == "specialized":
                    relevance_priority = 0
                elif relevance == "recommended":
                    relevance_priority = 1
                else:
                    relevance_priority = 2
            else:
                rating = course.get("í‰ì ", 0)
                try:
                    rating = float(rating) if rating else 0
                except:
                    rating = 0
                relevance_priority = 5 - rating
            
            return (source_priority, relevance_priority)
        
        sorted_courses = sorted(courses, key=sort_priority)
        
        for course in sorted_courses:
            course_signature = self._generate_course_signature(course)
            
            if course_signature not in seen_courses:
                if course_signature in self.course_deduplication_index:
                    duplicate_info = self.course_deduplication_index[course_signature]
                    
                    if course.get("source") == "college":
                        mysuni_data = self._find_mysuni_duplicate(duplicate_info, courses)
                        if mysuni_data:
                            course["mysuni_alternative"] = {
                                "available": True,
                                "card_name": mysuni_data.get("card_name"),
                                "í‰ì ": mysuni_data.get("í‰ì "),
                                "ì´ìˆ˜ììˆ˜": mysuni_data.get("ì´ìˆ˜ììˆ˜"),
                                "ë‚œì´ë„": mysuni_data.get("ë‚œì´ë„"),
                                "ì¸ì •í•™ìŠµì‹œê°„": mysuni_data.get("ì¸ì •í•™ìŠµì‹œê°„"),
                                "ì¹´í…Œê³ ë¦¬ëª…": mysuni_data.get("ì¹´í…Œê³ ë¦¬ëª…"),
                                "ì±„ë„ëª…": mysuni_data.get("ì±„ë„ëª…")
                            }
                        else:
                            course["mysuni_alternative"] = {"available": False}
                    else:
                        course["mysuni_alternative"] = {"available": False}
                    
                    course["alternative_platforms"] = duplicate_info.get("platforms", [])
                else:
                    if course.get("source") == "mysuni":
                        course["mysuni_alternative"] = {"available": False}
                
                deduplicated.append(course)
                seen_courses.add(course_signature)
        
        self.logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(courses)}ê°œ â†’ {len(deduplicated)}ê°œ")
        return deduplicated
    
    def _generate_course_signature(self, course: Dict) -> str:
        """ê³¼ì • ì¤‘ë³µ íŒë³„ì„ ìœ„í•œ ì‹œê·¸ë‹ˆì²˜ ìƒì„±"""
        name = course.get("course_name", course.get("card_name", "")).lower().strip()
        skills = sorted(course.get("target_skills", []))
        
        normalized_name = re.sub(r'[^\w\s]', '', name)
        normalized_name = re.sub(r'\s+', ' ', normalized_name)
        
        return f"{normalized_name}_{','.join(skills)}"
    
    def _find_mysuni_duplicate(self, duplicate_info: Dict, all_courses: List[Dict]) -> Dict:
        """ì¤‘ë³µ ì •ë³´ì—ì„œ mySUNI ê³¼ì • ì°¾ê¸°"""
        mysuni_course_info = None
        for course_info in duplicate_info.get("courses", []):
            if course_info.get("platform") == "mySUNI":
                course_id = course_info.get("course_id")
                for course in all_courses:
                    if (course.get("source") == "mysuni" and 
                        course.get("course_id") == course_id):
                        mysuni_course_info = course
                        break
                break
        
        return mysuni_course_info
    
    def _analyze_course_recommendations(self, courses: List[Dict]) -> Dict:
        """ì¶”ì²œ ê³¼ì • ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        if not courses:
            return {"message": "ì¶”ì²œí•  êµìœ¡ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        college_courses = [c for c in courses if c.get("source") == "college"]
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        
        specialized_count = len([c for c in college_courses if c.get("skill_relevance") == "specialized"])
        recommended_count = len([c for c in college_courses if c.get("skill_relevance") == "recommended"])
        required_count = len([c for c in college_courses if c.get("skill_relevance") == "common_required"])
        
        college_with_mysuni_alt = len([c for c in college_courses 
                                      if c.get("mysuni_alternative", {}).get("available")])
        
        mysuni_ratings = []
        for c in mysuni_courses:
            rating = c.get("í‰ì ", 0)
            try:
                rating = float(rating) if rating else 0
                if rating > 0:
                    mysuni_ratings.append(rating)
            except:
                continue
        
        avg_mysuni_rating = sum(mysuni_ratings) / len(mysuni_ratings) if mysuni_ratings else 0
        
        total_enrollments = 0
        for course in mysuni_courses:
            enrollments_str = str(course.get("ì´ìˆ˜ììˆ˜", "0"))
            try:
                enrollments = int(enrollments_str.replace(",", "")) if enrollments_str.replace(",", "").isdigit() else 0
                total_enrollments += enrollments
            except:
                continue
        
        return {
            "total_courses": len(courses),
            "college_courses": len(college_courses),
            "mysuni_courses": len(mysuni_courses),
            "skill_depth_analysis": {
                "specialized": specialized_count,
                "recommended": recommended_count, 
                "common_required": required_count
            },
            "learning_platforms": {
                "college_available": len(college_courses) > 0,
                "mysuni_available": len(mysuni_courses) > 0,
                "college_with_mysuni_alternatives": college_with_mysuni_alt
            },
            "mysuni_quality_metrics": {
                "average_rating": round(avg_mysuni_rating, 1),
                "total_enrollments": total_enrollments,
                "high_rated_courses": len([r for r in mysuni_ratings if r >= 4.5])
            }
        }
    
    def _generate_learning_path(self, courses: List[Dict]) -> List[Dict]:
        """í•™ìŠµ ê²½ë¡œ ì œì•ˆ ìƒì„±"""
        if not courses:
            return []
        
        path = []
        
        required_courses = [c for c in courses if c.get("skill_relevance") == "common_required"]
        if required_courses:
            path.append({
                "step": 1,
                "level": "ê¸°ì´ˆ/í•„ìˆ˜",
                "courses": required_courses[:2],
                "description": "ê¸°ë³¸ ì§€ì‹ ìŠµë“ì„ ìœ„í•œ í•„ìˆ˜ ê³¼ì •"
            })
        
        recommended_courses = [c for c in courses if c.get("skill_relevance") == "recommended"]
        if recommended_courses:
            path.append({
                "step": 2,
                "level": "í™•ì¥/ì‘ìš©",
                "courses": recommended_courses[:3],
                "description": "ê´€ë ¨ ê¸°ìˆ  í™•ì¥ì„ ìœ„í•œ ì¶”ì²œ ê³¼ì •"
            })
        
        specialized_courses = [c for c in courses if c.get("skill_relevance") == "specialized"]
        if specialized_courses:
            path.append({
                "step": 3,
                "level": "ì „ë¬¸/ì‹¬í™”",
                "courses": specialized_courses[:2],
                "description": "ì „ë¬¸ì„± ê°•í™”ë¥¼ ìœ„í•œ íŠ¹í™” ê³¼ì •"
            })
        
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        if mysuni_courses:
            path.append({
                "step": "ë³´ì™„",
                "level": "ì˜¨ë¼ì¸/ììœ¨",
                "courses": mysuni_courses[:3],
                "description": "ì˜¨ë¼ì¸ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ë³´ì™„ ê³¼ì •"
            })
        
        return path
    
    def _load_original_course_data(self):
        """ì›ë³¸ êµìœ¡ê³¼ì • ìƒì„¸ ë°ì´í„° ë¡œë“œ"""
        if not hasattr(self, 'original_mysuni_data'):
            try:
                mysuni_path = PathConfig.get_abs_path(PathConfig.MYSUNI_DETAILED)
                with open(mysuni_path, "r", encoding="utf-8") as f:
                    self.original_mysuni_data = json.load(f)
                self.logger.info(f"mySUNI ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_mysuni_data)}ê°œ")
            except FileNotFoundError:
                self.logger.warning("mySUNI ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.original_mysuni_data = []
                
        if not hasattr(self, 'original_college_data'):
            try:
                college_path = PathConfig.get_abs_path(PathConfig.COLLEGE_DETAILED)
                with open(college_path, "r", encoding="utf-8") as f:
                    self.original_college_data = json.load(f)
                self.logger.info(f"College ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_college_data)}ê°œ")
            except FileNotFoundError:
                self.logger.warning("College ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.original_college_data = []

    def _enrich_course_with_original_data(self, course: Dict) -> Dict:
        """ì›ë³¸ ë°ì´í„°ë¡œ ê³¼ì • ì •ë³´ ë³´ê°•"""
        self._load_original_course_data()
        
        course_id = course.get("course_id")
        source = course.get("source")
        
        if not course_id:
            return course
            
        if source == "mysuni":
            for original in self.original_mysuni_data:
                if original.get("course_id") == course_id:
                    course.update({
                        "ì¹´í…Œê³ ë¦¬ëª…": original.get("ì¹´í…Œê³ ë¦¬ëª…"),
                        "ì±„ë„ëª…": original.get("ì±„ë„ëª…"),
                        "íƒœê·¸ëª…": original.get("íƒœê·¸ëª…"),
                        "ë‚œì´ë„": original.get("ë‚œì´ë„"),
                        "í‰ì ": original.get("í‰ì "),
                        "ì´ìˆ˜ììˆ˜": original.get("ì´ìˆ˜ììˆ˜"),
                        "ì§ë¬´": original.get("ì§ë¬´", []),
                        "skillset": original.get("skillset", []),
                        "url": original.get("url")
                    })
                    break
                    
        elif source == "college":
            for original in self.original_college_data:
                if original.get("course_id") == course_id:
                    course.update({
                        "í•™ë¶€": original.get("í•™ë¶€"),
                        "í‘œì¤€ê³¼ì •": original.get("í‘œì¤€ê³¼ì •"),
                        "ì‚¬ì—…ë³„êµìœ¡ì²´ê³„": original.get("ì‚¬ì—…ë³„êµìœ¡ì²´ê³„"),
                        "êµìœ¡ìœ í˜•": original.get("êµìœ¡ìœ í˜•"),
                        "í•™ìŠµìœ í˜•": original.get("í•™ìŠµìœ í˜•"),
                        "ê³µê°œì—¬ë¶€": original.get("ê³µê°œì—¬ë¶€"),
                        "íŠ¹í™”ì§ë¬´": original.get("íŠ¹í™”ì§ë¬´", []),
                        "ì¶”ì²œì§ë¬´": original.get("ì¶”ì²œì§ë¬´", []),
                        "ê³µí†µí•„ìˆ˜ì§ë¬´": original.get("ê³µí†µí•„ìˆ˜ì§ë¬´", []),
                        "url": original.get("url")
                    })
                    break
        
        return course