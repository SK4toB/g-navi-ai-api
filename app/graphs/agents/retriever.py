# app/graphs/agents/retriever.py

import os
import json
import re
import requests
import logging
from typing import Dict, List, Any
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.schema import Document
from datetime import datetime, timedelta
from .k8s_chroma_adapter import K8sChromaDBAdapter, K8sChromaRetriever

from dotenv import load_dotenv
load_dotenv()


# ==================== ğŸ“‚ ê²½ë¡œ ì„¤ì • (ìˆ˜ì • í•„ìš”ì‹œ ì—¬ê¸°ë§Œ ë³€ê²½) ====================
class PathConfig:
    """
    ëª¨ë“  ê²½ë¡œ ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    K8s í™˜ê²½ì—ì„œëŠ” PVC ë§ˆìš´íŠ¸ ê²½ë¡œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # app ë””ë ‰í† ë¦¬
    
    @classmethod
    def _get_k8s_pvc_path(cls) -> str:
        """K8s PVC ë§ˆìš´íŠ¸ ê²½ë¡œ ë°˜í™˜"""
        return os.environ.get('APP_STORAGE_PVC_PATH', '/mnt/gnavi')
    
    @classmethod
    def _is_k8s_environment(cls) -> bool:
        """K8s í™˜ê²½ì¸ì§€ í™•ì¸"""
        pvc_path = cls._get_k8s_pvc_path()
        return os.path.exists(pvc_path)
    
    @classmethod
    def _get_app_root_dir(cls) -> str:
        """app ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë°˜í™˜ (graphs/agentsì—ì„œ appê¹Œì§€ ì˜¬ë¼ê°€ê¸°)"""
        # í˜„ì¬ íŒŒì¼ì´ app/graphs/agents/retriever.py ë¼ë©´
        # app ë””ë ‰í† ë¦¬ê¹Œì§€ ì˜¬ë¼ê°€ì•¼ í•¨
        current_dir = os.path.dirname(__file__)  # app/graphs/agents/
        app_dir = os.path.dirname(os.path.dirname(current_dir))  # app/
        return app_dir
    
    @classmethod
    def _get_smart_docs_path(cls, filename: str) -> str:
        """K8s í™˜ê²½ì´ë©´ PVC ê²½ë¡œ, ì•„ë‹ˆë©´ ë¡œì»¬ app/storage/docs ê²½ë¡œ ë°˜í™˜"""
        if cls._is_k8s_environment():
            # K8s í™˜ê²½: /mnt/gnavi/docs/filename
            k8s_path = os.path.join(cls._get_k8s_pvc_path(), 'docs', filename)
            if os.path.exists(k8s_path):
                return k8s_path
            # K8s í™˜ê²½ì´ì§€ë§Œ PVCì— íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì»¬ í´ë°±
            local_fallback = os.path.join(cls._get_app_root_dir(), 'storage', 'docs', filename)
            if os.path.exists(local_fallback):
                return local_fallback
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ K8s ê²½ë¡œ ë°˜í™˜ (ì›ë˜ ì˜ë„ëŒ€ë¡œ)
            return k8s_path
        else:
            # ë¡œì»¬ í™˜ê²½: app/storage/docs/filename  
            return os.path.join(cls._get_app_root_dir(), 'storage', 'docs', filename)
    
    # ğŸ“Š ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ (Chroma DB ì €ì¥ì†Œ) - ê¸°ì¡´ ë°©ì‹ ìœ ì§€
    CAREER_VECTOR_STORE = "../../storage/vector_stores/career_data"
    EDUCATION_VECTOR_STORE = "../../storage/vector_stores/education_courses"
    
    # ğŸ—„ï¸ ìºì‹œ ê²½ë¡œ (ì„ë² ë”© ìºì‹œ) - ê¸°ì¡´ ë°©ì‹ ìœ ì§€  
    CAREER_EMBEDDING_CACHE = "../../storage/cache/embedding_cache"
    EDUCATION_EMBEDDING_CACHE = "../../storage/cache/education_embedding_cache"
    
    # ğŸ“„ ë¬¸ì„œ ê²½ë¡œ (JSON ë°ì´í„° íŒŒì¼ë“¤) - ìŠ¤ë§ˆíŠ¸ ê²½ë¡œ ì ìš© (ê¸°ì¡´ ì†ì„±ëª… ìœ ì§€)
    @classmethod
    def _init_paths(cls):
        """ê²½ë¡œ ì´ˆê¸°í™” - ëª¨ë“ˆ ë¡œë“œ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰"""
        cls.CAREER_DOCS = cls._get_smart_docs_path("career_history.json")
        cls.EDUCATION_DOCS = cls._get_smart_docs_path("education_courses.json") 
        cls.SKILL_MAPPING = cls._get_smart_docs_path("skill_education_mapping.json")
        cls.COURSE_DEDUPLICATION = cls._get_smart_docs_path("course_deduplication_index.json")
        cls.COMPANY_VISION = cls._get_smart_docs_path("company_vision.json")
        cls.MYSUNI_DETAILED = cls._get_smart_docs_path("mysuni_courses_detailed.json")
        cls.COLLEGE_DETAILED = cls._get_smart_docs_path("college_courses_detailed.json")
    
    @classmethod
    def get_abs_path(cls, relative_path: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        return os.path.abspath(os.path.join(os.path.dirname(__file__), relative_path))
    
    @classmethod
    def log_current_environment(cls):
        """í˜„ì¬ í™˜ê²½ ì •ë³´ ë¡œê·¸ ì¶œë ¥"""
        env_type = "K8s PVC" if cls._is_k8s_environment() else "ë¡œì»¬"
        print(f"ğŸ” [PathConfig] í™˜ê²½ ê°ì§€: {env_type}")
        print(f"ğŸ“ [PathConfig] App ë£¨íŠ¸ ë””ë ‰í† ë¦¬: {cls._get_app_root_dir()}")
        if cls._is_k8s_environment():
            print(f"ğŸ“ [PathConfig] PVC ê²½ë¡œ: {cls._get_k8s_pvc_path()}")
        print(f"ğŸ“„ [PathConfig] ì»¤ë¦¬ì–´ ë¬¸ì„œ: {cls.CAREER_DOCS}")
        print(f"ğŸ“š [PathConfig] êµìœ¡ê³¼ì • ë¬¸ì„œ: {cls.EDUCATION_DOCS}")
        print(f"ğŸ”— [PathConfig] ìŠ¤í‚¬ ë§¤í•‘: {cls.SKILL_MAPPING}")
        print(f"ğŸ”„ [PathConfig] ì¤‘ë³µì œê±° ì¸ë±ìŠ¤: {cls.COURSE_DEDUPLICATION}")
        print(f"ğŸ¢ [PathConfig] íšŒì‚¬ ë¹„ì „: {cls.COMPANY_VISION}")
        print(f"ğŸ“ [PathConfig] mySUNI ìƒì„¸: {cls.MYSUNI_DETAILED}")
        print(f"ğŸ« [PathConfig] College ìƒì„¸: {cls.COLLEGE_DETAILED}")
        return env_type
    
    @classmethod
    def check_files_exist(cls):
        """ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        files_to_check = {
            "ì»¤ë¦¬ì–´ ë¬¸ì„œ": cls.CAREER_DOCS,
            "êµìœ¡ê³¼ì • ë¬¸ì„œ": cls.EDUCATION_DOCS,
            "ìŠ¤í‚¬ ë§¤í•‘": cls.SKILL_MAPPING,
            "ì¤‘ë³µì œê±° ì¸ë±ìŠ¤": cls.COURSE_DEDUPLICATION,
            "íšŒì‚¬ ë¹„ì „": cls.COMPANY_VISION,
            "mySUNI ìƒì„¸": cls.MYSUNI_DETAILED,
            "College ìƒì„¸": cls.COLLEGE_DETAILED
        }
        
        missing_files = []
        existing_files = []
        
        for name, path in files_to_check.items():
            if os.path.exists(path):
                existing_files.append(f"âœ… {name}: {path}")
            else:
                missing_files.append(f"âŒ {name}: {path}")
        
        print("ğŸ“‹ [PathConfig] íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸:")
        for file_info in existing_files:
            print(f"  {file_info}")
        for file_info in missing_files:
            print(f"  {file_info}")
        
        return len(missing_files) == 0

# í´ë˜ìŠ¤ ë¡œë“œ ì‹œ ê²½ë¡œ ì´ˆê¸°í™” ì‹¤í–‰
PathConfig._init_paths()

# ==================== ğŸ“‚ ê²½ë¡œ ì„¤ì • ë ====================

class CareerEnsembleRetrieverAgent:
    """
    ğŸ” ì»¤ë¦¬ì–´ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì—ì´ì „íŠ¸
    
    BM25 + LLM ì„ë² ë”© ì•™ìƒë¸”ì„ í™œìš©í•˜ì—¬ ì»¤ë¦¬ì–´ ì‚¬ë¡€ì™€ êµìœ¡ê³¼ì •ì„
    íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ChromaDBì™€ ìºì‹œë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    ğŸ“Š ê²€ìƒ‰ ê²°ê³¼:
    - ì»¤ë¦¬ì–´ ì‚¬ë¡€: ìµœëŒ€ 3ê°œê¹Œì§€ ê²€ìƒ‰
    - êµìœ¡ê³¼ì •: ìµœëŒ€ 3ê°œê¹Œì§€ ê²€ìƒ‰
    """
    def __init__(self, persist_directory: str = None, cache_directory: str = None):
        """
        CareerEnsembleRetrieverAgent ì´ˆê¸°í™”
        
        Args:
            persist_directory: ì»¤ë¦¬ì–´ ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ (ê¸°ë³¸ê°’: PathConfig.CAREER_VECTOR_STORE)
            cache_directory: ì»¤ë¦¬ì–´ ì„ë² ë”© ìºì‹œ ê²½ë¡œ (ê¸°ë³¸ê°’: PathConfig.CAREER_EMBEDDING_CACHE)
        """
        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = logging.getLogger(__name__)
        
        # í™˜ê²½ ì •ë³´ ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        env_type = PathConfig.log_current_environment()
        self.is_k8s = PathConfig._is_k8s_environment()
        print(f"ğŸ” [CareerRetrieverAgent] í™˜ê²½: {env_type}, K8s: {self.is_k8s}")
        
        # ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)
        self.persist_directory = PathConfig.get_abs_path(
            persist_directory or PathConfig.CAREER_VECTOR_STORE
        )
        self.career_cache_directory = PathConfig.get_abs_path(
            cache_directory or PathConfig.CAREER_EMBEDDING_CACHE
        )
        self.base_dir = PathConfig.BASE_DIR
        
        # ë””ë ‰í† ë¦¬ ìƒì„± (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
        if not self.is_k8s:
            os.makedirs(self.persist_directory, exist_ok=True)
            os.makedirs(self.career_cache_directory, exist_ok=True)

        # ì»¤ë¦¬ì–´ ì „ìš© ì„ë² ë”© ì„¤ì •
        self.base_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            dimensions=1536
        )
        
        # ìºì‹œ ì„¤ì • (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
        if not self.is_k8s:
            self.career_cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
                self.base_embeddings,
                LocalFileStore(self.career_cache_directory),
                namespace="career_embeddings"
            )
        else:
            # K8s í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì—†ì´ ì§ì ‘ ì„ë² ë”© ì‚¬ìš©
            self.career_cached_embeddings = self.base_embeddings
        
        # êµìœ¡ê³¼ì • ì „ìš© ì„ë² ë”© ì„¤ì •
        if not self.is_k8s:
            self.education_cache_directory = PathConfig.get_abs_path(PathConfig.EDUCATION_EMBEDDING_CACHE)
            os.makedirs(self.education_cache_directory, exist_ok=True)
            self.education_cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
                self.base_embeddings,
                LocalFileStore(self.education_cache_directory),
                namespace="education_embeddings"
            )
        else:
            # K8s í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì—†ì´ ì§ì ‘ ì„ë² ë”© ì‚¬ìš©
            self.education_cached_embeddings = self.base_embeddings
        
        self.vectorstore = None
        self.ensemble_retriever = None
        
        # êµìœ¡ê³¼ì • ê´€ë ¨ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)
        if not self.is_k8s:
            self.education_persist_dir = PathConfig.get_abs_path(PathConfig.EDUCATION_VECTOR_STORE)
        self.education_docs_path = PathConfig.EDUCATION_DOCS
        self.skill_mapping_path = PathConfig.SKILL_MAPPING
        self.deduplication_index_path = PathConfig.COURSE_DEDUPLICATION
        
        # íšŒì‚¬ ë¹„ì „ ê´€ë ¨ ê²½ë¡œ ì„¤ì •
        self.company_vision_path = PathConfig.COMPANY_VISION
        
        # ì§€ì—° ë¡œë”© ì†ì„±
        self.education_vectorstore = None
        self.skill_education_mapping = None
        self.course_deduplication_index = None
        self.company_vision_data = None
        
        self._load_vectorstore_and_retriever()

    def _load_vectorstore_and_retriever(self):
        """ë²¡í„°ìŠ¤í† ì–´ì™€ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ë¡œë“œ (í™˜ê²½ë³„ ë¶„ê¸°)"""
        if self.is_k8s:
            self._load_k8s_vectorstore_and_retriever()
        else:
            self._load_local_vectorstore_and_retriever()

    def _load_k8s_vectorstore_and_retriever(self):
        """K8s í™˜ê²½: ì™¸ë¶€ ChromaDB ì‚¬ìš©"""
        print("ğŸ”— [K8s ChromaDB] ì™¸ë¶€ ChromaDB ì—°ê²° ì¤‘...")
        
        # K8s ChromaDB ì–´ëŒ‘í„° ì´ˆê¸°í™”
        self.vectorstore = K8sChromaDBAdapter("career_history", self.career_cached_embeddings)
        
        # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection_info = self.vectorstore.get_collection_info()
        if collection_info.get("status") == "success":
            print(f"âœ… [K8s ChromaDB] ì—°ê²° ì„±ê³µ: {collection_info.get('document_count')}ê°œ ë¬¸ì„œ")
        else:
            print(f"âŒ [K8s ChromaDB] ì—°ê²° ì‹¤íŒ¨: {collection_info.get('message')}")
        
        # LLM ì„ë² ë”© ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰ ê²°ê³¼ë¥¼ 3ê°œë¡œ ì œí•œ)
        embedding_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # BM25ìš© docs ë¡œë“œ (JSON íŒŒì¼ì€ ì—¬ì „íˆ ì‚¬ìš©)
        docs_path = PathConfig.CAREER_DOCS
        all_docs = []
        try:
            with open(docs_path, 'r', encoding='utf-8') as f:
                json_docs = json.load(f)
                all_docs = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in json_docs]
            self.logger.info(f"BM25ìš© career_docs.json ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)}) - ê²½ë¡œ: {docs_path}")
        except Exception as e:
            self.logger.warning(f"BM25ìš© career_docs.json ë¡œë“œ ì‹¤íŒ¨: {e} - ê²½ë¡œ: {docs_path}")
        
        # ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±
        retrievers = [embedding_retriever]
        weights = [1.0]
        if all_docs:
            bm25_retriever = BM25Retriever.from_documents(all_docs)
            bm25_retriever.k = 3  # BM25ë„ 3ê°œë¡œ ì œí•œ
            retrievers.append(bm25_retriever)
            weights = [0.3, 0.7]  # K8s ChromaDB: 30%, BM25: 70%
        
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=retrievers,
            weights=weights
        )
        self.logger.info(f"K8s Career ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ (JSON ë¬¸ì„œ ìˆ˜: {len(all_docs)})")
        print(f"âœ… [K8s ì»¤ë¦¬ì–´ ì‚¬ë¡€ VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_local_vectorstore_and_retriever(self):
        """ë¡œì»¬ í™˜ê²½: ê¸°ì¡´ ë¡œì»¬ ChromaDB ì‚¬ìš©"""
        print("ğŸ’¾ [ë¡œì»¬ ChromaDB] ë¡œì»¬ ChromaDB ë¡œë“œ ì¤‘...")
        
        # Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.career_cached_embeddings,
            collection_name="career_history"
        )
        # LLM ì„ë² ë”© ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰ ê²°ê³¼ë¥¼ 3ê°œë¡œ ì œí•œ)
        embedding_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        # BM25ìš© docs ë¡œë“œ
        docs_path = PathConfig.CAREER_DOCS
        all_docs = []
        try:
            with open(docs_path, 'r', encoding='utf-8') as f:
                json_docs = json.load(f)
                all_docs = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in json_docs]
            self.logger.info(f"BM25ìš© career_docs.json ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)}) - ê²½ë¡œ: {docs_path}")
        except Exception as e:
            self.logger.warning(f"BM25ìš© career_docs.json ë¡œë“œ ì‹¤íŒ¨: {e} - ê²½ë¡œ: {docs_path}")
        
        retrievers = [embedding_retriever]
        weights = [1.0]
        if all_docs:
            bm25_retriever = BM25Retriever.from_documents(all_docs)
            bm25_retriever.k = 3  # BM25ë„ 3ê°œë¡œ ì œí•œ
            retrievers.append(bm25_retriever)
            weights = [0.3, 0.7]
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=retrievers,
            weights=weights
        )
        self.logger.info(f"ë¡œì»¬ Career ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(all_docs)})")
        print(f"âœ… [ë¡œì»¬ ì»¤ë¦¬ì–´ ì‚¬ë¡€ VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")

    def retrieve(self, query: str, k: int = 3):
        """ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¡œ ê²€ìƒ‰ (ê¸°ë³¸ 3ê°œ ê²°ê³¼) + ì‹œê°„ ê¸°ë°˜ í•„í„°ë§"""
        print(f"ğŸ” [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        
        if not self.ensemble_retriever:
            print(f"âŒ [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ê°€ ì—†ìŒ")
            return []
        
        # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
        # all_docs = self.ensemble_retriever.invoke(query)
        all_docs = self.ensemble_retriever.get_relevant_documents(query)
        
        # ìµœê·¼ í‚¤ì›Œë“œ ê°ì§€ ë° ì—°ë„ ì¶”ì¶œ
        recent_keywords = ['ìµœê·¼', 'ìµœì‹ ', 'recent', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ìƒˆë¡œìš´', 'ì‹ ê·œ', 'íŠ¸ë Œë“œ']
        is_recent_query = any(keyword in query.lower() for keyword in recent_keywords)
        
        # ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ
        years_info = self._extract_years_from_query(query)
        
        # "ì‹ ì…" ë˜ëŠ” "ì…ì‚¬" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹œì‘ ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        new_hire_keywords = ['ì‹ ì…', 'ì…ì‚¬', 'ìƒˆë¡œ', 'ì‹ ê·œ', 'ì‹œì‘', 'ì²˜ìŒ']
        focus_on_start_year = any(keyword in query.lower() for keyword in new_hire_keywords)
        
        if is_recent_query or years_info.get('n_years') or years_info.get('specific_year'):
            current_year = datetime.now().year
            
            # ì—°ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ 3ë…„
            if years_info.get('n_years'):
                min_year = current_year - years_info['n_years']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ì—°ë„: ìµœê·¼ {years_info['n_years']}ë…„ ({min_year}ë…„ ì´í›„)")
            elif years_info.get('specific_year'):
                min_year = years_info['specific_year']
                self.logger.info(f"ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ íŠ¹ì • ì—°ë„: {min_year}ë…„ ì´í›„")
            else:
                min_year = current_year - 3  # ê¸°ë³¸ê°’: ìµœê·¼ 3ë…„
                self.logger.info(f"ê¸°ë³¸ ì„¤ì •: ìµœê·¼ 3ë…„ ({min_year}ë…„ ì´í›„)")
            
            if focus_on_start_year:
                # ì‹ ì…/ì…ì‚¬ ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš°: ì‹œì‘ ì—°ë„ ê¸°ì¤€
                self.logger.info(f"ì‹ ì…/ì…ì‚¬ ê´€ë ¨ ì¿¼ë¦¬ ê°ì§€ë¨. {min_year}ë…„ ì´í›„ **ì‹œì‘ëœ** í™œë™ ë°ì´í„° í•„í„°ë§ ì‹œì‘...")
                filtered_docs = []
                for doc in all_docs:
                    try:
                        metadata = doc.metadata or {}
                        start_year = metadata.get('activity_start_year')
                        
                        if start_year and isinstance(start_year, int) and start_year >= min_year:
                            filtered_docs.append(doc)
                            self.logger.debug(f"í¬í•¨: {start_year}ë…„ ì‹œì‘ í™œë™ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                        else:
                            self.logger.debug(f"ì œì™¸: {start_year}ë…„ ì‹œì‘ í™œë™ (ìµœì†Œ ê¸°ì¤€: {min_year}ë…„ ì´í›„ ì‹œì‘) (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                    except Exception as e:
                        self.logger.warning(f"ë¬¸ì„œ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
            else:
                # ì¼ë°˜ ìµœê·¼ ì¿¼ë¦¬ì¸ ê²½ìš°: ìµœê·¼ í™œë™ì´ ìˆì—ˆë˜ ì§ì›ë“¤ ì¤‘ì—ì„œ
                self.logger.info(f"ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì‹œì‘: {min_year}ë…„ ì´í›„ **í™œë™ì´ ìˆì—ˆë˜** ë°ì´í„° ê²€ìƒ‰...")
                filtered_docs = []
                for doc in all_docs:
                    try:
                        metadata = doc.metadata or {}
                        
                        # í™œë™ ì—°ë„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§€ì •ëœ ê¸°ê°„ ë‚´ í™œë™ì´ ìˆëŠ”ì§€ í™•ì¸
                        activity_years = metadata.get('activity_years_list', [])
                        if activity_years and isinstance(activity_years, list):
                            recent_activity_years = [year for year in activity_years 
                                                   if isinstance(year, int) and year >= min_year]
                            if recent_activity_years:
                                filtered_docs.append(doc)
                                self.logger.debug(f"í¬í•¨: ìµœê·¼ í™œë™ ì—°ë„ {recent_activity_years} (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                                continue
                        
                        # í´ë°±: ì¢…ë£Œ ì—°ë„ê°€ ìµœê·¼ì¸ì§€ í™•ì¸
                        end_year = metadata.get('activity_end_year')
                        if end_year and isinstance(end_year, int) and end_year >= min_year:
                            filtered_docs.append(doc)
                            self.logger.debug(f"í¬í•¨: {end_year}ë…„ ì¢…ë£Œ í™œë™ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                        else:
                            self.logger.debug(f"ì œì™¸: ìµœê·¼ í™œë™ ì—†ìŒ (Employee: {doc.metadata.get('employee_id', 'Unknown')})")
                    except Exception as e:
                        self.logger.warning(f"ë¬¸ì„œ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
            
            self.logger.info(f"ì‹œê°„ í•„í„°ë§ ì™„ë£Œ: ì „ì²´ {len(all_docs)}ê°œ â†’ í•„í„°ë§ëœ {len(filtered_docs)}ê°œ ë¬¸ì„œ")
            final_docs = filtered_docs[:k]
        else:
            final_docs = all_docs[:k]
        
        # íšŒì‚¬ ë¹„ì „ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€ (ì»¤ë¦¬ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°)
        career_keywords = ['ì»¤ë¦¬ì–´', 'ì§„ë¡œ', 'ì„±ì¥', 'ë°œì „', 'ëª©í‘œ', 'ë°©í–¥', 'ê³„íš', 'ë¹„ì „', 'ë¯¸ë˜', 'íšŒì‚¬', 'ì¡°ì§']
        if any(keyword in query.lower() for keyword in career_keywords):
            company_vision = self._load_company_vision()
            if company_vision:
                # íšŒì‚¬ ë¹„ì „ì„ Document í˜•íƒœë¡œ ì¶”ê°€
                vision_content = self._format_company_vision_for_context(company_vision)
                vision_doc = Document(
                    page_content=vision_content,
                    metadata={"type": "company_vision", "source": "company_vision.json"}
                )
                final_docs.append(vision_doc)
                self.logger.info("íšŒì‚¬ ë¹„ì „ ì •ë³´ê°€ ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… [ì»¤ë¦¬ì–´ ì‚¬ë¡€ ê²€ìƒ‰] ì™„ë£Œ: {len(final_docs)}ê°œ ê²°ê³¼ ë°˜í™˜")
        return final_docs
    
    def _extract_years_from_query(self, query: str) -> dict:
        """ì¿¼ë¦¬ì—ì„œ ì—°ë„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        years_info = {'n_years': None, 'specific_year': None}
        
        # "ìµœê·¼ Në…„" íŒ¨í„´ ë§¤ì¹­
        year_patterns = [
            r'ìµœê·¼\s*(\d+)\s*ë…„',  # ìµœê·¼ 5ë…„
            r'ì§€ë‚œ\s*(\d+)\s*ë…„',  # ì§€ë‚œ 5ë…„
            r'ê³¼ê±°\s*(\d+)\s*ë…„',  # ê³¼ê±° 5ë…„
            r'(\d+)\s*ë…„\s*ë™ì•ˆ',  # 5ë…„ ë™ì•ˆ
            r'(\d+)\s*ë…„\s*ê°„',    # 5ë…„ê°„
            r'(\d+)\s*ë…„\s*ì´ë‚´',  # 5ë…„ ì´ë‚´
            r'(\d+)\s*ë…„\s*ì‚¬ì´',  # 5ë…„ ì‚¬ì´
        ]
        
        for pattern in year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    n_years = int(match.group(1))
                    if 1 <= n_years <= 50:  # ìœ íš¨í•œ ë²”ìœ„
                        years_info['n_years'] = n_years
                        break
                except ValueError:
                    continue
        
        # íŠ¹ì • ì—°ë„ íŒ¨í„´ ë§¤ì¹­ (ì˜ˆ: "2020ë…„ ì´í›„", "2023ë…„ë¶€í„°")
        specific_year_patterns = [
            r'(\d{4})\s*ë…„\s*ì´í›„',  # 2020ë…„ ì´í›„
            r'(\d{4})\s*ë…„\s*ë¶€í„°',  # 2020ë…„ë¶€í„°
            r'(\d{4})\s*ë…„\s*ì´ìƒ',  # 2020ë…„ ì´ìƒ
            r'(\d{4})\s*ì´í›„',      # 2020 ì´í›„
            r'(\d{4})\s*ë¶€í„°',      # 2020 ë¶€í„°
        ]
        
        for pattern in specific_year_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    year = int(match.group(1))
                    if 2000 <= year <= datetime.now().year:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        years_info['specific_year'] = year
                        break
                except ValueError:
                    continue
        
        return years_info
    
    def _get_latest_year_from_doc(self, doc: Document) -> int:
        """ë¬¸ì„œì—ì„œ ê°€ì¥ ìµœì‹  ì—°ë„ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        metadata = doc.metadata or {}
        
        # 1. í™œë™ ì¢…ë£Œ ì—°ë„ ìš°ì„  í™•ì¸ (ê°€ì¥ ì‹ ë¢°í•  ë§Œí•œ ì •ë³´)
        end_year = metadata.get('activity_end_year')
        if end_year and isinstance(end_year, int) and 2000 <= end_year <= 2030:
            return end_year
        
        # 2. í™œë™ ì—°ë„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœì‹  ì—°ë„ í™•ì¸
        activity_years = metadata.get('activity_years_list', [])
        if activity_years and isinstance(activity_years, list):
            try:
                valid_years = [year for year in activity_years if isinstance(year, int) and 2000 <= year <= 2030]
                if valid_years:
                    return max(valid_years)
            except:
                pass
        
        # 3. í™œë™ ì‹œì‘ ì—°ë„ í™•ì¸ (ì¢…ë£Œ ì—°ë„ê°€ ì—†ëŠ” ê²½ìš°)
        start_year = metadata.get('activity_start_year')
        if start_year and isinstance(start_year, int) and 2000 <= start_year <= 2030:
            return start_year
        
        # 4. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
        return self._extract_year_from_doc(doc)
    
    def _extract_year_from_doc(self, doc: Document) -> int:
        """ë¬¸ì„œì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ì°¾ê¸°
        metadata = doc.metadata or {}
        
        # ì§ì ‘ì ì¸ ì—°ë„ í•„ë“œë“¤ í™•ì¸
        year_fields = ['year', 'start_year', 'end_year', 'graduation_year', 'project_year']
        for field in year_fields:
            if field in metadata and metadata[field]:
                try:
                    year = int(metadata[field])
                    if 1980 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„
                        return year
                except:
                    continue
        
        # ë‚ ì§œ í˜•ì‹ì—ì„œ ì—°ë„ ì¶”ì¶œ
        date_fields = ['date', 'start_date', 'end_date', 'created_at', 'updated_at']
        for field in date_fields:
            if field in metadata and metadata[field]:
                try:
                    date_str = str(metadata[field])
                    # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD í˜•ì‹ ì²˜ë¦¬
                    year_match = re.search(r'(\d{4})', date_str)
                    if year_match:
                        year = int(year_match.group(1))
                        if 1980 <= year <= 2030:
                            return year
                except:
                    continue
        
        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì—°ë„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        content = doc.page_content or ""
        # "2023ë…„", "2024ë…„" ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
        year_patterns = [
            r'(\d{4})ë…„',  # 2023ë…„
            r'(\d{4})\s*-\s*(\d{4})',  # 2022-2024
            r'(\d{4})/(\d{1,2})',  # 2023/12
            r'(\d{4})\.(\d{1,2})'   # 2023.12
        ]
        
        years = []
        for pattern in year_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        # ì—¬ëŸ¬ ê·¸ë£¹ì´ ìˆëŠ” ê²½ìš° ëª¨ë“  ì—°ë„ ìˆ˜ì§‘
                        for group in match:
                            year = int(group)
                            if 1980 <= year <= 2030:
                                years.append(year)
                    else:
                        year = int(match)
                        if 1980 <= year <= 2030:
                            years.append(year)
                except:
                    continue
        
        # ê°€ì¥ ìµœê·¼ ì—°ë„ ë°˜í™˜
        return max(years) if years else None

    def _load_education_resources(self):
        """êµìœ¡ê³¼ì • ë¦¬ì†ŒìŠ¤ ì§€ì—° ë¡œë”©"""
        if self.education_vectorstore is None:
            self._initialize_education_vectorstore()
        if self.skill_education_mapping is None:
            self._load_skill_education_mapping()
        if self.course_deduplication_index is None:
            self._load_deduplication_index()
    
    def _initialize_education_vectorstore(self):
        """êµìœ¡ê³¼ì • VectorDB ì´ˆê¸°í™” (í™˜ê²½ë³„ ë¶„ê¸°)"""
        if self.is_k8s:
            self._initialize_k8s_education_vectorstore()
        else:
            self._initialize_local_education_vectorstore()
    
    def _initialize_k8s_education_vectorstore(self):
        """K8s í™˜ê²½: ì™¸ë¶€ êµìœ¡ê³¼ì • ChromaDB ì´ˆê¸°í™”"""
        try:
            print("ğŸ”— [K8s êµìœ¡ê³¼ì • ChromaDB] ì™¸ë¶€ ChromaDB ì—°ê²° ì¤‘...")
            self.education_vectorstore = K8sChromaDBAdapter("education_courses", self.education_cached_embeddings)
            
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = self.education_vectorstore.get_collection_info()
            if collection_info.get("status") == "success":
                print(f"âœ… [K8s êµìœ¡ê³¼ì • ChromaDB] ì—°ê²° ì„±ê³µ: {collection_info.get('document_count')}ê°œ ë¬¸ì„œ")
            else:
                print(f"âŒ [K8s êµìœ¡ê³¼ì • ChromaDB] ì—°ê²° ì‹¤íŒ¨: {collection_info.get('message')}")
                self.education_vectorstore = None
        except Exception as e:
            self.logger.error(f"K8s êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âŒ [K8s êµìœ¡ê³¼ì • ChromaDB] ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.education_vectorstore = None
    
    def _initialize_local_education_vectorstore(self):
        """ë¡œì»¬ í™˜ê²½: ê¸°ì¡´ ë¡œì»¬ êµìœ¡ê³¼ì • ChromaDB ì´ˆê¸°í™”"""
        try:
            if os.path.exists(self.education_persist_dir):
                self.education_vectorstore = Chroma(
                    persist_directory=self.education_persist_dir,
                    embedding_function=self.education_cached_embeddings,
                    collection_name="education_courses"
                )
                self.logger.info("ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì™„ë£Œ")
                print(f"âœ… [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("ë¡œì»¬ êµìœ¡ê³¼ì • VectorDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"âš ï¸  [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ì—†ìŒ - JSON íŒŒì¼ë¡œ í´ë°± ê²€ìƒ‰ ì§„í–‰")
        except Exception as e:
            self.logger.error(f"ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âŒ [ë¡œì»¬ êµìœ¡ê³¼ì • VectorDB] ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.education_vectorstore = None
    
    def _load_skill_education_mapping(self):
        """ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ"""
        try:
            if os.path.exists(self.skill_mapping_path):
                with open(self.skill_mapping_path, "r", encoding="utf-8") as f:
                    self.skill_education_mapping = json.load(f)
                self.logger.info(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.skill_education_mapping)}ê°œ ìŠ¤í‚¬")
            else:
                self.skill_education_mapping = {}
                self.logger.warning("ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìŠ¤í‚¬-êµìœ¡ê³¼ì • ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.skill_education_mapping = {}
    
    def _load_deduplication_index(self):
        """ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if os.path.exists(self.deduplication_index_path):
                with open(self.deduplication_index_path, "r", encoding="utf-8") as f:
                    self.course_deduplication_index = json.load(f)
                self.logger.info(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.course_deduplication_index)}ê°œ ê·¸ë£¹")
            else:
                self.course_deduplication_index = {}
                self.logger.warning("ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ ì œê±° ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.course_deduplication_index = {}
    
    def _load_company_vision(self):
        """íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ"""
        if self.company_vision_data is not None:
            return self.company_vision_data
            
        try:
            if os.path.exists(self.company_vision_path):
                with open(self.company_vision_path, "r", encoding="utf-8") as f:
                    self.company_vision_data = json.load(f)
                self.logger.info("íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            else:
                self.company_vision_data = {}
                self.logger.warning("íšŒì‚¬ ë¹„ì „ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"íšŒì‚¬ ë¹„ì „ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.company_vision_data = {}
        
        return self.company_vision_data
    
    def _format_company_vision_for_context(self, vision_data: Dict) -> str:
        """íšŒì‚¬ ë¹„ì „ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not vision_data:
            return ""
        
        sections = []
        
        # íšŒì‚¬ ê¸°ë³¸ ì •ë³´
        if vision_data.get('company_name'):
            sections.append(f"íšŒì‚¬ëª…: {vision_data['company_name']}")
        
        # ë¹„ì „
        if vision_data.get('vision'):
            vision = vision_data['vision']
            sections.append(f"ë¹„ì „: {vision.get('title', '')}")
            if vision.get('description'):
                sections.append(f"ë¹„ì „ ì„¤ëª…: {vision['description']}")
        
        # í•µì‹¬ ê°€ì¹˜
        if vision_data.get('core_values'):
            values_text = []
            for value in vision_data['core_values']:
                values_text.append(f"- {value.get('name', '')}: {value.get('description', '')}")
            if values_text:
                sections.append("í•µì‹¬ ê°€ì¹˜:\n" + "\n".join(values_text))
        
        # ì „ëµ ë°©í–¥
        if vision_data.get('strategic_directions'):
            strategy_text = []
            for direction in vision_data['strategic_directions']:
                strategy_text.append(f"- {direction.get('category', '')}: {direction.get('description', '')}")
            if strategy_text:
                sections.append("ì „ëµ ë°©í–¥:\n" + "\n".join(strategy_text))
        
        # ì¸ì¬ ê°œë°œ
        if vision_data.get('talent_development'):
            talent = vision_data['talent_development']
            sections.append(f"ì¸ì¬ ê°œë°œ ì² í•™: {talent.get('philosophy', '')}")
            if talent.get('focus_areas'):
                focus_text = []
                for area in talent['focus_areas']:
                    focus_text.append(f"- {area.get('area', '')}: {area.get('description', '')}")
                if focus_text:
                    sections.append("ì—­ëŸ‰ ê°œë°œ ì¤‘ì  ì˜ì—­:\n" + "\n".join(focus_text))
        
        # ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™
        if vision_data.get('career_guidance_principles'):
            principles_text = []
            for principle in vision_data['career_guidance_principles']:
                principles_text.append(f"- {principle.get('principle', '')}: {principle.get('description', '')}")
            if principles_text:
                sections.append("ì»¤ë¦¬ì–´ ê°€ì´ë“œ ì›ì¹™:\n" + "\n".join(principles_text))
        
        return "\n\n".join(sections)

    def search_education_courses(self, query: str, user_profile: Dict, intent_analysis: Dict) -> Dict:
        """êµìœ¡ê³¼ì • ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜ - ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ê²€ìƒ‰"""
        print(f"ğŸ” [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì‹œì‘ - '{query}'")
        self._load_education_resources()
        
        try:
            # ì‚¬ìš©ìì˜ êµìœ¡ê³¼ì • ì†ŒìŠ¤ ì„ í˜¸ë„ í™•ì¸
            preferred_source = self._get_preferred_education_source(query, user_profile, intent_analysis)
            
            # 1ë‹¨ê³„: ìŠ¤í‚¬ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°ë§
            skill_based_courses = self._skill_based_course_filter(user_profile, intent_analysis)
            
            # 2ë‹¨ê³„: VectorDB ì˜ë¯¸ì  ê²€ìƒ‰ (VectorDBê°€ ì—†ìœ¼ë©´ JSON í´ë°±)
            semantic_matches = self._semantic_course_search(query, skill_based_courses)
            
            # 3ë‹¨ê³„: ì„ í˜¸ë„ì— ë”°ë¥¸ ì†ŒìŠ¤ í•„í„°ë§
            if preferred_source:
                semantic_matches = self._filter_by_preferred_source(semantic_matches, preferred_source)
            
            # 4ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            deduplicated_courses = self._deduplicate_courses(semantic_matches)
            
            # ìµœì¢…ì ìœ¼ë¡œ 3ê°œê¹Œì§€ë§Œ ì œí•œ
            deduplicated_courses = deduplicated_courses[:3]
            
            # 5ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° í•™ìŠµ ê²½ë¡œ ìƒì„±
            course_analysis = self._analyze_course_recommendations(deduplicated_courses)
            learning_path = self._generate_learning_path(deduplicated_courses)
            
            self.logger.info(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì™„ë£Œ: ìµœì¢… {len(deduplicated_courses)}ê°œ ê³¼ì • ë°˜í™˜")
            print(f"âœ… [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì™„ë£Œ: {len(deduplicated_courses)}ê°œ ê³¼ì • ë°˜í™˜")
            
            return {
                "recommended_courses": deduplicated_courses,
                "course_analysis": course_analysis,
                "learning_path": learning_path
            }
        except Exception as e:
            self.logger.error(f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ [êµìœ¡ê³¼ì • ê²€ìƒ‰] ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "recommended_courses": [],
                "course_analysis": {"message": f"êµìœ¡ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"},
                "learning_path": []
            }
    
    def _skill_based_course_filter(self, user_profile: Dict, intent_analysis: Dict) -> List[Dict]:
        """ìŠ¤í‚¬ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ - JSON ì¸ë±ìŠ¤ í™œìš©"""
        filtered_courses = []
        
        # ì‚¬ìš©ì í˜„ì¬ ìŠ¤í‚¬ ì¶”ì¶œ
        current_skills = self._extract_user_skills(user_profile)
        
        # ì˜ë„ ë¶„ì„ì—ì„œ ëª©í‘œ ìŠ¤í‚¬ ì¶”ì¶œ
        target_skills = intent_analysis.get("career_history", [])
        
        # ê²€ìƒ‰í•  ìŠ¤í‚¬ ëª©ë¡ ìƒì„±
        search_skills = list(set(current_skills + target_skills))
        
        for skill_code in search_skills:
            if skill_code in self.skill_education_mapping:
                skill_courses = self.skill_education_mapping[skill_code]
                
                # College ê³¼ì • - ì„¸ë¶„í™” ë ˆë²¨ë³„ ì¶”ê°€
                for course_type in ["specialized", "recommended", "common_required"]:
                    if course_type in skill_courses.get("college", {}):
                        courses = skill_courses["college"][course_type]
                        for course in courses:
                            course_info = course.copy()
                            course_info["source"] = "college"
                            course_info["skill_relevance"] = course_type
                            course_info["target_skill"] = skill_code
                            filtered_courses.append(course_info)
                
                # mySUNI ê³¼ì • ì¶”ê°€
                if "mysuni" in skill_courses:
                    for course in skill_courses["mysuni"]:
                        course_info = course.copy()
                        course_info["source"] = "mysuni"
                        course_info["skill_relevance"] = "general"
                        course_info["target_skill"] = skill_code
                        filtered_courses.append(course_info)
        
        self.logger.info(f"ìŠ¤í‚¬ ê¸°ë°˜ í•„í„°ë§ ê²°ê³¼: {len(filtered_courses)}ê°œ ê³¼ì •")
        return filtered_courses
    
    def _extract_user_skills(self, user_profile: Dict) -> List[str]:
        """ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ìŠ¤í‚¬ ì¶”ì¶œ"""
        skills = []
        
        # ì§ì ‘ì ì¸ ìŠ¤í‚¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if "skills" in user_profile:
            skills.extend(user_profile["skills"])
        
        # ê²½ë ¥ ì •ë³´ì—ì„œ ìŠ¤í‚¬ ì¶”ì¶œ
        if "career_history" in user_profile:
            for career in user_profile["career_history"]:
                if "skills" in career:
                    skills.extend(career["skills"])
        
        return list(set(skills))
    
    def _semantic_course_search(self, query: str, filtered_courses: List[Dict]) -> List[Dict]:
        """VectorDBë¥¼ í™œìš©í•œ ì˜ë¯¸ì  ê²€ìƒ‰ (VectorDBê°€ ì—†ìœ¼ë©´ JSONì—ì„œ ê²€ìƒ‰) - 3ê°œê¹Œì§€ë§Œ ê²€ìƒ‰"""
        if not self.education_vectorstore:
            # VectorDBê°€ ì—†ìœ¼ë©´ JSON íŒŒì¼ì—ì„œ ì§ì ‘ ê²€ìƒ‰
            self.logger.info("VectorDB ì—†ìŒ - JSON íŒŒì¼ì—ì„œ ê²€ìƒ‰")
            return self._search_from_json_documents(query, filtered_courses)
            
        if not filtered_courses:
            # í•„í„°ë§ëœ ê³¼ì •ì´ ì—†ìœ¼ë©´ ì „ì²´ VectorDBì—ì„œ ê²€ìƒ‰ (3ê°œë¡œ ì œí•œ)
            docs = self.education_vectorstore.similarity_search(query, k=3)
            courses = [self._doc_to_course_dict(doc) for doc in docs]
            # ì›ë³¸ ë°ì´í„°ë¡œ ìƒì„¸ ì •ë³´ ë³´ê°•
            courses = [self._enrich_course_with_original_data(course) for course in courses]
        else:
            # í•„í„°ë§ëœ ê³¼ì •ë“¤ì˜ course_idë¡œ VectorDBì—ì„œ ìƒì„¸ ê²€ìƒ‰
            course_ids = [course.get("course_id") for course in filtered_courses if course.get("course_id")]
            courses = self._search_by_course_ids(course_ids, query)
            
            # í•„í„°ë§ ì •ë³´ë¥¼ VectorDB ê²°ê³¼ì— ë³‘í•©
            for course in courses:
                for filtered_course in filtered_courses:
                    if course.get("course_id") == filtered_course.get("course_id"):
                        course.update(filtered_course)
                        break
            
            # ì›ë³¸ ë°ì´í„°ë¡œ ìƒì„¸ ì •ë³´ ë³´ê°•
            courses = [self._enrich_course_with_original_data(course) for course in courses]
        
        # ê²°ê³¼ë¥¼ 3ê°œë¡œ ì œí•œ
        courses = courses[:3]
        self.logger.info(f"ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {len(courses)}ê°œ ê³¼ì • (3ê°œë¡œ ì œí•œ)")
        return courses
    
    def _search_from_json_documents(self, query: str, filtered_courses: List[Dict]) -> List[Dict]:
        """JSON ë¬¸ì„œì—ì„œ ì§ì ‘ ê²€ìƒ‰ (VectorDB ëŒ€ì•ˆ) - 3ê°œê¹Œì§€ë§Œ ê²€ìƒ‰"""
        try:
            with open(self.education_docs_path, "r", encoding="utf-8") as f:
                all_docs = json.load(f)
        except FileNotFoundError:
            self.logger.warning("êµìœ¡ê³¼ì • ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # í•„í„°ë§ëœ ê³¼ì •ì´ë¼ë„ ë°˜í™˜í•˜ì (3ê°œë¡œ ì œí•œ)
            return filtered_courses[:3] if filtered_courses else []
        
        # í•„í„°ë§ëœ ê³¼ì •ì´ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
        if filtered_courses:
            # filtered_coursesì˜ course_idë“¤ê³¼ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œë“¤ ì°¾ê¸°
            filtered_course_ids = {course.get("course_id") for course in filtered_courses}
            matching_docs = []
            
            for doc in all_docs:
                metadata = doc.get("metadata", {})
                course_id = metadata.get("course_id")
                
                if course_id in filtered_course_ids:
                    course_dict = self._doc_to_course_dict_from_json(doc)
                    # í•„í„°ë§ ì •ë³´ ë³‘í•©
                    for filtered_course in filtered_courses:
                        if course_dict.get("course_id") == filtered_course.get("course_id"):
                            course_dict.update(filtered_course)
                            break
                    matching_docs.append(course_dict)
            
            if matching_docs:
                # 3ê°œë¡œ ì œí•œ
                matching_docs = matching_docs[:3]
                self.logger.info(f"í•„í„°ë§ëœ ê³¼ì • ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(matching_docs)}ê°œ (3ê°œë¡œ ì œí•œ)")
                return matching_docs
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_keywords = query.lower().split()
        matching_docs = []
        
        for doc in all_docs:
            content = doc.get("page_content", "").lower()
            metadata = doc.get("metadata", {})
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in query_keywords:
                if keyword in content:
                    score += 1
            
            if score > 0:
                course_dict = self._doc_to_course_dict_from_json(doc)
                course_dict["match_score"] = score
                matching_docs.append(course_dict)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        matching_docs.sort(key=lambda x: x.get("match_score", 0), reverse=True)
        
        # 3ê°œë¡œ ì œí•œ
        matching_docs = matching_docs[:3]
        self.logger.info(f"í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(matching_docs)}ê°œ (3ê°œë¡œ ì œí•œ)")
        return matching_docs
    
    def _doc_to_course_dict_from_json(self, doc_data: Dict) -> Dict:
        """JSON ë¬¸ì„œ ë°ì´í„°ë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc_data.get("metadata", {})
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc_data.get("page_content", ""),
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")  # URL í•„ë“œ ì¶”ê°€
        }
    
    def _search_by_course_ids(self, course_ids: List[str], query: str) -> List[Dict]:
        """íŠ¹ì • ê³¼ì • IDë“¤ì— ëŒ€í•œ VectorDB ê²€ìƒ‰ - 3ê°œê¹Œì§€ë§Œ ê²€ìƒ‰"""
        if not course_ids:
            return []
        
        # ê° course_idì— ëŒ€í•´ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ í†µí•©
        all_docs = []
        for course_id in course_ids[:10]:  # ê²€ìƒ‰í•  course_idëŠ” ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            try:
                # ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰
                docs = self.education_vectorstore.similarity_search(
                    query, 
                    k=1,  # ê° ê³¼ì •ë‹¹ 1ê°œì”©ë§Œ ê°€ì ¸ì™€ì„œ ì „ì²´ì ìœ¼ë¡œ 3ê°œ ì œí•œ ìœ ì§€
                    filter={"course_id": course_id}
                )
                all_docs.extend(docs)
                # ì´ë¯¸ 3ê°œê°€ ë˜ë©´ ì¤‘ë‹¨
                if len(all_docs) >= 3:
                    break
            except Exception as e:
                self.logger.warning(f"Course ID {course_id} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì¼ë°˜ ê²€ìƒ‰ë„ ìˆ˜í–‰ (ë°±ì—…) - 3ê°œë¡œ ì œí•œ
        if not all_docs:
            all_docs = self.education_vectorstore.similarity_search(query, k=3)
        
        # ê²°ê³¼ë¥¼ 3ê°œë¡œ ì œí•œ
        all_docs = all_docs[:3]
        return [self._doc_to_course_dict(doc) for doc in all_docs]
    
    def _doc_to_course_dict(self, doc: Document) -> Dict:
        """VectorDB Documentë¥¼ ê³¼ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        metadata = doc.metadata or {}
        return {
            "course_id": metadata.get("course_id"),
            "course_name": metadata.get("course_name", metadata.get("card_name")),
            "source": metadata.get("source"),
            "content": doc.page_content,
            "target_skills": metadata.get("target_skills", []),
            "skill_relevance": metadata.get("skill_relevance"),
            "duration_hours": metadata.get("duration_hours", metadata.get("ì¸ì •í•™ìŠµì‹œê°„")),
            "difficulty_level": metadata.get("difficulty_level", metadata.get("ë‚œì´ë„")),
            "department": metadata.get("department", metadata.get("í•™ë¶€")),
            "course_type": metadata.get("course_type", metadata.get("êµìœ¡ìœ í˜•")),
            "í‰ì ": metadata.get("í‰ì "),
            "ì´ìˆ˜ììˆ˜": metadata.get("ì´ìˆ˜ììˆ˜"),
            "ì¹´í…Œê³ ë¦¬ëª…": metadata.get("ì¹´í…Œê³ ë¦¬ëª…"),
            "ì±„ë„ëª…": metadata.get("ì±„ë„ëª…"),
            "í‘œì¤€ê³¼ì •": metadata.get("í‘œì¤€ê³¼ì •"),
            "url": metadata.get("url")  # URL í•„ë“œ ì¶”ê°€
        }
    
    def _deduplicate_courses(self, courses: List[Dict]) -> List[Dict]:
        """Collegeì™€ mySUNI ê°„ ì¤‘ë³µ ê³¼ì • ì œê±° (mySUNI ë©”íƒ€ë°ì´í„° ë³´ì¡´)"""
        if not courses:
            return []
        
        deduplicated = []
        seen_courses = set()
        
        # ìš°ì„ ìˆœìœ„: College > mySUNI (Collegeê°€ ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ)
        def sort_priority(course):
            source_priority = 0 if course.get("source") == "college" else 1
            
            if course.get("source") == "college":
                relevance = course.get("skill_relevance", "")
                if relevance == "specialized":
                    relevance_priority = 0
                elif relevance == "recommended":
                    relevance_priority = 1
                else:  # common_required
                    relevance_priority = 2
            else:
                # mySUNIëŠ” í‰ì  ê¸°ì¤€ ì •ë ¬
                rating = course.get("í‰ì ", 0)
                try:
                    rating = float(rating) if rating else 0
                except:
                    rating = 0
                relevance_priority = 5 - rating  # í‰ì ì´ ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ
            
            return (source_priority, relevance_priority)
        
        sorted_courses = sorted(courses, key=sort_priority)
        
        for course in sorted_courses:
            course_signature = self._generate_course_signature(course)
            
            if course_signature not in seen_courses:
                # ì¤‘ë³µ ê³¼ì •ì´ ìˆëŠ” ê²½ìš° mySUNI ë°ì´í„°ë¥¼ College ê³¼ì •ì— í†µí•©
                if course_signature in self.course_deduplication_index:
                    duplicate_info = self.course_deduplication_index[course_signature]
                    
                    # College ê³¼ì •ì´ ìš°ì„ ì´ë¯€ë¡œ mySUNI ë°ì´í„°ë¥¼ ì¶”ê°€ ì •ë³´ë¡œ ë³‘í•©
                    if course.get("source") == "college":
                        mysuni_data = self._find_mysuni_duplicate(duplicate_info, courses)
                        if mysuni_data:
                            course["mysuni_alternative"] = {
                                "available": True,
                                "card_name": mysuni_data.get("card_name"),
                                "í‰ì ": mysuni_data.get("í‰ì "),
                                "ì´ìˆ˜ììˆ˜": mysuni_data.get("ì´ìˆ˜ììˆ˜"),
                                "ë‚œì´ë„": mysuni_data.get("ë‚œì´ë„"),
                                "ì¸ì •í•™ìŠµì‹œê°„": mysuni_data.get("ì¸ì •í•™ìŠµì‹œê°„"),
                                "ì¹´í…Œê³ ë¦¬ëª…": mysuni_data.get("ì¹´í…Œê³ ë¦¬ëª…"),
                                "ì±„ë„ëª…": mysuni_data.get("ì±„ë„ëª…")
                            }
                        else:
                            course["mysuni_alternative"] = {"available": False}
                    else:
                        # mySUNI ê³¼ì •ì¸ ê²½ìš° ì›ë³¸ ë°ì´í„° ìœ ì§€
                        course["mysuni_alternative"] = {"available": False}
                    
                    course["alternative_platforms"] = duplicate_info.get("platforms", [])
                else:
                    # ì¤‘ë³µì´ ì—†ëŠ” ê³¼ì •ì¸ ê²½ìš°
                    if course.get("source") == "mysuni":
                        course["mysuni_alternative"] = {"available": False}
                
                deduplicated.append(course)
                seen_courses.add(course_signature)
        
        self.logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(courses)}ê°œ â†’ {len(deduplicated)}ê°œ")
        return deduplicated
    
    def _generate_course_signature(self, course: Dict) -> str:
        """ê³¼ì • ì¤‘ë³µ íŒë³„ì„ ìœ„í•œ ì‹œê·¸ë‹ˆì²˜ ìƒì„±"""
        name = course.get("course_name", course.get("card_name", "")).lower().strip()
        skills = sorted(course.get("target_skills", []))
        
        # ìœ ì‚¬í•œ ê³¼ì •ëª… ì •ê·œí™”
        normalized_name = re.sub(r'[^\w\s]', '', name)
        normalized_name = re.sub(r'\s+', ' ', normalized_name)
        
        return f"{normalized_name}_{','.join(skills)}"
    
    def _find_mysuni_duplicate(self, duplicate_info: Dict, all_courses: List[Dict]) -> Dict:
        """ì¤‘ë³µ ì •ë³´ì—ì„œ mySUNI ê³¼ì • ì°¾ê¸°"""
        mysuni_course_info = None
        for course_info in duplicate_info.get("courses", []):
            if course_info.get("platform") == "mySUNI":
                course_id = course_info.get("course_id")
                # ì „ì²´ ê³¼ì • ë¦¬ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ mySUNI ê³¼ì • ì°¾ê¸°
                for course in all_courses:
                    if (course.get("source") == "mysuni" and 
                        course.get("course_id") == course_id):
                        mysuni_course_info = course
                        break
                break
        
        return mysuni_course_info
    
    def _analyze_course_recommendations(self, courses: List[Dict]) -> Dict:
        """ì¶”ì²œ ê³¼ì • ë¶„ì„ ê²°ê³¼ ìƒì„± (mySUNI ë°ì´í„° í¬í•¨)"""
        if not courses:
            return {"message": "ì¶”ì²œí•  êµìœ¡ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        college_courses = [c for c in courses if c.get("source") == "college"]
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        
        # College ê³¼ì • ì„¸ë¶„í™” ë¶„ì„
        specialized_count = len([c for c in college_courses if c.get("skill_relevance") == "specialized"])
        recommended_count = len([c for c in college_courses if c.get("skill_relevance") == "recommended"])
        required_count = len([c for c in college_courses if c.get("skill_relevance") == "common_required"])
        
        # mySUNI ëŒ€ì•ˆ ì •ë³´ ë¶„ì„
        college_with_mysuni_alt = len([c for c in college_courses 
                                      if c.get("mysuni_alternative", {}).get("available")])
        
        # mySUNI ê³¼ì • í‰ì  ë¶„ì„
        mysuni_ratings = []
        for c in mysuni_courses:
            rating = c.get("í‰ì ", 0)
            try:
                rating = float(rating) if rating else 0
                if rating > 0:
                    mysuni_ratings.append(rating)
            except:
                continue
        
        avg_mysuni_rating = sum(mysuni_ratings) / len(mysuni_ratings) if mysuni_ratings else 0
        
        # ì´ìˆ˜ì ìˆ˜ í•©ê³„
        total_enrollments = 0
        for course in mysuni_courses:
            enrollments_str = str(course.get("ì´ìˆ˜ììˆ˜", "0"))
            try:
                enrollments = int(enrollments_str.replace(",", "")) if enrollments_str.replace(",", "").isdigit() else 0
                total_enrollments += enrollments
            except:
                continue
        
        return {
            "total_courses": len(courses),
            "college_courses": len(college_courses),
            "mysuni_courses": len(mysuni_courses),
            "skill_depth_analysis": {
                "specialized": specialized_count,
                "recommended": recommended_count, 
                "common_required": required_count
            },
            "learning_platforms": {
                "college_available": len(college_courses) > 0,
                "mysuni_available": len(mysuni_courses) > 0,
                "college_with_mysuni_alternatives": college_with_mysuni_alt
            },
            "mysuni_quality_metrics": {
                "average_rating": round(avg_mysuni_rating, 1),
                "total_enrollments": total_enrollments,
                "high_rated_courses": len([r for r in mysuni_ratings if r >= 4.5])
            }
        }
    
    def _generate_learning_path(self, courses: List[Dict]) -> List[Dict]:
        """í•™ìŠµ ê²½ë¡œ ì œì•ˆ ìƒì„±"""
        if not courses:
            return []
        
        path = []
        
        # 1ë‹¨ê³„: ê³µí†µ í•„ìˆ˜ ê³¼ì •
        required_courses = [c for c in courses if c.get("skill_relevance") == "common_required"]
        if required_courses:
            path.append({
                "step": 1,
                "level": "ê¸°ì´ˆ/í•„ìˆ˜",
                "courses": required_courses[:2],  # ìµœëŒ€ 2ê°œ
                "description": "ê¸°ë³¸ ì§€ì‹ ìŠµë“ì„ ìœ„í•œ í•„ìˆ˜ ê³¼ì •"
            })
        
        # 2ë‹¨ê³„: ì¶”ì²œ ê³¼ì •
        recommended_courses = [c for c in courses if c.get("skill_relevance") == "recommended"]
        if recommended_courses:
            path.append({
                "step": 2,
                "level": "í™•ì¥/ì‘ìš©",
                "courses": recommended_courses[:3],  # ìµœëŒ€ 3ê°œ
                "description": "ê´€ë ¨ ê¸°ìˆ  í™•ì¥ì„ ìœ„í•œ ì¶”ì²œ ê³¼ì •"
            })
        
        # 3ë‹¨ê³„: ì „ë¬¸í™” ê³¼ì •
        specialized_courses = [c for c in courses if c.get("skill_relevance") == "specialized"]
        if specialized_courses:
            path.append({
                "step": 3,
                "level": "ì „ë¬¸/ì‹¬í™”",
                "courses": specialized_courses[:2],  # ìµœëŒ€ 2ê°œ
                "description": "ì „ë¬¸ì„± ê°•í™”ë¥¼ ìœ„í•œ íŠ¹í™” ê³¼ì •"
            })
        
        # mySUNI ê³¼ì •ì€ ë³´ì™„/ëŒ€ì•ˆìœ¼ë¡œ ì œì‹œ
        mysuni_courses = [c for c in courses if c.get("source") == "mysuni"]
        if mysuni_courses:
            path.append({
                "step": "ë³´ì™„",
                "level": "ì˜¨ë¼ì¸/ììœ¨",
                "courses": mysuni_courses[:3],  # ìµœëŒ€ 3ê°œ
                "description": "ì˜¨ë¼ì¸ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ë³´ì™„ ê³¼ì •"
            })
        
        return path

    def _load_original_course_data(self):
        """ì›ë³¸ êµìœ¡ê³¼ì • ìƒì„¸ ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ ì†ì„± ë°©ì‹ ì‚¬ìš©)"""
        if not hasattr(self, 'original_mysuni_data'):
            try:
                mysuni_path = PathConfig.MYSUNI_DETAILED
                with open(mysuni_path, "r", encoding="utf-8") as f:
                    self.original_mysuni_data = json.load(f)
                self.logger.info(f"mySUNI ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_mysuni_data)}ê°œ - ê²½ë¡œ: {mysuni_path}")
            except FileNotFoundError:
                self.logger.warning(f"mySUNI ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. - ê²½ë¡œ: {PathConfig.MYSUNI_DETAILED}")
                self.original_mysuni_data = []
                
        if not hasattr(self, 'original_college_data'):
            try:
                college_path = PathConfig.COLLEGE_DETAILED
                with open(college_path, "r", encoding="utf-8") as f:
                    self.original_college_data = json.load(f)
                self.logger.info(f"College ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.original_college_data)}ê°œ - ê²½ë¡œ: {college_path}")
            except FileNotFoundError:
                self.logger.warning(f"College ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. - ê²½ë¡œ: {PathConfig.COLLEGE_DETAILED}")
                self.original_college_data = []

    def _enrich_course_with_original_data(self, course: Dict) -> Dict:
        """VectorDB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ë³´ê°•"""
        self._load_original_course_data()
        
        course_id = course.get("course_id")
        source = course.get("source")
        
        if not course_id:
            return course
            
        # mySUNI ê³¼ì •ì¸ ê²½ìš°
        if source == "mysuni":
            for original in self.original_mysuni_data:
                if original.get("course_id") == course_id:
                    # ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    course.update({
                        "ì¹´í…Œê³ ë¦¬ëª…": original.get("ì¹´í…Œê³ ë¦¬ëª…"),
                        "ì±„ë„ëª…": original.get("ì±„ë„ëª…"),
                        "íƒœê·¸ëª…": original.get("íƒœê·¸ëª…"),
                        "ë‚œì´ë„": original.get("ë‚œì´ë„"),
                        "í‰ì ": original.get("í‰ì "),
                        "ì´ìˆ˜ììˆ˜": original.get("ì´ìˆ˜ììˆ˜"),
                        "ì§ë¬´": original.get("ì§ë¬´", []),
                        "skillset": original.get("skillset", []),
                        "url": original.get("url")
                    })
                    break
                    
        # College ê³¼ì •ì¸ ê²½ìš°
        elif source == "college":
            for original in self.original_college_data:
                if original.get("course_id") == course_id:
                    # ì›ë³¸ ë°ì´í„°ì˜ ìƒì„¸ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    course.update({
                        "í•™ë¶€": original.get("í•™ë¶€"),
                        "í‘œì¤€ê³¼ì •": original.get("í‘œì¤€ê³¼ì •"),
                        "ì‚¬ì—…ë³„êµìœ¡ì²´ê³„": original.get("ì‚¬ì—…ë³„êµìœ¡ì²´ê³„"),
                        "êµìœ¡ìœ í˜•": original.get("êµìœ¡ìœ í˜•"),
                        "í•™ìŠµìœ í˜•": original.get("í•™ìŠµìœ í˜•"),
                        "ê³µê°œì—¬ë¶€": original.get("ê³µê°œì—¬ë¶€"),
                        "íŠ¹í™”ì§ë¬´": original.get("íŠ¹í™”ì§ë¬´", []),
                        "ì¶”ì²œì§ë¬´": original.get("ì¶”ì²œì§ë¬´", []),
                        "ê³µí†µí•„ìˆ˜ì§ë¬´": original.get("ê³µí†µí•„ìˆ˜ì§ë¬´", []),
                        "url": original.get("url")
                    })
                    break
        
        return course
    
    def _get_preferred_education_source(self, query: str, user_profile: Dict, intent_analysis: Dict) -> str:
        """ì‚¬ìš©ìì˜ êµìœ¡ê³¼ì • ì†ŒìŠ¤ ì„ í˜¸ë„ ê°ì§€"""
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ëª…ì‹œì  ì–¸ê¸‰ í™•ì¸
        query_lower = query.lower()
        if 'mysuni' in query_lower or 'my suni' in query_lower:
            return 'mysuni'
        elif 'college' in query_lower or 'ì»¬ë¦¬ì§€' in query_lower:
            return 'college'
        
        # 2. ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ì„ í˜¸ë„ í™•ì¸
        preferred_source = user_profile.get('preferred_education_source', '')
        if preferred_source in ['mysuni', 'college']:
            return preferred_source
        
        # 3. ì˜ë„ ë¶„ì„ì—ì„œ ì„ í˜¸ë„ í™•ì¸
        intent_preferred = intent_analysis.get('preferred_source', '')
        if intent_preferred in ['mysuni', 'college']:
            return intent_preferred
        
        # ê¸°ë³¸ê°’: ì„ í˜¸ë„ ì—†ìŒ
        return ''
    
    def _filter_by_preferred_source(self, courses: List[Dict], preferred_source: str) -> List[Dict]:
        """ì„ í˜¸í•˜ëŠ” êµìœ¡ê³¼ì • ì†ŒìŠ¤ë¡œ í•„í„°ë§"""
        if not preferred_source or not courses:
            return courses
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ë“¤ ë¨¼ì € ì¶”ì¶œ
        preferred_courses = [course for course in courses if course.get('source') == preferred_source]
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ì´ ì¶©ë¶„íˆ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ë°˜í™˜ (ìµœì†Œ 3ê°œ)
        if len(preferred_courses) >= 3:
            self.logger.info(f"{preferred_source} ê³¼ì • {len(preferred_courses)}ê°œë¡œ í•„í„°ë§")
            return preferred_courses
        
        # ì„ í˜¸ ì†ŒìŠ¤ì˜ ê³¼ì •ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ì†ŒìŠ¤ë„ í¬í•¨í•˜ë˜ ì„ í˜¸ ì†ŒìŠ¤ ìš°ì„  ì •ë ¬
        other_courses = [course for course in courses if course.get('source') != preferred_source]
        result = preferred_courses + other_courses[:7-len(preferred_courses)]  # ìµœëŒ€ 7ê°œê¹Œì§€
        
        self.logger.info(f"{preferred_source} ìš°ì„  í•„í„°ë§: {len(preferred_courses)}ê°œ + ê¸°íƒ€ {len(result)-len(preferred_courses)}ê°œ")
        return result